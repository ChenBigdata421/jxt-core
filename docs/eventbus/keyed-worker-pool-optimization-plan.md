# jxt-core EventBus Keyed-Worker æ± ä¼˜åŒ–æ–¹æ¡ˆ (Phase 1)

## ğŸ¯ **è®¾è®¡ç›®æ ‡**

- âœ… **100% é¡ºåºä¿è¯**ï¼šåŒä¸€èšåˆIDçš„é¢†åŸŸäº‹ä»¶ä¸¥æ ¼æŒ‰é¡ºåºå¤„ç†
- âœ… **æ¶æ„ç®€åŒ–**ï¼šå›ºå®šå¤§å°çš„ Worker æ± ï¼Œé¿å…å¤æ‚çš„åŠ¨æ€ç®¡ç†
- âœ… **èµ„æºå¯æ§**ï¼šæœ‰ç•Œå¹¶å‘ï¼Œå†…å­˜å’ŒCPUä½¿ç”¨ç¨³å®šå¯é¢„æµ‹
- âœ… **é«˜å¯ç”¨æ€§**ï¼šæ”¯æŒèƒŒå‹æ§åˆ¶ã€æ¯’ä¸¸å¤„ç†ã€æ•°æ®å®Œæ•´æ€§æ ¡éªŒ
- âœ… **æ˜“äºè¿ç»´**ï¼šç®€å•çš„é…ç½®å’Œç›‘æ§ï¼Œæ•…éšœæ’æŸ¥å®¹æ˜“

## ğŸ—ï¸ **æ ¸å¿ƒæ¶æ„è®¾è®¡**

### 1. **Keyed-Worker æ± æ¶æ„**

```go
// æ ¸å¿ƒæ¶æ„ç»„ä»¶
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Message       â”‚    â”‚  Consistent      â”‚    â”‚  Keyed-Worker   â”‚
â”‚   Router        â”‚â”€â”€â”€â–¶â”‚  Hash Router     â”‚â”€â”€â”€â–¶â”‚  Pool (M=1024)  â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                        â”‚
                                â–¼                        â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  aggregateId     â”‚    â”‚  Worker[0..M-1] â”‚
                       â”‚  hash % M        â”‚    â”‚  æ¯ä¸ªWorker:    â”‚
                       â”‚  = workerIndex   â”‚    â”‚  - æœ‰ç•Œé˜Ÿåˆ—     â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  - ä¸²è¡Œå¤„ç†     â”‚
                                               â”‚  - èƒŒå‹æ§åˆ¶     â”‚
                                               â”‚  - é¡ºåºæ ¡éªŒ     â”‚
                                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. **Worker å†…éƒ¨ç»“æ„**

```go
type KeyedWorker struct {
    // åŸºç¡€å±æ€§
    id           int
    queue        chan *OrderedMessage
    queueSize    int
    
    // é¡ºåºæ§åˆ¶
    lastVersions map[string]int64  // aggregateId -> lastEventVersion
    waitingMsgs  map[string][]*OrderedMessage  // ç­‰å¾…å‰é©±æ¶ˆæ¯çš„é˜Ÿåˆ—
    
    // èƒŒå‹æ§åˆ¶
    pausedKeys   map[string]bool   // æš‚åœå¤„ç†çš„èšåˆID
    
    // æ¯’ä¸¸å¤„ç†
    retryBudgets map[string]*RetryBudget
    dlqSender    DLQSender
    
    // ç”Ÿå‘½å‘¨æœŸ
    ctx          context.Context
    cancel       context.CancelFunc
    wg           sync.WaitGroup
    
    // ç›‘æ§ç»Ÿè®¡
    stats        *WorkerStats
}

type OrderedMessage struct {
    AggregateID    string
    EventVersion   int64     // äº‹ä»¶ç‰ˆæœ¬å·ï¼Œç¡®ä¿é¡ºåº
    Payload        []byte
    Headers        map[string][]byte
    Timestamp      time.Time
    
    // å¤„ç†æ§åˆ¶
    Handler        MessageHandler
    Done           chan error
    RetryCount     int
    
    // å…ƒæ•°æ®
    SourceTopic    string
    Partition      int32     // Kafkaåˆ†åŒº
    Offset         int64     // Kafkaåç§»é‡
    Sequence       uint64    // NATSåºåˆ—å·
}
```

### 3. **ä¸€è‡´æ€§å“ˆå¸Œè·¯ç”±**

```go
type ConsistentHashRouter struct {
    workerCount  int
    hashFunc     hash.Hash32
    mu           sync.RWMutex
}

func (chr *ConsistentHashRouter) RouteMessage(aggregateID string) int {
    chr.mu.RLock()
    defer chr.mu.RUnlock()
    
    chr.hashFunc.Reset()
    chr.hashFunc.Write([]byte(aggregateID))
    hashValue := chr.hashFunc.Sum32()
    
    return int(hashValue % uint32(chr.workerCount))
}

// ç¡®ä¿ç›¸åŒèšåˆIDæ€»æ˜¯è·¯ç”±åˆ°åŒä¸€ä¸ªWorker
func (chr *ConsistentHashRouter) GetWorkerForAggregate(aggregateID string) int {
    return chr.RouteMessage(aggregateID)
}
```

---

## ğŸ”§ **æ ¸å¿ƒåŠŸèƒ½å®ç°**

### 1. **Keyed-Worker æ± ç®¡ç†å™¨**

```go
type KeyedWorkerPoolManager struct {
    // Workeræ± 
    workers      []*KeyedWorker
    workerCount  int
    
    // è·¯ç”±å™¨
    router       *ConsistentHashRouter
    
    // èƒŒå‹æ§åˆ¶
    backpressure *BackpressureController
    
    // é…ç½®
    config       *KeyedWorkerConfig
    
    // ç”Ÿå‘½å‘¨æœŸ
    ctx          context.Context
    cancel       context.CancelFunc
    wg           sync.WaitGroup
    
    // ç›‘æ§
    metrics      *PoolMetrics
}

type KeyedWorkerConfig struct {
    // Workeræ± é…ç½®
    WorkerCount    int           `yaml:"workerCount"`    // å›ºå®šWorkeræ•°é‡ï¼Œå¦‚1024
    QueueSize      int           `yaml:"queueSize"`      // æ¯ä¸ªWorkeré˜Ÿåˆ—å¤§å°ï¼Œå¦‚1000
    
    // é¡ºåºæ§åˆ¶é…ç½®
    EnableVersionCheck bool      `yaml:"enableVersionCheck"` // å¯ç”¨ç‰ˆæœ¬é¡ºåºæ£€æŸ¥
    WaitTimeout       time.Duration `yaml:"waitTimeout"`     // ç­‰å¾…å‰é©±æ¶ˆæ¯è¶…æ—¶
    
    // èƒŒå‹æ§åˆ¶é…ç½®
    BackpressureThreshold float64 `yaml:"backpressureThreshold"` // èƒŒå‹è§¦å‘é˜ˆå€¼ï¼Œå¦‚0.8
    BackpressureStrategy  string  `yaml:"backpressureStrategy"`  // kafka_pause/nats_flow_control
    
    // æ¯’ä¸¸å¤„ç†é…ç½®
    MaxRetries        int           `yaml:"maxRetries"`        // æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œå¦‚3
    RetryBackoffBase  time.Duration `yaml:"retryBackoffBase"`  // é‡è¯•é€€é¿åŸºæ•°ï¼Œå¦‚100ms
    RetryBackoffMax   time.Duration `yaml:"retryBackoffMax"`   // æœ€å¤§é€€é¿æ—¶é—´ï¼Œå¦‚30s
    DLQEnabled        bool          `yaml:"dlqEnabled"`        // å¯ç”¨æ­»ä¿¡é˜Ÿåˆ—
    DLQTopic          string        `yaml:"dlqTopic"`          // æ­»ä¿¡é˜Ÿåˆ—ä¸»é¢˜
}

func NewKeyedWorkerPoolManager(config *KeyedWorkerConfig) (*KeyedWorkerPoolManager, error) {
    ctx, cancel := context.WithCancel(context.Background())
    
    // åˆ›å»ºä¸€è‡´æ€§å“ˆå¸Œè·¯ç”±å™¨
    router := &ConsistentHashRouter{
        workerCount: config.WorkerCount,
        hashFunc:    fnv.New32a(),
    }
    
    // åˆ›å»ºWorkeræ± 
    workers := make([]*KeyedWorker, config.WorkerCount)
    for i := 0; i < config.WorkerCount; i++ {
        worker, err := NewKeyedWorker(i, config, ctx)
        if err != nil {
            cancel()
            return nil, fmt.Errorf("failed to create worker %d: %w", i, err)
        }
        workers[i] = worker
    }
    
    // åˆ›å»ºèƒŒå‹æ§åˆ¶å™¨
    backpressure := NewBackpressureController(config, workers)
    
    manager := &KeyedWorkerPoolManager{
        workers:      workers,
        workerCount:  config.WorkerCount,
        router:       router,
        backpressure: backpressure,
        config:       config,
        ctx:          ctx,
        cancel:       cancel,
        metrics:      NewPoolMetrics(),
    }
    
    // å¯åŠ¨æ‰€æœ‰Worker
    for _, worker := range workers {
        worker.Start()
    }
    
    // å¯åŠ¨èƒŒå‹ç›‘æ§
    manager.startBackpressureMonitoring()
    
    return manager, nil
}

func (kwpm *KeyedWorkerPoolManager) ProcessMessage(msg *OrderedMessage) error {
    // 1. è·¯ç”±åˆ°å¯¹åº”çš„Worker
    workerIndex := kwpm.router.RouteMessage(msg.AggregateID)
    worker := kwpm.workers[workerIndex]
    
    // 2. æ£€æŸ¥Workeræ˜¯å¦æš‚åœ
    if worker.IsPaused(msg.AggregateID) {
        return ErrWorkerPaused
    }
    
    // 3. å°è¯•å…¥é˜Ÿ
    select {
    case worker.queue <- msg:
        kwpm.metrics.MessagesEnqueued.Inc()
        return nil
    default:
        // é˜Ÿåˆ—æ»¡ï¼Œè§¦å‘èƒŒå‹
        kwpm.backpressure.HandleQueueFull(workerIndex, msg.AggregateID)
        return ErrQueueFull
    }
}
```

### 2. **Worker æ ¸å¿ƒå¤„ç†é€»è¾‘**

```go
func (kw *KeyedWorker) Start() {
    kw.wg.Add(1)
    go kw.processLoop()
}

func (kw *KeyedWorker) processLoop() {
    defer kw.wg.Done()
    
    for {
        select {
        case msg := <-kw.queue:
            kw.processMessage(msg)
            
        case <-kw.ctx.Done():
            return
        }
    }
}

func (kw *KeyedWorker) processMessage(msg *OrderedMessage) {
    defer func() {
        if r := recover(); r != nil {
            kw.handlePanic(msg, r)
        }
    }()
    
    // 1. æ£€æŸ¥æ˜¯å¦æš‚åœå¤„ç†è¯¥èšåˆID
    if kw.pausedKeys[msg.AggregateID] {
        kw.requeueMessage(msg)
        return
    }
    
    // 2. é¡ºåºæ ¡éªŒ
    if kw.config.EnableVersionCheck {
        if !kw.checkMessageOrder(msg) {
            return // æ¶ˆæ¯è¢«ç¼“å­˜ç­‰å¾…å‰é©±
        }
    }
    
    // 3. å¤„ç†æ¶ˆæ¯
    err := kw.handleMessage(msg)
    
    // 4. å¤„ç†ç»“æœ
    if err != nil {
        kw.handleError(msg, err)
    } else {
        kw.handleSuccess(msg)
    }
    
    // 5. å¤„ç†ç­‰å¾…é˜Ÿåˆ—ä¸­çš„åç»­æ¶ˆæ¯
    if kw.config.EnableVersionCheck {
        kw.processWaitingMessages(msg.AggregateID)
    }
}

func (kw *KeyedWorker) checkMessageOrder(msg *OrderedMessage) bool {
    lastVersion, exists := kw.lastVersions[msg.AggregateID]
    
    if !exists {
        // ç¬¬ä¸€æ¡æ¶ˆæ¯ï¼Œç›´æ¥å¤„ç†
        return true
    }
    
    expectedVersion := lastVersion + 1
    
    if msg.EventVersion == expectedVersion {
        // é¡ºåºæ­£ç¡®ï¼Œå¯ä»¥å¤„ç†
        return true
    } else if msg.EventVersion > expectedVersion {
        // æ¶ˆæ¯è¶…å‰ï¼Œéœ€è¦ç­‰å¾…å‰é©±æ¶ˆæ¯
        kw.addToWaitingQueue(msg)
        return false
    } else {
        // æ¶ˆæ¯é‡å¤æˆ–è¿‡æœŸï¼Œå‘é€åˆ°DLQ
        kw.sendToDLQ(msg, fmt.Errorf("duplicate or outdated message: expected %d, got %d", 
            expectedVersion, msg.EventVersion))
        return false
    }
}

func (kw *KeyedWorker) handleMessage(msg *OrderedMessage) error {
    // æ‰§è¡Œä¸šåŠ¡å¤„ç†é€»è¾‘
    return msg.Handler(msg.Payload, msg.Headers)
}

func (kw *KeyedWorker) handleError(msg *OrderedMessage, err error) {
    // è·å–é‡è¯•é¢„ç®—
    budget := kw.getRetryBudget(msg.AggregateID)
    
    if budget.CanRetry() {
        // æŒ‡æ•°é€€é¿é‡è¯•
        backoffDuration := kw.calculateBackoff(msg.RetryCount)
        
        time.AfterFunc(backoffDuration, func() {
            msg.RetryCount++
            budget.UseRetry()
            
            select {
            case kw.queue <- msg:
                // é‡æ–°å…¥é˜ŸæˆåŠŸ
            default:
                // é˜Ÿåˆ—æ»¡ï¼Œç›´æ¥å‘é€åˆ°DLQ
                kw.sendToDLQ(msg, fmt.Errorf("retry queue full: %w", err))
            }
        })
    } else {
        // é‡è¯•é¢„ç®—è€—å°½ï¼Œå‘é€åˆ°DLQ
        kw.sendToDLQ(msg, fmt.Errorf("retry budget exhausted: %w", err))
    }
}

func (kw *KeyedWorker) handleSuccess(msg *OrderedMessage) {
    // æ›´æ–°ç‰ˆæœ¬å·
    if kw.config.EnableVersionCheck {
        kw.lastVersions[msg.AggregateID] = msg.EventVersion
    }
    
    // é‡ç½®é‡è¯•é¢„ç®—
    kw.resetRetryBudget(msg.AggregateID)
    
    // é€šçŸ¥å¤„ç†å®Œæˆ
    if msg.Done != nil {
        msg.Done <- nil
    }
    
    // æ›´æ–°ç»Ÿè®¡
    kw.stats.MessagesProcessed.Inc()
}
```

### 3. **èƒŒå‹æ§åˆ¶æœºåˆ¶**

```go
type BackpressureController struct {
    config      *KeyedWorkerConfig
    workers     []*KeyedWorker
    
    // KafkaèƒŒå‹æ§åˆ¶
    kafkaConsumer KafkaConsumerController
    
    // NATSèƒŒå‹æ§åˆ¶
    natsConsumer  NATSConsumerController
    
    // ç›‘æ§
    metrics       *BackpressureMetrics
}

func (bc *BackpressureController) HandleQueueFull(workerIndex int, aggregateID string) {
    worker := bc.workers[workerIndex]
    
    // è®¡ç®—é˜Ÿåˆ—ä½¿ç”¨ç‡
    queueUsage := float64(len(worker.queue)) / float64(cap(worker.queue))
    
    if queueUsage >= bc.config.BackpressureThreshold {
        switch bc.config.BackpressureStrategy {
        case "kafka_pause":
            bc.handleKafkaBackpressure(workerIndex, aggregateID)
        case "nats_flow_control":
            bc.handleNATSBackpressure(workerIndex, aggregateID)
        }
    }
}

func (bc *BackpressureController) handleKafkaBackpressure(workerIndex int, aggregateID string) {
    // æš‚åœç›¸å…³åˆ†åŒºçš„æ¶ˆè´¹
    partitions := bc.kafkaConsumer.GetPartitionsForAggregate(aggregateID)
    
    for _, partition := range partitions {
        bc.kafkaConsumer.PausePartition(partition)
        bc.metrics.PausedPartitions.Inc()
    }
    
    // å¯åŠ¨æ¢å¤ç›‘æ§
    go bc.monitorKafkaRecovery(workerIndex, aggregateID, partitions)
}

func (bc *BackpressureController) handleNATSBackpressure(workerIndex int, aggregateID string) {
    // é™ä½æ‹‰å–é€Ÿç‡
    bc.natsConsumer.ReduceFetchRate(aggregateID)
    
    // æ”¶ç´§MaxAckPending
    bc.natsConsumer.SetMaxAckPending(aggregateID, 10) // é™ä½åˆ°10
    
    // å¯ç”¨Flow Control
    bc.natsConsumer.EnableFlowControl(aggregateID)
    
    bc.metrics.FlowControlActivated.Inc()
    
    // å¯åŠ¨æ¢å¤ç›‘æ§
    go bc.monitorNATSRecovery(workerIndex, aggregateID)
}

func (bc *BackpressureController) monitorKafkaRecovery(workerIndex int, aggregateID string, partitions []int32) {
    ticker := time.NewTicker(5 * time.Second)
    defer ticker.Stop()
    
    for range ticker.C {
        worker := bc.workers[workerIndex]
        queueUsage := float64(len(worker.queue)) / float64(cap(worker.queue))
        
        if queueUsage < 0.5 { // é˜Ÿåˆ—ä½¿ç”¨ç‡é™åˆ°50%ä»¥ä¸‹
            // æ¢å¤åˆ†åŒºæ¶ˆè´¹
            for _, partition := range partitions {
                bc.kafkaConsumer.ResumePartition(partition)
                bc.metrics.ResumedPartitions.Inc()
            }
            return
        }
    }
}
```

---

## ğŸ“‹ **é…ç½®ç¤ºä¾‹**

### 1. **ç”Ÿäº§ç¯å¢ƒé…ç½®**

```yaml
# config/keyed-worker-pool-production.yaml
eventbus:
  type: kafka
  kafka:
    brokers: ["kafka-1:9092", "kafka-2:9092", "kafka-3:9092"]
    consumerGroup: "order-service"
  
  # Keyed-Workeræ± é…ç½®
  keyedWorkerPool:
    # Workeræ± åŸºç¡€é…ç½®
    workerCount: 1024              # å›ºå®š1024ä¸ªWorker
    queueSize: 1000                # æ¯ä¸ªWorkeré˜Ÿåˆ—1000æ¡æ¶ˆæ¯
    
    # é¡ºåºæ§åˆ¶é…ç½®
    enableVersionCheck: true       # å¯ç”¨ç‰ˆæœ¬é¡ºåºæ£€æŸ¥
    waitTimeout: 30s               # ç­‰å¾…å‰é©±æ¶ˆæ¯30ç§’è¶…æ—¶
    
    # èƒŒå‹æ§åˆ¶é…ç½®
    backpressureThreshold: 0.8     # é˜Ÿåˆ—80%æ»¡æ—¶è§¦å‘èƒŒå‹
    backpressureStrategy: "kafka_pause"  # Kafkaæš‚åœåˆ†åŒºç­–ç•¥
    
    # æ¯’ä¸¸å¤„ç†é…ç½®
    maxRetries: 3                  # æœ€å¤§é‡è¯•3æ¬¡
    retryBackoffBase: 100ms        # åŸºç¡€é€€é¿100ms
    retryBackoffMax: 30s           # æœ€å¤§é€€é¿30ç§’
    dlqEnabled: true               # å¯ç”¨æ­»ä¿¡é˜Ÿåˆ—
    dlqTopic: "order-events-dlq"   # æ­»ä¿¡é˜Ÿåˆ—ä¸»é¢˜
    
    # ç›‘æ§é…ç½®
    metricsEnabled: true           # å¯ç”¨æŒ‡æ ‡æ”¶é›†
    metricsInterval: 30s           # æŒ‡æ ‡æ”¶é›†é—´éš”
```

### 2. **å¼€å‘ç¯å¢ƒé…ç½®**

```yaml
# config/keyed-worker-pool-development.yaml
eventbus:
  type: kafka
  kafka:
    brokers: ["localhost:9092"]
    consumerGroup: "dev-order-service"
  
  keyedWorkerPool:
    workerCount: 64                # å¼€å‘ç¯å¢ƒè¾ƒå°‘Worker
    queueSize: 100                 # è¾ƒå°é˜Ÿåˆ—ä¾¿äºæµ‹è¯•
    
    enableVersionCheck: true       # ä¿æŒé¡ºåºæ£€æŸ¥
    waitTimeout: 10s               # è¾ƒçŸ­è¶…æ—¶ä¾¿äºè°ƒè¯•
    
    backpressureThreshold: 0.7     # æ›´æ—©è§¦å‘èƒŒå‹ä¾¿äºæµ‹è¯•
    backpressureStrategy: "kafka_pause"
    
    maxRetries: 2                  # è¾ƒå°‘é‡è¯•ä¾¿äºè°ƒè¯•
    retryBackoffBase: 50ms
    retryBackoffMax: 5s
    dlqEnabled: true
    dlqTopic: "dev-order-events-dlq"
    
    metricsEnabled: true
    metricsInterval: 10s           # æ›´é¢‘ç¹çš„æŒ‡æ ‡æ”¶é›†
```

---

## ğŸ“Š **æ€§èƒ½é¢„æœŸ**

### 1. **å¹¶å‘å¤„ç†èƒ½åŠ›**

```go
// åŸºäºå›ºå®šWorkeræ± çš„å¹¶å‘èƒ½åŠ›
Workeræ•°é‡: 1024ä¸ª
æ¯Workeré˜Ÿåˆ—: 1000æ¡æ¶ˆæ¯
ç†è®ºå¹¶å‘æ¶ˆæ¯: 1,024,000æ¡

// å®é™…èšåˆIDå¹¶å‘å¤„ç†èƒ½åŠ›
åŒæ—¶å¤„ç†çš„èšåˆIDæ•°é‡: æ— é™åˆ¶ (é€šè¿‡å“ˆå¸Œåˆ†æ•£)
æ¯ä¸ªèšåˆID: ä¸¥æ ¼ä¸²è¡Œå¤„ç†
ä¸åŒèšåˆID: å®Œå…¨å¹¶å‘å¤„ç†

// èµ„æºä½¿ç”¨é¢„æµ‹
å†…å­˜å ç”¨: ~200MB (1024 * 1000 * 200å­—èŠ‚/æ¶ˆæ¯)
Goroutineæ•°é‡: 1024ä¸ª (æ¯Workerä¸€ä¸ª)
CPUä½¿ç”¨: å¯æ§ä¸”ç¨³å®š
```

### 2. **æ€§èƒ½åŸºå‡†**

```go
// é¢„æœŸæ€§èƒ½æŒ‡æ ‡
ååé‡: 100,000+ TPS
å»¶è¿Ÿ: P99 < 50ms
å†…å­˜ä½¿ç”¨: ç¨³å®šåœ¨200MBå·¦å³
CPUä½¿ç”¨: 60-80%
é¡ºåºå‡†ç¡®æ€§: 100%
å¯ç”¨æ€§: 99.9%+
```

è¿™ä¸ª Phase 1 æ–¹æ¡ˆé€šè¿‡å›ºå®šå¤§å°çš„ Keyed-Worker æ± ï¼Œæ—¢ä¿è¯äº†åŒä¸€èšåˆIDçš„ä¸¥æ ¼é¡ºåºå¤„ç†ï¼Œåˆå¤§å¤§ç®€åŒ–äº†æ¶æ„å¤æ‚åº¦ï¼Œèµ„æºä½¿ç”¨ç¨³å®šå¯æ§ï¼Œæ˜¯ä¸€ä¸ªéå¸¸å®ç”¨çš„è½åœ°æ–¹æ¡ˆã€‚
