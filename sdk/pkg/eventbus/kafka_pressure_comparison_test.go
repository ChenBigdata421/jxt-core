package eventbus

import (
	"context"
	"encoding/json"
	"fmt"
	"runtime"
	"sync/atomic"
	"testing"
	"time"
)

// KafkaPressureMetrics Kafkaå‹åŠ›æµ‹è¯•æŒ‡æ ‡
type KafkaPressureMetrics struct {
	TestName          string
	MessageCount      int64
	ReceivedCount     int64
	Errors            int64
	Duration          time.Duration
	Throughput        float64
	SuccessRate       float64
	SendRate          float64
	SendDuration      time.Duration
	FirstMessageTime  time.Time
	LastMessageTime   time.Time
	ProcessingLatency time.Duration
	InitialGoroutines int
	PeakGoroutines    int
	InitialMemory     uint64
	PeakMemory        uint64
}

// TestKafkaPressureComparison Kafkaä½å‹ã€ä¸­å‹ã€é«˜å‹å¯¹æ¯”æµ‹è¯•
func TestKafkaPressureComparison(t *testing.T) {
	t.Logf("ğŸš€ Starting Kafka Pressure Comparison Test...")

	// æµ‹è¯•åœºæ™¯å®šä¹‰
	testScenarios := []struct {
		name         string
		messageCount int
		timeout      time.Duration
		description  string
	}{
		{"Low", 1000, 3 * time.Minute, "ä½å‹åŠ›æµ‹è¯• - 1000æ¡æ¶ˆæ¯"},
		{"Medium", 3000, 5 * time.Minute, "ä¸­å‹åŠ›æµ‹è¯• - 3000æ¡æ¶ˆæ¯"},
		{"High", 6000, 8 * time.Minute, "é«˜å‹åŠ›æµ‹è¯• - 6000æ¡æ¶ˆæ¯"},
	}

	// å­˜å‚¨æ‰€æœ‰æµ‹è¯•ç»“æœ
	results := make(map[string]*KafkaPressureMetrics)

	for _, scenario := range testScenarios {
		t.Logf("\nğŸ¯ ===== %s =====", scenario.description)

		// è¿è¡ŒKafkaå‹åŠ›æµ‹è¯•
		metrics := runKafkaPressureTest(t, scenario.name, scenario.messageCount, scenario.timeout)
		results[scenario.name] = metrics

		// ç­‰å¾…ä¸€ä¸‹ï¼Œå‡†å¤‡ä¸‹ä¸€ä¸ªæµ‹è¯•
		time.Sleep(10 * time.Second)
	}

	// ç”Ÿæˆå‹åŠ›æµ‹è¯•å¯¹æ¯”æŠ¥å‘Š
	generatePressureComparisonReport(t, results)
}

// runKafkaPressureTest è¿è¡ŒKafkaå‹åŠ›æµ‹è¯•
func runKafkaPressureTest(t *testing.T, testName string, messageCount int, timeout time.Duration) *KafkaPressureMetrics {
	// Kafkaé…ç½® - ä½¿ç”¨ä¼˜åŒ–åçš„å¹³è¡¡é…ç½®
	config := &KafkaConfig{
		Brokers:  []string{"localhost:29094"}, // RedPanda
		ClientID: fmt.Sprintf("kafka-pressure-%s", testName),
		Producer: ProducerConfig{
			MaxMessageBytes: 1024 * 1024,
			RequiredAcks:    1,
			Timeout:         5 * time.Second,
			Compression:     "", // è‡ªåŠ¨ä¼˜åŒ–ä¸ºsnappy
			MaxInFlight:     0,  // è‡ªåŠ¨ä¼˜åŒ–ä¸º50
		},
		Consumer: ConsumerConfig{
			GroupID:            fmt.Sprintf("kafka-pressure-%s-group", testName),
			AutoOffsetReset:    "earliest",
			SessionTimeout:     6 * time.Second,        // å¹³è¡¡é…ç½®
			HeartbeatInterval:  2 * time.Second,        // å¹³è¡¡é…ç½®
			MaxProcessingTime:  5 * time.Second,        // å¹³è¡¡é…ç½®
			FetchMinBytes:      1024 * 1024,            // 1MB
			FetchMaxBytes:      50 * 1024 * 1024,       // 50MB
			FetchMaxWait:       100 * time.Millisecond, // å¹³è¡¡é…ç½®
			MaxPollRecords:     2000,                   // å¤§æ‰¹é‡
			EnableAutoCommit:   true,
			AutoCommitInterval: 500 * time.Millisecond,
		},
	}

	eventBus, err := NewKafkaEventBus(config)
	if err != nil {
		t.Fatalf("Failed to create Kafka EventBus: %v", err)
	}
	defer eventBus.Close()

	// è®¾ç½®é¢„è®¢é˜…topic
	kafkaBus := eventBus.(*kafkaEventBus)
	testTopic := fmt.Sprintf("kafka.pressure.%s.test", testName)
	kafkaBus.allPossibleTopics = []string{testTopic}

	ctx, cancel := context.WithTimeout(context.Background(), timeout)
	defer cancel()

	// åˆå§‹åŒ–æŒ‡æ ‡
	metrics := &KafkaPressureMetrics{
		TestName:          testName,
		MessageCount:      int64(messageCount),
		InitialGoroutines: runtime.NumGoroutine(),
	}

	// è·å–åˆå§‹å†…å­˜
	var m runtime.MemStats
	runtime.ReadMemStats(&m)
	metrics.InitialMemory = m.Alloc

	// æ¶ˆæ¯å¤„ç†ç»Ÿè®¡
	var receivedCount int64

	// æ¶ˆæ¯å¤„ç†å™¨
	handler := func(ctx context.Context, message []byte) error {
		receiveTime := time.Now()
		count := atomic.AddInt64(&receivedCount, 1)

		if count == 1 {
			metrics.FirstMessageTime = receiveTime
		}
		metrics.LastMessageTime = receiveTime

		return nil
	}

	// è®¢é˜…topicï¼ˆåŒ…å«é¢„çƒ­ï¼‰
	err = eventBus.Subscribe(ctx, testTopic, handler)
	if err != nil {
		t.Fatalf("Failed to subscribe to Kafka: %v", err)
	}

	t.Logf("ğŸš€ Starting Kafka %s pressure test with %d messages", testName, messageCount)

	// å‘é€æ¶ˆæ¯
	sendStart := time.Now()
	errors := 0
	for i := 0; i < messageCount; i++ {
		message := map[string]interface{}{
			"id":       fmt.Sprintf("kafka-%s-msg-%d", testName, i),
			"content":  fmt.Sprintf("Kafka %s pressure test message %d", testName, i),
			"sendTime": time.Now().Format(time.RFC3339Nano),
			"index":    i,
			"testName": testName,
		}

		messageBytes, _ := json.Marshal(message)
		err := eventBus.Publish(ctx, testTopic, messageBytes)
		if err != nil {
			errors++
			if errors <= 5 {
				t.Logf("Kafka publish error %d: %v", errors, err)
			}
		}
	}
	sendEnd := time.Now()
	metrics.SendDuration = sendEnd.Sub(sendStart)
	metrics.SendRate = float64(messageCount-errors) / metrics.SendDuration.Seconds()

	t.Logf("ğŸ“¤ Kafka sent %d messages in %.2f seconds (%.2f msg/s)",
		messageCount-errors, metrics.SendDuration.Seconds(), metrics.SendRate)

	// ç­‰å¾…æ¶ˆæ¯å¤„ç†
	waitTime := timeout - time.Since(sendStart) - 15*time.Second
	if waitTime > 0 {
		t.Logf("â³ Kafka waiting %.0f seconds for message processing...", waitTime.Seconds())
		time.Sleep(waitTime)
	}

	// æ”¶é›†æœ€ç»ˆæŒ‡æ ‡
	endTime := time.Now()
	metrics.Duration = endTime.Sub(sendStart)
	metrics.ReceivedCount = atomic.LoadInt64(&receivedCount)
	metrics.Errors = int64(errors)
	metrics.SuccessRate = float64(metrics.ReceivedCount) / float64(metrics.MessageCount) * 100
	metrics.Throughput = float64(metrics.ReceivedCount) / metrics.Duration.Seconds()

	// è®¡ç®—å¤„ç†å»¶è¿Ÿ
	if !metrics.FirstMessageTime.IsZero() && !metrics.LastMessageTime.IsZero() {
		metrics.ProcessingLatency = metrics.LastMessageTime.Sub(metrics.FirstMessageTime)
	}

	// è·å–å³°å€¼èµ„æºä½¿ç”¨
	metrics.PeakGoroutines = runtime.NumGoroutine()
	runtime.ReadMemStats(&m)
	metrics.PeakMemory = m.Alloc

	// è¾“å‡ºæµ‹è¯•ç»“æœ
	t.Logf("ğŸ“Š Kafka %s Pressure Results:", testName)
	t.Logf("   ğŸ“¤ Messages sent: %d", messageCount-errors)
	t.Logf("   ğŸ“¥ Messages received: %d", metrics.ReceivedCount)
	t.Logf("   âœ… Success rate: %.2f%%", metrics.SuccessRate)
	t.Logf("   ğŸš€ Send rate: %.2f msg/s", metrics.SendRate)
	t.Logf("   ğŸš€ Throughput: %.2f msg/s", metrics.Throughput)
	t.Logf("   â±ï¸  Processing latency: %v", metrics.ProcessingLatency)
	t.Logf("   âš ï¸ Errors: %d", metrics.Errors)
	t.Logf("   ğŸ”§ Goroutines: %d â†’ %d (+%d)",
		metrics.InitialGoroutines, metrics.PeakGoroutines,
		metrics.PeakGoroutines-metrics.InitialGoroutines)
	t.Logf("   ğŸ’¾ Memory: %.2f MB â†’ %.2f MB (+%.2f MB)",
		float64(metrics.InitialMemory)/1024/1024,
		float64(metrics.PeakMemory)/1024/1024,
		float64(metrics.PeakMemory-metrics.InitialMemory)/1024/1024)

	return metrics
}

// generatePressureComparisonReport ç”Ÿæˆå‹åŠ›æµ‹è¯•å¯¹æ¯”æŠ¥å‘Š
func generatePressureComparisonReport(t *testing.T, results map[string]*KafkaPressureMetrics) {
	t.Logf("\nğŸ† ===== KAFKA PRESSURE COMPARISON REPORT =====")

	scenarios := []string{"Low", "Medium", "High"}

	// è¡¨æ ¼å¤´éƒ¨
	t.Logf("ğŸ“Š Kafka Pressure Performance Table:")
	t.Logf("%-12s | %-12s | %-12s | %-12s | %-12s | %-12s | %-12s",
		"Pressure", "Messages", "Success Rate", "Send Rate", "Throughput", "Latency", "Goroutines")
	t.Logf("%-12s-+-%-12s-+-%-12s-+-%-12s-+-%-12s-+-%-12s-+-%-12s",
		"------------", "------------", "------------", "------------", "------------", "------------", "------------")

	for _, scenario := range scenarios {
		metrics := results[scenario]
		if metrics != nil {
			goroutineIncrease := metrics.PeakGoroutines - metrics.InitialGoroutines
			t.Logf("%-12s | %10d | %10.2f%% | %9.2f/s | %9.2f/s | %9.0fms | %9d",
				scenario,
				metrics.MessageCount,
				metrics.SuccessRate,
				metrics.SendRate,
				metrics.Throughput,
				float64(metrics.ProcessingLatency.Nanoseconds())/1000000,
				goroutineIncrease)
		}
	}

	// è¯¦ç»†åˆ†æ
	t.Logf("\nğŸ“ˆ Detailed Pressure Analysis:")

	for _, scenario := range scenarios {
		metrics := results[scenario]
		if metrics != nil {
			t.Logf("\nğŸ¯ %s Pressure Analysis:", scenario)

			// æ€§èƒ½è¯„ä¼°
			if metrics.SuccessRate >= 90 {
				t.Logf("   âœ… Success Rate: Excellent (%.2f%%)", metrics.SuccessRate)
			} else if metrics.SuccessRate >= 70 {
				t.Logf("   âš ï¸ Success Rate: Good (%.2f%%)", metrics.SuccessRate)
			} else {
				t.Logf("   âŒ Success Rate: Poor (%.2f%%)", metrics.SuccessRate)
			}

			if metrics.SendRate >= 100 {
				t.Logf("   ğŸš€ Send Rate: High Performance (%.2f msg/s)", metrics.SendRate)
			} else if metrics.SendRate >= 10 {
				t.Logf("   ğŸ“Š Send Rate: Moderate Performance (%.2f msg/s)", metrics.SendRate)
			} else {
				t.Logf("   âš ï¸ Send Rate: Low Performance (%.2f msg/s)", metrics.SendRate)
			}

			if metrics.Throughput >= 10 {
				t.Logf("   ğŸ“ˆ Throughput: Good (%.2f msg/s)", metrics.Throughput)
			} else if metrics.Throughput >= 5 {
				t.Logf("   ğŸ“Š Throughput: Moderate (%.2f msg/s)", metrics.Throughput)
			} else {
				t.Logf("   âš ï¸ Throughput: Low (%.2f msg/s)", metrics.Throughput)
			}

			// èµ„æºä½¿ç”¨åˆ†æ
			goroutineIncrease := metrics.PeakGoroutines - metrics.InitialGoroutines
			memoryIncrease := float64(metrics.PeakMemory-metrics.InitialMemory) / 1024 / 1024

			if goroutineIncrease < 100 {
				t.Logf("   ğŸ”§ Resource Usage: Efficient (+%d goroutines, +%.2f MB)", goroutineIncrease, memoryIncrease)
			} else {
				t.Logf("   âš ï¸ Resource Usage: High (+%d goroutines, +%.2f MB)", goroutineIncrease, memoryIncrease)
			}
		}
	}

	// å‹åŠ›çº§åˆ«å¯¹æ¯”
	t.Logf("\nğŸ“Š Pressure Level Comparison:")

	lowMetrics := results["Low"]
	mediumMetrics := results["Medium"]
	highMetrics := results["High"]

	if lowMetrics != nil && mediumMetrics != nil && highMetrics != nil {
		t.Logf("   ğŸ“ˆ Success Rate Trend: Low(%.1f%%) â†’ Medium(%.1f%%) â†’ High(%.1f%%)",
			lowMetrics.SuccessRate, mediumMetrics.SuccessRate, highMetrics.SuccessRate)

		t.Logf("   ğŸš€ Send Rate Trend: Low(%.1f) â†’ Medium(%.1f) â†’ High(%.1f) msg/s",
			lowMetrics.SendRate, mediumMetrics.SendRate, highMetrics.SendRate)

		t.Logf("   ğŸ“Š Throughput Trend: Low(%.1f) â†’ Medium(%.1f) â†’ High(%.1f) msg/s",
			lowMetrics.Throughput, mediumMetrics.Throughput, highMetrics.Throughput)

		// æ€§èƒ½è¶‹åŠ¿åˆ†æ
		if highMetrics.SuccessRate >= lowMetrics.SuccessRate*0.8 {
			t.Logf("   âœ… Scalability: Good - maintains performance under pressure")
		} else {
			t.Logf("   âš ï¸ Scalability: Limited - performance degrades under pressure")
		}

		// æœ€ä½³å‹åŠ›çº§åˆ«æ¨è
		bestScenario := "Low"
		bestScore := lowMetrics.SuccessRate + lowMetrics.Throughput

		mediumScore := mediumMetrics.SuccessRate + mediumMetrics.Throughput
		if mediumScore > bestScore {
			bestScenario = "Medium"
			bestScore = mediumScore
		}

		highScore := highMetrics.SuccessRate + highMetrics.Throughput
		if highScore > bestScore {
			bestScenario = "High"
		}

		t.Logf("   ğŸ† Recommended Pressure Level: %s (best overall performance)", bestScenario)
	}

	t.Logf("\nğŸ¯ Kafka pressure comparison test completed!")
}
