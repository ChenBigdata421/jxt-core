package eventbus

import (
	"context"
	"fmt"
	"strings"
	"sync"
	"sync/atomic"
	"time"

	"github.com/ChenBigdata421/jxt-core/sdk/config"
	"github.com/ChenBigdata421/jxt-core/sdk/pkg/logger"
	"github.com/IBM/sarama"
	"go.uber.org/zap"
)

// åŠ¨æ€è®¢é˜…æ›´æ–°ç±»å‹
type subscriptionUpdate struct {
	action  string // "add" or "remove"
	topic   string
	handler MessageHandler
}

// ReconnectConfig é‡è¿é…ç½®
type ReconnectConfig struct {
	MaxAttempts      int           // æœ€å¤§é‡è¿æ¬¡æ•°
	InitialBackoff   time.Duration // åˆå§‹é€€é¿æ—¶é—´
	MaxBackoff       time.Duration // æœ€å¤§é€€é¿æ—¶é—´
	BackoffFactor    float64       // é€€é¿å› å­
	FailureThreshold int           // è§¦å‘é‡è¿çš„è¿ç»­å¤±è´¥æ¬¡æ•°
}

// DefaultReconnectConfig é»˜è®¤é‡è¿é…ç½®
func DefaultReconnectConfig() ReconnectConfig {
	return ReconnectConfig{
		MaxAttempts:      DefaultMaxReconnectAttempts,
		InitialBackoff:   DefaultReconnectInitialBackoff,
		MaxBackoff:       DefaultReconnectMaxBackoff,
		BackoffFactor:    DefaultReconnectBackoffFactor,
		FailureThreshold: DefaultReconnectFailureThreshold,
	}
}

// kafkaEventBus Kafkaäº‹ä»¶æ€»çº¿å®ç°
// ä¼ä¸šçº§å¢å¼ºç‰ˆæœ¬ï¼Œé›†æˆç§¯å‹æ£€æµ‹ã€æµé‡æ§åˆ¶ã€èšåˆå¤„ç†ç­‰ç‰¹æ€§
// æ”¯æŒæ–¹æ¡ˆAï¼ˆEnvelopeï¼‰æ¶ˆæ¯åŒ…ç»œ
//
// ğŸ”¥ æ€§èƒ½ä¼˜åŒ–ï¼šé«˜é¢‘è·¯å¾„å…é”ï¼Œä½é¢‘è·¯å¾„ä¿ç•™é”
// - é«˜é¢‘è·¯å¾„ï¼ˆPublishã€ConsumeClaimã€é…ç½®è¯»å–ï¼‰ï¼šä½¿ç”¨ atomic.Valueã€sync.Map å®ç°æ— é”è¯»å–
// - ä½é¢‘è·¯å¾„ï¼ˆSubscribeã€Closeã€é…ç½®è®¾ç½®ï¼‰ï¼šä¿ç•™ sync.Mutexï¼Œä¿æŒä»£ç æ¸…æ™°
type kafkaEventBus struct {
	config *KafkaConfig // ä½¿ç”¨å†…éƒ¨é…ç½®ç»“æ„ï¼Œå®ç°è§£è€¦
	logger *zap.Logger

	// âœ… ä½é¢‘è·¯å¾„ï¼šä¿ç•™ muï¼ˆç”¨äº Subscribeã€Close ç­‰ä½é¢‘æ“ä½œï¼‰
	mu     sync.Mutex
	closed atomic.Bool // ğŸ”¥ P0ä¿®å¤ï¼šæ”¹ä¸º atomic.Boolï¼Œçƒ­è·¯å¾„æ— é”è¯»å–

	// ğŸ”¥ é«˜é¢‘è·¯å¾„ï¼šæ”¹ä¸º atomic.Valueï¼ˆå‘å¸ƒæ—¶æ— é”è¯»å–ï¼‰
	asyncProducer atomic.Value // stores sarama.AsyncProducer
	consumer      atomic.Value // stores sarama.Consumer
	client        atomic.Value // stores sarama.Client
	admin         atomic.Value // stores sarama.ClusterAdmin

	// ä¼ä¸šçº§ç‰¹æ€§
	backlogDetector          *BacklogDetector          // è®¢é˜…ç«¯ç§¯å‹æ£€æµ‹å™¨
	publisherBacklogDetector *PublisherBacklogDetector // å‘é€ç«¯ç§¯å‹æ£€æµ‹å™¨
	rateLimiter              *RateLimiter              // è®¢é˜…ç«¯æµé‡æ§åˆ¶å™¨
	publishRateLimiter       *RateLimiter              // å‘å¸ƒç«¯æµé‡æ§åˆ¶å™¨

	messageFormatter MessageFormatter
	publishCallback  PublishCallback
	errorHandler     ErrorHandler
	messageRouter    MessageRouter

	// ç»Ÿè®¡ä¿¡æ¯
	publishedMessages atomic.Int64
	consumedMessages  atomic.Int64
	errorCount        atomic.Int64

	// å¥åº·æ£€æŸ¥æ§åˆ¶
	healthCheckCancel context.CancelFunc
	healthCheckDone   chan struct{}

	// å¥åº·æ£€æŸ¥è®¢é˜…ç›‘æ§å™¨
	healthCheckSubscriber *HealthCheckSubscriber
	// å¥åº·æ£€æŸ¥å‘å¸ƒå™¨
	healthChecker *HealthChecker
	// å¥åº·æ£€æŸ¥é…ç½®ï¼ˆä» Enterprise.HealthCheck è½¬æ¢è€Œæ¥ï¼‰
	healthCheckConfig config.HealthCheckConfig

	// è‡ªåŠ¨é‡è¿æ§åˆ¶
	reconnectConfig   ReconnectConfig
	failureCount      atomic.Int32
	lastReconnectTime atomic.Value // time.Time
	reconnectCallback ReconnectCallback

	// é¢„è®¢é˜…æ¨¡å¼ - ç»Ÿä¸€æ¶ˆè´¹è€…ç»„ç®¡ç†
	unifiedConsumerGroup atomic.Value // stores sarama.ConsumerGroup

	// Round-Robin è½®è¯¢è®¡æ•°å™¨ï¼ˆç”¨äºæ— èšåˆIDæ¶ˆæ¯çš„è´Ÿè½½å‡è¡¡ï¼‰
	roundRobinCounter atomic.Uint64

	// ğŸ”¥ P0ä¿®å¤ï¼šé¢„è®¢é˜…topicç®¡ç† - ä½¿ç”¨ atomic.Value å­˜å‚¨ä¸å¯å˜åˆ‡ç‰‡å¿«ç…§
	allPossibleTopicsMu sync.Mutex   // ä¿æŠ¤å†™å…¥
	allPossibleTopics   []string     // ä¸»å‰¯æœ¬ï¼ˆä»…åœ¨æŒæœ‰é”æ—¶ä¿®æ”¹ï¼‰
	topicsSnapshot      atomic.Value // åªè¯»å¿«ç…§ï¼ˆ[]stringï¼‰ï¼Œæ¶ˆè´¹goroutineæ— é”è¯»å–

	// ğŸ”¥ P0ä¿®å¤ï¼šæ”¹ä¸º sync.Mapï¼ˆæ¶ˆæ¯è·¯ç”±æ—¶æ— é”æŸ¥æ‰¾ï¼‰
	activeTopicHandlers sync.Map // key: string (topic), value: *handlerWrapper

	// é¢„è®¢é˜…æ¶ˆè´¹æ§åˆ¶
	consumerCtx     context.Context
	consumerCancel  context.CancelFunc
	consumerDone    chan struct{}
	consumerMu      sync.Mutex
	consumerStarted bool

	// é¢„è®¢é˜…é…ç½®
	preSubscriptionEnabled bool
	maxTopicsPerGroup      int

	// ä¼˜åŒ–4ï¼šé¢„çƒ­çŠ¶æ€ç›‘æ§
	warmupCompleted bool
	warmupStartTime time.Time
	warmupMu        sync.RWMutex

	// å¼‚æ­¥å‘å¸ƒç»“æœé€šé“ï¼ˆç”¨äºOutboxæ¨¡å¼ï¼‰
	publishResultChan chan *PublishResult

	// å¤šç§Ÿæˆ· ACK é€šé“æ”¯æŒ
	tenantPublishResultChans map[string]chan *PublishResult // key: tenantID, value: ACK channel
	tenantChannelsMu         sync.RWMutex                   // ä¿æŠ¤ tenantPublishResultChans çš„è¯»å†™é”

	// ğŸ”¥ é«˜é¢‘è·¯å¾„ï¼šæ”¹ä¸º sync.Mapï¼ˆæ¶ˆæ¯å¤„ç†æ—¶æ— é”æŸ¥æ‰¾ï¼‰
	// è®¢é˜…ç®¡ç†ï¼ˆç”¨äºé‡è¿åæ¢å¤è®¢é˜…ï¼‰- ä¿æŒå…¼å®¹æ€§
	subscriptions sync.Map // key: string (topic), value: MessageHandler

	// å…¨å±€ Hollywood Actor Poolï¼ˆæ‰€æœ‰ topic å…±äº«ï¼‰
	globalActorPool *HollywoodActorPool // â­ å…¨å±€ Hollywood Actor Poolï¼ˆç»Ÿä¸€æ¶æ„ï¼‰

	// ğŸ”¥ é«˜é¢‘è·¯å¾„ï¼šæ”¹ä¸º sync.Mapï¼ˆå‘å¸ƒæ—¶æ— é”è¯»å–é…ç½®ï¼‰
	// ä¸»é¢˜é…ç½®ç®¡ç†
	topicConfigs          sync.Map                  // key: string (topic), value: TopicOptions
	topicConfigStrategy   TopicConfigStrategy       // é…ç½®ç­–ç•¥
	topicConfigOnMismatch TopicConfigMismatchAction // é…ç½®ä¸ä¸€è‡´æ—¶çš„è¡Œä¸º
}

// ğŸ”¥ é«˜é¢‘è·¯å¾„è¾…åŠ©æ–¹æ³•ï¼šæ— é”è¯»å– atomic.Value å­˜å‚¨çš„å¯¹è±¡

// getAsyncProducer æ— é”è¯»å– AsyncProducer
func (k *kafkaEventBus) getAsyncProducer() (sarama.AsyncProducer, error) {
	producerAny := k.asyncProducer.Load()
	if producerAny == nil {
		return nil, fmt.Errorf("async producer not initialized")
	}
	producer, ok := producerAny.(sarama.AsyncProducer)
	if !ok {
		return nil, fmt.Errorf("invalid async producer type")
	}
	return producer, nil
}

// getConsumer æ— é”è¯»å– Consumer
func (k *kafkaEventBus) getConsumer() (sarama.Consumer, error) {
	consumerAny := k.consumer.Load()
	if consumerAny == nil {
		return nil, fmt.Errorf("consumer not initialized")
	}
	consumer, ok := consumerAny.(sarama.Consumer)
	if !ok {
		return nil, fmt.Errorf("invalid consumer type")
	}
	return consumer, nil
}

// getClient æ— é”è¯»å– Client
func (k *kafkaEventBus) getClient() (sarama.Client, error) {
	clientAny := k.client.Load()
	if clientAny == nil {
		return nil, fmt.Errorf("client not initialized")
	}
	client, ok := clientAny.(sarama.Client)
	if !ok {
		return nil, fmt.Errorf("invalid client type")
	}
	return client, nil
}

// getAdmin æ— é”è¯»å– ClusterAdmin
func (k *kafkaEventBus) getAdmin() (sarama.ClusterAdmin, error) {
	adminAny := k.admin.Load()
	if adminAny == nil {
		return nil, fmt.Errorf("admin not initialized")
	}
	admin, ok := adminAny.(sarama.ClusterAdmin)
	if !ok {
		return nil, fmt.Errorf("invalid admin type")
	}
	return admin, nil
}

// getUnifiedConsumerGroup æ— é”è¯»å– ConsumerGroup
func (k *kafkaEventBus) getUnifiedConsumerGroup() (sarama.ConsumerGroup, error) {
	groupAny := k.unifiedConsumerGroup.Load()
	if groupAny == nil {
		return nil, fmt.Errorf("consumer group not initialized")
	}
	group, ok := groupAny.(sarama.ConsumerGroup)
	if !ok {
		return nil, fmt.Errorf("invalid consumer group type")
	}
	return group, nil
}

// NewKafkaEventBus åˆ›å»ºä¼ä¸šçº§Kafkaäº‹ä»¶æ€»çº¿
// ä½¿ç”¨å†…éƒ¨é…ç½®ç»“æ„ï¼Œå®ç°é…ç½®è§£è€¦
func NewKafkaEventBus(cfg *KafkaConfig) (EventBus, error) {
	if cfg == nil {
		return nil, fmt.Errorf("kafka config cannot be nil")
	}

	if len(cfg.Brokers) == 0 {
		return nil, fmt.Errorf("kafka brokers cannot be empty")
	}

	// åˆ›å»ºSaramaé…ç½®
	saramaConfig := sarama.NewConfig()

	// ä¼˜åŒ–1ï¼šAsyncProduceré…ç½®ï¼ˆConfluentå®˜æ–¹æ¨èï¼‰
	saramaConfig.Producer.RequiredAcks = sarama.RequiredAcks(cfg.Producer.RequiredAcks)

	// ğŸ”¥ é‡æ„ï¼šç§»é™¤ Producer çº§åˆ«çš„å‹ç¼©é…ç½®ï¼Œæ”¹ä¸º topic çº§åˆ«é…ç½®
	// å‹ç¼©é…ç½®ç°åœ¨é€šè¿‡ TopicBuilder åœ¨ topic çº§åˆ«è®¾ç½®
	// å‚è€ƒï¼šcreateKafkaTopic() å‡½æ•°ä¸­çš„ compression.type é…ç½®
	saramaConfig.Producer.Compression = sarama.CompressionNone // é»˜è®¤ä¸å‹ç¼©ï¼Œç”± topic é…ç½®å†³å®š

	// ä¼˜åŒ–2ï¼šæ‰¹å¤„ç†é…ç½®ï¼ˆConfluentå®˜æ–¹æ¨èå€¼ï¼‰
	if cfg.Producer.FlushFrequency > 0 {
		saramaConfig.Producer.Flush.Frequency = cfg.Producer.FlushFrequency
	} else {
		saramaConfig.Producer.Flush.Frequency = 10 * time.Millisecond // Confluentæ¨èï¼š10ms
	}

	if cfg.Producer.FlushMessages > 0 {
		saramaConfig.Producer.Flush.Messages = cfg.Producer.FlushMessages
	} else {
		saramaConfig.Producer.Flush.Messages = 100 // Confluentæ¨èï¼š100æ¡æ¶ˆæ¯
	}

	if cfg.Producer.FlushBytes > 0 {
		saramaConfig.Producer.Flush.Bytes = cfg.Producer.FlushBytes
	} else {
		saramaConfig.Producer.Flush.Bytes = 100000 // Confluentæ¨èï¼š100KB
	}

	saramaConfig.Producer.Retry.Max = cfg.Producer.RetryMax
	saramaConfig.Producer.Timeout = cfg.Producer.Timeout
	saramaConfig.Producer.MaxMessageBytes = cfg.Producer.MaxMessageBytes
	saramaConfig.Producer.Idempotent = cfg.Producer.Idempotent

	// é…ç½® Hash Partitionerï¼ˆä¿è¯ç›¸åŒ key è·¯ç”±åˆ°åŒä¸€ partitionï¼Œç¡®ä¿é¡ºåºï¼‰
	saramaConfig.Producer.Partitioner = sarama.NewHashPartitioner

	// AsyncProduceréœ€è¦é…ç½®æˆåŠŸå’Œé”™è¯¯channel
	saramaConfig.Producer.Return.Successes = true
	saramaConfig.Producer.Return.Errors = true

	// ä¼˜åŒ–4ï¼šå¹¶å‘è¯·æ±‚æ•°ï¼ˆä¸šç•Œæœ€ä½³å®è·µï¼‰
	if cfg.Producer.MaxInFlight <= 0 {
		saramaConfig.Net.MaxOpenRequests = 100 // ä¸šç•Œæ¨èï¼š100å¹¶å‘è¯·æ±‚
	} else {
		saramaConfig.Net.MaxOpenRequests = cfg.Producer.MaxInFlight
	}

	// ä¼˜åŒ–5ï¼šConsumeré…ç½®ï¼ˆConfluentå®˜æ–¹æ¨èï¼‰
	saramaConfig.Consumer.Group.Session.Timeout = cfg.Consumer.SessionTimeout
	saramaConfig.Consumer.Group.Heartbeat.Interval = cfg.Consumer.HeartbeatInterval
	if cfg.Consumer.MaxProcessingTime > 0 {
		saramaConfig.Consumer.MaxProcessingTime = cfg.Consumer.MaxProcessingTime
	} else {
		saramaConfig.Consumer.MaxProcessingTime = 1 * time.Second // é»˜è®¤1ç§’
	}

	// ä¼˜åŒ–6ï¼šFetché…ç½®ï¼ˆConfluentå®˜æ–¹æ¨èå€¼ï¼‰
	if cfg.Consumer.FetchMinBytes > 0 {
		saramaConfig.Consumer.Fetch.Min = int32(cfg.Consumer.FetchMinBytes)
	} else {
		saramaConfig.Consumer.Fetch.Min = 100 * 1024 // Confluentæ¨èï¼š100KB
	}
	if cfg.Consumer.FetchMaxBytes > 0 {
		saramaConfig.Consumer.Fetch.Max = int32(cfg.Consumer.FetchMaxBytes)
	} else {
		saramaConfig.Consumer.Fetch.Max = 10 * 1024 * 1024 // Confluentæ¨èï¼š10MB
	}
	if cfg.Consumer.FetchMaxWait > 0 {
		saramaConfig.Consumer.MaxWaitTime = cfg.Consumer.FetchMaxWait
	} else {
		saramaConfig.Consumer.MaxWaitTime = 500 * time.Millisecond // Confluentæ¨èï¼š500ms
	}

	// ä¼˜åŒ–7ï¼šé¢„å–ç¼“å†²ï¼ˆä¸šç•Œæœ€ä½³å®è·µï¼‰
	saramaConfig.ChannelBufferSize = 1000 // é¢„å–1000æ¡æ¶ˆæ¯

	saramaConfig.Consumer.Offsets.Initial = getOffsetInitial(cfg.Consumer.AutoOffsetReset)
	saramaConfig.Consumer.Group.Rebalance.Strategy = getRebalanceStrategy(cfg.Consumer.RebalanceStrategy)
	saramaConfig.Consumer.IsolationLevel = getIsolationLevel(cfg.Consumer.IsolationLevel)

	// ä¼˜åŒ–Consumerç¨³å®šæ€§é…ç½®
	saramaConfig.Consumer.Return.Errors = true                            // å¯ç”¨é”™è¯¯è¿”å›
	saramaConfig.Consumer.Offsets.Retry.Max = 3                           // å¢åŠ é‡è¯•æ¬¡æ•°
	saramaConfig.Consumer.Group.Rebalance.Timeout = 60 * time.Second      // å¢åŠ é‡å¹³è¡¡è¶…æ—¶
	saramaConfig.Consumer.Group.Rebalance.Retry.Max = 4                   // å¢åŠ é‡å¹³è¡¡é‡è¯•æ¬¡æ•°
	saramaConfig.Consumer.Group.Rebalance.Retry.Backoff = 2 * time.Second // é‡å¹³è¡¡é‡è¯•é—´éš”

	// ä¼˜åŒ–8ï¼šç½‘ç»œè¶…æ—¶é…ç½®ï¼ˆConfluentå®˜æ–¹æ¨èï¼‰
	// æ ¹æ®éƒ¨ç½²ç¯å¢ƒè°ƒæ•´è¶…æ—¶æ—¶é—´
	if cfg.Net.DialTimeout > 0 {
		saramaConfig.Net.DialTimeout = cfg.Net.DialTimeout
	} else {
		saramaConfig.Net.DialTimeout = 10 * time.Second // ä¸šç•Œæ¨èï¼šæœ¬åœ°/äº‘éƒ¨ç½²10ç§’
	}
	if cfg.Net.ReadTimeout > 0 {
		saramaConfig.Net.ReadTimeout = cfg.Net.ReadTimeout
	} else {
		saramaConfig.Net.ReadTimeout = 30 * time.Second // ä¿æŒ30ç§’ï¼ˆé€‚åˆäº‘éƒ¨ç½²ï¼‰
	}
	if cfg.Net.WriteTimeout > 0 {
		saramaConfig.Net.WriteTimeout = cfg.Net.WriteTimeout
	} else {
		saramaConfig.Net.WriteTimeout = 30 * time.Second // ä¿æŒ30ç§’ï¼ˆé€‚åˆäº‘éƒ¨ç½²ï¼‰
	}
	if cfg.Net.KeepAlive > 0 {
		saramaConfig.Net.KeepAlive = cfg.Net.KeepAlive
	} else {
		saramaConfig.Net.KeepAlive = 30 * time.Second // ä¿æŒ30ç§’
	}

	// é…ç½®å®¢æˆ·ç«¯
	saramaConfig.ClientID = cfg.ClientID
	saramaConfig.Metadata.RefreshFrequency = cfg.MetadataRefreshFreq
	saramaConfig.Metadata.Retry.Max = cfg.MetadataRetryMax
	saramaConfig.Metadata.Retry.Backoff = cfg.MetadataRetryBackoff

	// é…ç½®å®‰å…¨è®¤è¯
	if cfg.Security.Enabled {
		// TODO: å®ç°å®‰å…¨é…ç½®
		logger.Warn("Security configuration not yet implemented")
	}

	// åˆ›å»ºKafkaå®¢æˆ·ç«¯
	client, err := sarama.NewClient(cfg.Brokers, saramaConfig)
	if err != nil {
		return nil, fmt.Errorf("failed to create kafka client: %w", err)
	}

	// ä¼˜åŒ–1ï¼šåˆ›å»ºAsyncProducerï¼ˆConfluentå®˜æ–¹æ¨èï¼‰
	asyncProducer, err := sarama.NewAsyncProducerFromClient(client)
	if err != nil {
		client.Close()
		return nil, fmt.Errorf("failed to create async producer: %w", err)
	}

	// åˆ›å»ºç»Ÿä¸€æ¶ˆè´¹è€…ç»„
	unifiedConsumerGroup, err := sarama.NewConsumerGroupFromClient(cfg.Consumer.GroupID, client)
	if err != nil {
		client.Close()
		asyncProducer.Close()
		return nil, fmt.Errorf("failed to create unified consumer group: %w", err)
	}

	// åˆ›å»ºç®¡ç†å®¢æˆ·ç«¯ï¼ˆç”¨äºä¸»é¢˜é…ç½®ç®¡ç†ï¼‰
	admin, err := sarama.NewClusterAdminFromClient(client)
	if err != nil {
		unifiedConsumerGroup.Close()
		asyncProducer.Close()
		client.Close()
		return nil, fmt.Errorf("failed to create kafka admin: %w", err)
	}

	// è½¬æ¢å¥åº·æ£€æŸ¥é…ç½®ï¼ˆä» eventbus.HealthCheckConfig è½¬æ¢ä¸º config.HealthCheckConfigï¼‰
	healthCheckConfig := convertHealthCheckConfig(cfg.Enterprise.HealthCheck)

	// åˆ›å»ºäº‹ä»¶æ€»çº¿å®ä¾‹
	bus := &kafkaEventBus{
		config: cfg,
		logger: zap.NewNop(), // ä½¿ç”¨æ— æ“ä½œlogger

		// å¥åº·æ£€æŸ¥é…ç½®
		healthCheckConfig: healthCheckConfig,

		// ğŸ”¥ P0ä¿®å¤ï¼šé¢„è®¢é˜…æ¨¡å¼å­—æ®µ
		allPossibleTopics:      make([]string, 0),
		preSubscriptionEnabled: true,
		maxTopicsPerGroup:      100, // é»˜è®¤æœ€å¤§100ä¸ªtopic
		consumerDone:           make(chan struct{}),

		// ğŸš€ å¼‚æ­¥å‘å¸ƒç»“æœé€šé“ï¼ˆç¼“å†²åŒºå¤§å°ï¼š10000ï¼‰
		publishResultChan: make(chan *PublishResult, 10000),
	}

	// ğŸ”¥ P0ä¿®å¤ï¼šåˆå§‹åŒ– atomic å­—æ®µ
	bus.closed.Store(false)
	bus.topicsSnapshot.Store([]string{})

	// âœ… ä½¿ç”¨ atomic.Value å­˜å‚¨ï¼ˆé«˜é¢‘è·¯å¾„æ— é”è¯»å–ï¼‰
	bus.client.Store(client)
	bus.asyncProducer.Store(asyncProducer)
	bus.unifiedConsumerGroup.Store(unifiedConsumerGroup)
	bus.admin.Store(admin)

	// æ³¨æ„ï¼šsubscriptions å’Œ topicConfigs æ˜¯ sync.Mapï¼Œé›¶å€¼å¯ç”¨ï¼Œä¸éœ€è¦åˆå§‹åŒ–

	// â­ åˆ›å»ºå…¨å±€ Hollywood Actor Poolï¼ˆæ‰€æœ‰ topic å…±äº«ï¼‰
	// åˆ›å»º Prometheus ç›‘æ§æ”¶é›†å™¨
	// ä½¿ç”¨ ClientID ä½œä¸ºå‘½åç©ºé—´ï¼Œç¡®ä¿æ¯ä¸ªå®ä¾‹çš„æŒ‡æ ‡ä¸å†²çª
	// æ³¨æ„ï¼šPrometheus æŒ‡æ ‡åç§°åªèƒ½åŒ…å« [a-zA-Z0-9_]ï¼Œéœ€è¦æ›¿æ¢ - ä¸º _
	metricsNamespace := fmt.Sprintf("kafka_eventbus_%s", strings.ReplaceAll(cfg.ClientID, "-", "_"))
	actorPoolMetrics := NewPrometheusActorPoolMetricsCollector(metricsNamespace)

	bus.globalActorPool = NewHollywoodActorPool(HollywoodActorPoolConfig{
		PoolSize:    256,  // å›ºå®š Actor æ•°é‡
		InboxSize:   1000, // Inbox é˜Ÿåˆ—å¤§å°
		MaxRestarts: 3,    // Supervisor æœ€å¤§é‡å¯æ¬¡æ•°
	}, actorPoolMetrics)

	bus.logger.Info("Kafka EventBus using Hollywood Actor Pool",
		zap.Int("poolSize", 256),
		zap.Int("inboxSize", 1000),
		zap.Int("maxRestarts", 3))

	// åˆå§‹åŒ–å‘å¸ƒç«¯æµé‡æ§åˆ¶å™¨
	if cfg.Enterprise.Publisher.RateLimit.Enabled {
		bus.publishRateLimiter = NewRateLimiter(cfg.Enterprise.Publisher.RateLimit)
		logger.Info("Publisher rate limiter enabled",
			"ratePerSecond", cfg.Enterprise.Publisher.RateLimit.RatePerSecond,
			"burstSize", cfg.Enterprise.Publisher.RateLimit.BurstSize)
	}

	// ä¼˜åŒ–1ï¼šå¯åŠ¨AsyncProducerçš„æˆåŠŸå’Œé”™è¯¯å¤„ç†goroutine
	go bus.handleAsyncProducerSuccess()
	go bus.handleAsyncProducerErrors()

	logger.Info("Kafka EventBus created successfully (AsyncProducer mode)",
		"brokers", cfg.Brokers,
		"clientId", cfg.ClientID,
		"healthCheckInterval", cfg.HealthCheckInterval,
		"compressionMode", "topic-level", // ğŸ”¥ é‡æ„ï¼šå‹ç¼©é…ç½®æ”¹ä¸º topic çº§åˆ«
		"flushFrequency", saramaConfig.Producer.Flush.Frequency,
		"flushMessages", saramaConfig.Producer.Flush.Messages,
		"flushBytes", saramaConfig.Producer.Flush.Bytes)

	return bus, nil
}

// ä¼˜åŒ–1ï¼šå¤„ç†AsyncProduceræˆåŠŸæ¶ˆæ¯
func (k *kafkaEventBus) handleAsyncProducerSuccess() {
	// âœ… æ— é”è¯»å– asyncProducer
	producerAny := k.asyncProducer.Load()
	if producerAny == nil {
		k.logger.Warn("AsyncProducer not initialized, cannot handle success messages")
		return
	}
	producer := producerAny.(sarama.AsyncProducer)

	for success := range producer.Successes() {
		// è®°å½•æˆåŠŸæŒ‡æ ‡
		k.publishedMessages.Add(1)

		// âœ… æå– EventIDï¼ˆä» Header ä¸­ï¼‰
		var eventID string
		var aggregateID string
		var eventType string
		var tenantID string
		for _, header := range success.Headers {
			switch string(header.Key) {
			case "X-Event-ID":
				eventID = string(header.Value)
			case "X-Aggregate-ID":
				aggregateID = string(header.Value)
			case "X-Event-Type":
				eventType = string(header.Value)
			case "X-Tenant-ID":
				tenantID = string(header.Value) // â† ç§Ÿæˆ·IDï¼ˆå¤šç§Ÿæˆ·æ”¯æŒï¼Œç”¨äºOutbox ACKè·¯ç”±ï¼‰
			}
		}

		// âœ… å¦‚æœæœ‰ EventIDï¼Œå‘é€æˆåŠŸç»“æœåˆ° publishResultChanï¼ˆç”¨äº Outbox æ¨¡å¼ï¼‰
		if eventID != "" {
			result := &PublishResult{
				EventID:     eventID,
				Topic:       success.Topic,
				Success:     true,
				Error:       nil,
				Timestamp:   time.Now(),
				AggregateID: aggregateID,
				EventType:   eventType,
				TenantID:    tenantID, // â† ç§Ÿæˆ·IDï¼ˆå¤šç§Ÿæˆ·æ”¯æŒï¼Œç”¨äºOutbox ACKè·¯ç”±ï¼‰
			}

			// âœ… å‘é€åˆ°ç§Ÿæˆ·ä¸“å±é€šé“æˆ–å…¨å±€é€šé“
			k.sendResultToChannel(result)
		}

		// å¦‚æœé…ç½®äº†å›è°ƒï¼Œæ‰§è¡Œå›è°ƒ
		if k.publishCallback != nil {
			// æå–æ¶ˆæ¯å†…å®¹
			var message []byte
			if success.Value != nil {
				message, _ = success.Value.Encode()
			}
			k.publishCallback(context.Background(), success.Topic, message, nil)
		}
	}
}

// ä¼˜åŒ–1ï¼šå¤„ç†AsyncProduceré”™è¯¯
func (k *kafkaEventBus) handleAsyncProducerErrors() {
	// âœ… æ— é”è¯»å– asyncProducer
	producer, err := k.getAsyncProducer()
	if err != nil {
		k.logger.Warn("AsyncProducer not initialized, cannot handle error messages", zap.Error(err))
		return
	}

	for err := range producer.Errors() {
		// è®°å½•é”™è¯¯
		k.errorCount.Add(1)
		k.logger.Error("Async producer error",
			zap.String("topic", err.Msg.Topic),
			zap.Error(err.Err))

		// âœ… æå– EventIDï¼ˆä» Header ä¸­ï¼‰
		var eventID string
		var aggregateID string
		var eventType string
		var tenantID string
		for _, header := range err.Msg.Headers {
			switch string(header.Key) {
			case "X-Event-ID":
				eventID = string(header.Value)
			case "X-Aggregate-ID":
				aggregateID = string(header.Value)
			case "X-Event-Type":
				eventType = string(header.Value)
			case "X-Tenant-ID":
				tenantID = string(header.Value) // â† ç§Ÿæˆ·IDï¼ˆå¤šç§Ÿæˆ·æ”¯æŒï¼Œç”¨äºOutbox ACKè·¯ç”±ï¼‰
			}
		}

		// âœ… å¦‚æœæœ‰ EventIDï¼Œå‘é€å¤±è´¥ç»“æœåˆ° publishResultChanï¼ˆç”¨äº Outbox æ¨¡å¼ï¼‰
		if eventID != "" {
			result := &PublishResult{
				EventID:     eventID,
				Topic:       err.Msg.Topic,
				Success:     false,
				Error:       err.Err,
				Timestamp:   time.Now(),
				AggregateID: aggregateID,
				EventType:   eventType,
				TenantID:    tenantID, // â† ç§Ÿæˆ·IDï¼ˆå¤šç§Ÿæˆ·æ”¯æŒï¼Œç”¨äºOutbox ACKè·¯ç”±ï¼‰
			}

			// âœ… å‘é€åˆ°ç§Ÿæˆ·ä¸“å±é€šé“æˆ–å…¨å±€é€šé“
			k.sendResultToChannel(result)
		}

		// æå–æ¶ˆæ¯å†…å®¹
		var message []byte
		if err.Msg.Value != nil {
			message, _ = err.Msg.Value.Encode()
		}

		// å¦‚æœé…ç½®äº†å›è°ƒï¼Œæ‰§è¡Œå›è°ƒ
		if k.publishCallback != nil {
			k.publishCallback(context.Background(), err.Msg.Topic, message, err.Err)
		}

		// å¦‚æœé…ç½®äº†é”™è¯¯å¤„ç†å™¨ï¼Œæ‰§è¡Œé”™è¯¯å¤„ç†
		if k.errorHandler != nil {
			k.errorHandler.HandleError(context.Background(), err.Err, message, err.Msg.Topic)
		}

		// å‘å¸ƒç«¯é”™è¯¯å¤„ç†ï¼šå‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—
		if k.config.Enterprise.Publisher.ErrorHandling.DeadLetterTopic != "" {
			k.sendToPublisherDeadLetter(err.Msg.Topic, message, err.Err)
		}
	}
}

// sendToPublisherDeadLetter å°†å‘å¸ƒå¤±è´¥çš„æ¶ˆæ¯å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—
func (k *kafkaEventBus) sendToPublisherDeadLetter(originalTopic string, message []byte, publishErr error) {
	deadLetterTopic := k.config.Enterprise.Publisher.ErrorHandling.DeadLetterTopic

	// åˆ›å»ºæ­»ä¿¡æ¶ˆæ¯ï¼ŒåŒ…å«åŸå§‹ä¸»é¢˜å’Œé”™è¯¯ä¿¡æ¯
	dlqMsg := &sarama.ProducerMessage{
		Topic: deadLetterTopic,
		Value: sarama.ByteEncoder(message),
		Headers: []sarama.RecordHeader{
			{Key: []byte("X-Original-Topic"), Value: []byte(originalTopic)},
			{Key: []byte("X-Error-Message"), Value: []byte(publishErr.Error())},
			{Key: []byte("X-Timestamp"), Value: []byte(time.Now().Format(time.RFC3339))},
		},
	}

	// åº”ç”¨é‡è¯•ç­–ç•¥å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—
	retryPolicy := k.config.Enterprise.Publisher.RetryPolicy
	if retryPolicy.Enabled && retryPolicy.MaxRetries > 0 {
		k.sendWithRetry(dlqMsg, retryPolicy, originalTopic, deadLetterTopic)
	} else {
		// ä¸ä½¿ç”¨é‡è¯•ï¼Œç›´æ¥å‘é€
		k.sendMessageNonBlocking(dlqMsg, originalTopic, deadLetterTopic)
	}
}

// sendWithRetry ä½¿ç”¨é‡è¯•ç­–ç•¥å‘é€æ¶ˆæ¯
func (k *kafkaEventBus) sendWithRetry(msg *sarama.ProducerMessage, retryPolicy RetryPolicyConfig, originalTopic, targetTopic string) {
	// âœ… æ— é”è¯»å– asyncProducer
	producer, err := k.getAsyncProducer()
	if err != nil {
		k.logger.Error("Failed to get async producer for retry",
			zap.String("originalTopic", originalTopic),
			zap.String("targetTopic", targetTopic),
			zap.Error(err))
		return
	}

	var lastErr error
	backoff := retryPolicy.InitialInterval

	for attempt := 0; attempt <= retryPolicy.MaxRetries; attempt++ {
		// å°è¯•å‘é€
		select {
		case producer.Input() <- msg:
			k.logger.Info("Message sent successfully with retry",
				zap.String("originalTopic", originalTopic),
				zap.String("targetTopic", targetTopic),
				zap.Int("attempt", attempt+1))
			return
		case <-time.After(100 * time.Millisecond):
			lastErr = fmt.Errorf("async producer input queue full")
		}

		// å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…åé‡è¯•
		if attempt < retryPolicy.MaxRetries {
			k.logger.Warn("Retry sending message to dead letter queue",
				zap.String("originalTopic", originalTopic),
				zap.String("targetTopic", targetTopic),
				zap.Int("attempt", attempt+1),
				zap.Int("maxRetries", retryPolicy.MaxRetries),
				zap.Duration("backoff", backoff))

			time.Sleep(backoff)

			// è®¡ç®—ä¸‹ä¸€æ¬¡é€€é¿æ—¶é—´
			backoff = time.Duration(float64(backoff) * retryPolicy.Multiplier)
			if backoff > retryPolicy.MaxInterval {
				backoff = retryPolicy.MaxInterval
			}
		}
	}

	// æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
	k.logger.Error("Failed to send message after all retries",
		zap.String("originalTopic", originalTopic),
		zap.String("targetTopic", targetTopic),
		zap.Int("maxRetries", retryPolicy.MaxRetries),
		zap.Error(lastErr))
}

// sendMessageNonBlocking éé˜»å¡å‘é€æ¶ˆæ¯
func (k *kafkaEventBus) sendMessageNonBlocking(msg *sarama.ProducerMessage, originalTopic, targetTopic string) {
	// âœ… æ— é”è¯»å– asyncProducer
	producer, err := k.getAsyncProducer()
	if err != nil {
		k.logger.Error("Failed to get async producer for non-blocking send",
			zap.String("originalTopic", originalTopic),
			zap.String("targetTopic", targetTopic),
			zap.Error(err))
		return
	}

	select {
	case producer.Input() <- msg:
		k.logger.Info("Message sent to target topic",
			zap.String("originalTopic", originalTopic),
			zap.String("targetTopic", targetTopic))
	case <-time.After(100 * time.Millisecond):
		k.logger.Error("Failed to send message to target topic",
			zap.String("originalTopic", originalTopic),
			zap.String("targetTopic", targetTopic))
	}
}

// ========== é…ç½®è½¬æ¢è¾…åŠ©å‡½æ•° ==========

// convertHealthCheckConfig å°† eventbus.HealthCheckConfig è½¬æ¢ä¸º config.HealthCheckConfig
func convertHealthCheckConfig(ebConfig HealthCheckConfig) config.HealthCheckConfig {
	// å¦‚æœé…ç½®ä¸ºç©ºæˆ–æœªå¯ç”¨ï¼Œè¿”å›é»˜è®¤é…ç½®
	if !ebConfig.Enabled {
		return GetDefaultHealthCheckConfig()
	}

	// è½¬æ¢é…ç½®
	return config.HealthCheckConfig{
		Enabled: ebConfig.Enabled,
		Publisher: config.HealthCheckPublisherConfig{
			Topic:            ebConfig.Topic,
			Interval:         ebConfig.Interval,
			Timeout:          ebConfig.Timeout,
			FailureThreshold: ebConfig.FailureThreshold,
			MessageTTL:       ebConfig.MessageTTL,
		},
		Subscriber: config.HealthCheckSubscriberConfig{
			Topic:             ebConfig.Topic,
			MonitorInterval:   30 * time.Second, // ä½¿ç”¨é»˜è®¤å€¼
			WarningThreshold:  3,                // ä½¿ç”¨é»˜è®¤å€¼
			ErrorThreshold:    5,                // ä½¿ç”¨é»˜è®¤å€¼
			CriticalThreshold: 10,               // ä½¿ç”¨é»˜è®¤å€¼
		},
	}
}

// getCompressionCodec è·å–å‹ç¼©ç¼–è§£ç å™¨
func getCompressionCodec(compression string) sarama.CompressionCodec {
	switch strings.ToLower(compression) {
	case "gzip":
		return sarama.CompressionGZIP
	case "snappy":
		return sarama.CompressionSnappy
	case "lz4":
		return sarama.CompressionLZ4
	case "zstd":
		return sarama.CompressionZSTD
	default:
		return sarama.CompressionNone
	}
}

// getOffsetInitial è·å–åˆå§‹åç§»é‡ç­–ç•¥
func getOffsetInitial(autoOffsetReset string) int64 {
	switch strings.ToLower(autoOffsetReset) {
	case "earliest":
		return sarama.OffsetOldest
	case "latest":
		return sarama.OffsetNewest
	default:
		return sarama.OffsetNewest
	}
}

// getRebalanceStrategy è·å–å†å¹³è¡¡ç­–ç•¥
func getRebalanceStrategy(strategy string) sarama.BalanceStrategy {
	switch strings.ToLower(strategy) {
	case "range":
		return sarama.BalanceStrategyRange
	case "roundrobin":
		return sarama.BalanceStrategyRoundRobin
	case "sticky":
		return sarama.BalanceStrategySticky
	default:
		return sarama.BalanceStrategyRange
	}
}

// getIsolationLevel è·å–éš”ç¦»çº§åˆ«
func getIsolationLevel(level string) sarama.IsolationLevel {
	switch strings.ToLower(level) {
	case "read_uncommitted":
		return sarama.ReadUncommitted
	case "read_committed":
		return sarama.ReadCommitted
	default:
		return sarama.ReadCommitted
	}
}

// kafkaConsumerHandler Kafkaæ¶ˆè´¹è€…å¤„ç†å™¨
type kafkaConsumerHandler struct {
	eventBus *kafkaEventBus
	handler  MessageHandler
	topic    string
}

// Setup æ¶ˆè´¹è€…ç»„è®¾ç½®
func (h *kafkaConsumerHandler) Setup(sarama.ConsumerGroupSession) error {
	return nil
}

// Cleanup æ¶ˆè´¹è€…ç»„æ¸…ç†
func (h *kafkaConsumerHandler) Cleanup(sarama.ConsumerGroupSession) error {
	return nil
}

// ConsumeClaim æ¶ˆè´¹æ¶ˆæ¯
func (h *kafkaConsumerHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error {
	for {
		select {
		case message := <-claim.Messages():
			if message == nil {
				return nil
			}

			// å¤„ç†æ¶ˆæ¯
			if err := h.processMessage(session.Context(), message); err != nil {
				h.eventBus.logger.Error("Failed to process message",
					zap.String("topic", message.Topic),
					zap.Int32("partition", message.Partition),
					zap.Int64("offset", message.Offset),
					zap.Error(err))
				h.eventBus.errorCount.Add(1)
			} else {
				h.eventBus.consumedMessages.Add(1)
				session.MarkMessage(message, "")
			}

		case <-session.Context().Done():
			return nil
		}
	}
}

// processMessage å¤„ç†å•ä¸ªæ¶ˆæ¯
func (h *kafkaConsumerHandler) processMessage(ctx context.Context, message *sarama.ConsumerMessage) error {
	// æµé‡æ§åˆ¶
	if h.eventBus.rateLimiter != nil {
		if err := h.eventBus.rateLimiter.Wait(ctx); err != nil {
			return fmt.Errorf("rate limit error: %w", err)
		}
	}

	// æ™ºèƒ½è·¯ç”±å†³ç­–ï¼šæ ¹æ®èšåˆIDæå–ç»“æœå†³å®šå¤„ç†æ¨¡å¼
	// æ„å»ºheadersæ˜ å°„
	headersMap := make(map[string]string, len(message.Headers))
	for _, header := range message.Headers {
		headersMap[string(header.Key)] = string(header.Value)
	}

	// å°è¯•æå–èšåˆIDï¼ˆä¼˜å…ˆçº§ï¼šEnvelope > Header > Kafka Keyï¼‰
	aggregateID, _ := ExtractAggregateID(message.Value, headersMap, message.Key, "")
	
	// å¦‚æœæ— æ³•æå–èšåˆIDï¼Œä½¿ç”¨ Round-Robin ç”Ÿæˆè·¯ç”±é”®
	if aggregateID == "" {
		index := h.eventBus.roundRobinCounter.Add(1)
		aggregateID = fmt.Sprintf("rr-%d", index)
	}

	// â­ ç»Ÿä¸€å¤„ç†ï¼šæ‰€æœ‰æ¶ˆæ¯éƒ½é€šè¿‡ Hollywood Actor Pool è·¯ç”±
	// - æœ‰èšåˆIDï¼šä½¿ç”¨ä¸€è‡´æ€§å“ˆå¸Œè·¯ç”±åˆ°å›ºå®š Actorï¼ˆé¡ºåºå¤„ç†ï¼‰
	// - æ— èšåˆIDï¼šä½¿ç”¨ Round-Robin ç”Ÿæˆçš„è·¯ç”±é”®åˆ†æ•£åˆ°ä¸åŒ Actorï¼ˆå¹¶å‘å¤„ç†ï¼‰
	pool := h.eventBus.globalActorPool
	if pool != nil {
		aggMsg := &AggregateMessage{
			Topic:       message.Topic,
			Partition:   message.Partition,
			Offset:      message.Offset,
			Key:         message.Key,
			Value:       message.Value,
			Headers:     make(map[string][]byte),
			Timestamp:   message.Timestamp,
			AggregateID: aggregateID,
			Context:     ctx,
			Done:        make(chan error, 1),
			Handler:     h.handler,
		}
		for _, header := range message.Headers {
			aggMsg.Headers[string(header.Key)] = header.Value
		}
		if err := pool.ProcessMessage(ctx, aggMsg); err != nil {
			return err
		}
		select {
		case err := <-aggMsg.Done:
			return err
		case <-ctx.Done():
			return ctx.Err()
		}
	}
	
	// å¤‡ç”¨æ–¹æ¡ˆï¼šå¦‚æœ Actor Pool ä¸å¯ç”¨ï¼Œç›´æ¥è°ƒç”¨ handler
	return h.handler(ctx, message.Value)
}

// preSubscriptionConsumerHandler é¢„è®¢é˜…æ¶ˆè´¹è€…å¤„ç†å™¨ - é¢„è®¢é˜…æ¨¡å¼
type preSubscriptionConsumerHandler struct {
	eventBus *kafkaEventBus
}

// Setup æ¶ˆè´¹è€…ç»„è®¾ç½®
func (h *preSubscriptionConsumerHandler) Setup(sarama.ConsumerGroupSession) error {
	h.eventBus.logger.Info("Pre-subscription consumer group session started")
	return nil
}

// Cleanup æ¶ˆè´¹è€…ç»„æ¸…ç†
func (h *preSubscriptionConsumerHandler) Cleanup(sarama.ConsumerGroupSession) error {
	h.eventBus.logger.Info("Pre-subscription consumer group session ended")
	return nil
}

// ConsumeClaim é¢„è®¢é˜…æ¶ˆè´¹æ¶ˆæ¯ - æ ¹æ®topicè·¯ç”±åˆ°æ¿€æ´»çš„handler
func (h *preSubscriptionConsumerHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error {
	ctx := session.Context()

	for {
		select {
		case message := <-claim.Messages():
			if message == nil {
				return nil
			}

			// ğŸ”¥ P0ä¿®å¤ï¼šæ— é”è¯»å– handlerï¼ˆä½¿ç”¨ sync.Mapï¼‰
			wrapperAny, exists := h.eventBus.activeTopicHandlers.Load(message.Topic)
			if !exists {
				// æœªæ¿€æ´»çš„ topicï¼Œè·³è¿‡
				session.MarkMessage(message, "")
				continue
			}
			wrapper := wrapperAny.(*handlerWrapper)

			if true {
				// ä¼˜åŒ–ï¼šå¢å¼ºæ¶ˆæ¯å¤„ç†é”™è¯¯å¤„ç†å’Œç›‘æ§
				h.eventBus.logger.Debug("Processing message",
					zap.String("topic", message.Topic),
					zap.Int64("offset", message.Offset),
					zap.Int32("partition", message.Partition),
					zap.Bool("isEnvelope", wrapper.isEnvelope))

				// ä½¿ç”¨ Hollywood Actor Pool å¤„ç†æ¶ˆæ¯ï¼ˆæœ‰èšåˆIDç”¨å“ˆå¸Œè·¯ç”±ï¼Œæ— èšåˆIDç”¨è½®è¯¢è·¯ç”±ï¼‰
				if err := h.processMessageWithKeyedPool(ctx, message, wrapper, session); err != nil {
					h.eventBus.logger.Error("Failed to process message",
						zap.String("topic", message.Topic),
						zap.Int64("offset", message.Offset),
						zap.Error(err))
					// å¤„ç†å¤±è´¥ï¼Œä»ç„¶æ ‡è®°æ¶ˆæ¯ï¼ˆé¿å…é˜»å¡ï¼‰
					session.MarkMessage(message, "")
				}
			} else {
				// æœªæ¿€æ´»çš„topicï¼Œç›´æ¥æ ‡è®°ä¸ºå·²å¤„ç†ï¼ˆé¢„è®¢é˜…æ¨¡å¼çš„ä¼˜åŠ¿ï¼‰
				h.eventBus.logger.Debug("Topic not activated, skipping message",
					zap.String("topic", message.Topic),
					zap.Int64("offset", message.Offset))
				session.MarkMessage(message, "")
			}

		case <-ctx.Done():
			return nil
		}
	}
}

// processMessageWithKeyedPool ä½¿ç”¨ Hollywood Actor Pool å¤„ç†æ¶ˆæ¯ï¼ˆç»Ÿä¸€æ¶æ„ï¼‰
func (h *preSubscriptionConsumerHandler) processMessageWithKeyedPool(ctx context.Context, message *sarama.ConsumerMessage, wrapper *handlerWrapper, session sarama.ConsumerGroupSession) error {
	// è½¬æ¢ Headers ä¸º map
	headersMap := make(map[string]string, len(message.Headers))
	for _, header := range message.Headers {
		headersMap[string(header.Key)] = string(header.Value)
	}

	// å°è¯•æå–èšåˆIDï¼ˆä¼˜å…ˆçº§ï¼šEnvelope > Header > Kafka Keyï¼‰
	aggregateID, _ := ExtractAggregateID(message.Value, headersMap, message.Key, "")

	// â­ æ ¸å¿ƒå˜æ›´ï¼šç»Ÿä¸€ä½¿ç”¨ Hollywood Actor Pool
	// è·¯ç”±ç­–ç•¥ï¼š
	// - æœ‰èšåˆIDï¼šä½¿ç”¨ aggregateID ä½œä¸ºè·¯ç”±é”®ï¼ˆä¿æŒæœ‰åºï¼‰
	// - æ— èšåˆIDï¼šä½¿ç”¨ Round-Robin è½®è¯¢ï¼ˆä¿æŒå¹¶å‘ï¼‰
	routingKey := aggregateID
	if routingKey == "" {
		// ä½¿ç”¨è½®è¯¢è®¡æ•°å™¨ç”Ÿæˆè·¯ç”±é”®
		index := h.eventBus.roundRobinCounter.Add(1)
		routingKey = fmt.Sprintf("rr-%d", index)
	}

	pool := h.eventBus.globalActorPool
	if pool == nil {
		return fmt.Errorf("hollywood actor pool not initialized")
	}

	aggMsg := &AggregateMessage{
		Topic:       message.Topic,
		Partition:   message.Partition,
		Offset:      message.Offset,
		Key:         message.Key,
		Value:       message.Value,
		Headers:     make(map[string][]byte),
		Timestamp:   message.Timestamp,
		AggregateID: routingKey, // â­ ä½¿ç”¨ routingKeyï¼ˆå¯èƒ½æ˜¯ aggregateID æˆ– Round-Robin ç´¢å¼•ï¼‰
		Context:     ctx,
		Done:        make(chan error, 1),
		Handler:     wrapper.handler,
		IsEnvelope:  wrapper.isEnvelope, // â­ è®¾ç½® Envelope æ ‡è®°
	}
	for _, header := range message.Headers {
		aggMsg.Headers[string(header.Key)] = header.Value
	}

	// æäº¤åˆ° Actor Pool
	if err := pool.ProcessMessage(ctx, aggMsg); err != nil {
		return err
	}

	// ç­‰å¾…å¤„ç†å®Œæˆ
	select {
	case err := <-aggMsg.Done:
		if err != nil {
			if wrapper.isEnvelope {
				// â­ Envelope æ¶ˆæ¯ï¼šä¸ MarkMessageï¼Œè®© Kafka é‡æ–°æŠ•é€’ï¼ˆat-least-once è¯­ä¹‰ï¼‰
				h.eventBus.logger.Warn("Envelope message processing failed, will be redelivered",
					zap.String("topic", message.Topic),
					zap.Int64("offset", message.Offset),
					zap.Error(err))
				return err
			} else {
				// â­ æ™®é€šæ¶ˆæ¯ï¼šMarkMessageï¼Œé¿å…é‡å¤æŠ•é€’ï¼ˆat-most-once è¯­ä¹‰ï¼‰
				h.eventBus.logger.Warn("Regular message processing failed, marking as processed",
					zap.String("topic", message.Topic),
					zap.Int64("offset", message.Offset),
					zap.Error(err))
				session.MarkMessage(message, "")
				return err
			}
		}
		// æˆåŠŸï¼šMarkMessage
		session.MarkMessage(message, "")
		return nil
	case <-ctx.Done():
		return ctx.Err()
	}
}

// initEnterpriseFeatures åˆå§‹åŒ–ä¼ä¸šçº§ç‰¹æ€§
func (k *kafkaEventBus) initEnterpriseFeatures() error {
	// ä½¿ç”¨ç¨‹åºå‘˜é…ç½®å±‚çš„ä¼ä¸šçº§ç‰¹æ€§é…ç½®
	enterpriseConfig := k.config.Enterprise

	// åˆå§‹åŒ–è®¢é˜…ç«¯ç§¯å‹æ£€æµ‹å™¨
	if enterpriseConfig.Subscriber.BacklogDetection.Enabled {
		// âœ… æ— é”è¯»å– client å’Œ admin
		client, err := k.getClient()
		if err != nil {
			return fmt.Errorf("failed to get client for backlog detector: %w", err)
		}
		admin, err := k.getAdmin()
		if err != nil {
			return fmt.Errorf("failed to get admin for backlog detector: %w", err)
		}

		backlogConfig := BacklogDetectionConfig{
			MaxLagThreshold:  enterpriseConfig.Subscriber.BacklogDetection.MaxLagThreshold,
			MaxTimeThreshold: enterpriseConfig.Subscriber.BacklogDetection.MaxTimeThreshold,
			CheckInterval:    enterpriseConfig.Subscriber.BacklogDetection.CheckInterval,
		}
		k.backlogDetector = NewBacklogDetector(client, admin, k.config.Consumer.GroupID, backlogConfig)
		k.logger.Info("Subscriber backlog detector initialized",
			zap.Int64("maxLagThreshold", backlogConfig.MaxLagThreshold),
			zap.Duration("maxTimeThreshold", backlogConfig.MaxTimeThreshold),
			zap.Duration("checkInterval", backlogConfig.CheckInterval))
	}

	// åˆå§‹åŒ–å‘é€ç«¯ç§¯å‹æ£€æµ‹å™¨
	if enterpriseConfig.Publisher.BacklogDetection.Enabled {
		// âœ… æ— é”è¯»å– client å’Œ admin
		client, err := k.getClient()
		if err != nil {
			return fmt.Errorf("failed to get client for publisher backlog detector: %w", err)
		}
		admin, err := k.getAdmin()
		if err != nil {
			return fmt.Errorf("failed to get admin for publisher backlog detector: %w", err)
		}

		k.publisherBacklogDetector = NewPublisherBacklogDetector(client, admin, enterpriseConfig.Publisher.BacklogDetection)
		k.logger.Info("Publisher backlog detector initialized",
			zap.Int64("maxQueueDepth", enterpriseConfig.Publisher.BacklogDetection.MaxQueueDepth),
			zap.Duration("maxPublishLatency", enterpriseConfig.Publisher.BacklogDetection.MaxPublishLatency),
			zap.Float64("rateThreshold", enterpriseConfig.Publisher.BacklogDetection.RateThreshold),
			zap.Duration("checkInterval", enterpriseConfig.Publisher.BacklogDetection.CheckInterval))
	}

	k.logger.Info("Enterprise features initialized successfully")
	return nil
}

// Kafka EventBus å®ç°

// Publish å‘å¸ƒæ¶ˆæ¯
// ğŸ”¥ é«˜é¢‘è·¯å¾„ä¼˜åŒ–ï¼šæ— é”æ£€æŸ¥å…³é—­çŠ¶æ€ï¼Œæ— é”è¯»å– producer
func (k *kafkaEventBus) Publish(ctx context.Context, topic string, message []byte) error {
	// ğŸ”¥ P0ä¿®å¤ï¼šæ— é”æ£€æŸ¥å…³é—­çŠ¶æ€
	if k.closed.Load() {
		return fmt.Errorf("kafka eventbus is closed")
	}

	// å‘å¸ƒç«¯æµé‡æ§åˆ¶
	if k.publishRateLimiter != nil {
		if err := k.publishRateLimiter.Wait(ctx); err != nil {
			k.errorCount.Add(1)
			return fmt.Errorf("publisher rate limit error: %w", err)
		}
	}

	// è®¢é˜…ç«¯æµé‡æ§åˆ¶ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰
	if k.rateLimiter != nil {
		if err := k.rateLimiter.Wait(ctx); err != nil {
			k.errorCount.Add(1)
			return fmt.Errorf("subscriber rate limit error: %w", err)
		}
	}

	// âœ… æ— é”è¯»å– asyncProducer
	producer, err := k.getAsyncProducer()
	if err != nil {
		k.errorCount.Add(1)
		return fmt.Errorf("failed to get async producer: %w", err)
	}

	// åˆ›å»ºKafkaæ¶ˆæ¯
	msg := &sarama.ProducerMessage{
		Topic: topic,
		Value: sarama.ByteEncoder(message),
	}

	// ä¼˜åŒ–1ï¼šä½¿ç”¨AsyncProducerå¼‚æ­¥å‘é€ï¼ˆéé˜»å¡ï¼‰
	select {
	case producer.Input() <- msg:
		// æ¶ˆæ¯å·²æäº¤åˆ°å‘é€é˜Ÿåˆ—ï¼ŒæˆåŠŸ/å¤±è´¥ç”±åå°goroutineå¤„ç†
		k.logger.Debug("Message queued for async publishing",
			zap.String("topic", topic))
		return nil
	case <-time.After(100 * time.Millisecond):
		// å‘é€é˜Ÿåˆ—æ»¡ï¼Œåº”ç”¨èƒŒå‹
		k.errorCount.Add(1)
		k.logger.Warn("Async producer input queue full, applying backpressure",
			zap.String("topic", topic))
		// é˜»å¡ç­‰å¾…ï¼Œç¡®ä¿æ¶ˆæ¯ä¸ä¸¢å¤±
		producer.Input() <- msg
		return nil
	}
}

// startPreSubscriptionConsumer å¯åŠ¨é¢„è®¢é˜…æ¶ˆè´¹å¾ªç¯
func (k *kafkaEventBus) startPreSubscriptionConsumer(ctx context.Context) error {
	k.consumerMu.Lock()
	defer k.consumerMu.Unlock()

	// å¦‚æœå·²ç»å¯åŠ¨ï¼Œç›´æ¥è¿”å›
	if k.consumerStarted {
		return nil
	}

	// åˆ›å»ºæ¶ˆè´¹è€…ä¸Šä¸‹æ–‡
	k.consumerCtx, k.consumerCancel = context.WithCancel(ctx)
	k.consumerDone = make(chan struct{})

	// åˆ›å»ºé¢„è®¢é˜…æ¶ˆè´¹è€…å¤„ç†å™¨
	handler := &preSubscriptionConsumerHandler{
		eventBus: k,
	}

	// ä¸šç•Œæœ€ä½³å®è·µï¼šåœ¨å¾ªç¯ä¸­è°ƒç”¨ Consumeï¼Œæ”¯æŒé‡å¹³è¡¡åé‡æ–°è®¢é˜…
	// å‚è€ƒï¼šhttps://pkg.go.dev/github.com/IBM/sarama#ConsumerGroup
	// "Consume should be called inside an infinite loop, when a server-side rebalance happens,
	// the consumer session will need to be recreated to get the new claims"
	go func() {
		defer close(k.consumerDone)

		// ğŸ”¥ P0ä¿®å¤ï¼šæ— é”è¯»å– topicsSnapshot
		topics := k.topicsSnapshot.Load().([]string)
		k.logger.Info("Pre-subscription consumer started",
			zap.Int("topicCount", len(topics)),
			zap.Strings("topics", topics))

		for {
			// æ£€æŸ¥ context æ˜¯å¦å·²å–æ¶ˆ
			if k.consumerCtx.Err() != nil {
				k.logger.Info("Pre-subscription consumer context cancelled")
				return
			}

			// ğŸ”¥ P0ä¿®å¤ï¼šæ— é”è¯»å– topicsSnapshot
			topics := k.topicsSnapshot.Load().([]string)

			// æ£€æŸ¥æ˜¯å¦æœ‰ topic éœ€è¦è®¢é˜…
			if len(topics) == 0 {
				k.logger.Warn("No topics configured for pre-subscription, consumer will wait")
				// ç­‰å¾… context å–æ¶ˆ
				<-k.consumerCtx.Done()
				return
			}

			// å…³é”®ï¼šè°ƒç”¨ Consume è®¢é˜…æ‰€æœ‰é¢„é…ç½®çš„ topic
			// è¿™ä¼šé˜»å¡ç›´åˆ°å‘ç”Ÿé‡å¹³è¡¡æˆ– context å–æ¶ˆ
			k.logger.Info("Pre-subscription consumer consuming topics",
				zap.Strings("topics", topics),
				zap.Int("topicCount", len(topics)))

			// âœ… æ— é”è¯»å– unifiedConsumerGroup
			consumerGroup, err := k.getUnifiedConsumerGroup()
			if err != nil {
				k.logger.Error("Failed to get unified consumer group",
					zap.Error(err))
				// çŸ­æš‚ç­‰å¾…åé‡è¯•
				select {
				case <-k.consumerCtx.Done():
					return
				case <-time.After(2 * time.Second):
					continue
				}
			}

			// âœ… å®‰å…¨ï¼štopics æ˜¯ä¸å¯å˜å‰¯æœ¬ï¼Œä¸ä¼šè¢«ä¿®æ”¹
			err = consumerGroup.Consume(k.consumerCtx, topics, handler)
			if err != nil {
				if k.consumerCtx.Err() != nil {
					k.logger.Info("Pre-subscription consumer context cancelled during Consume")
					return
				}

				k.logger.Error("Pre-subscription consumer error, will retry",
					zap.Error(err))

				// çŸ­æš‚ç­‰å¾…åé‡è¯•
				select {
				case <-k.consumerCtx.Done():
					return
				case <-time.After(2 * time.Second):
					continue
				}
			}

			// Consume æ­£å¸¸è¿”å›ï¼ˆé€šå¸¸æ˜¯å› ä¸ºé‡å¹³è¡¡ï¼‰ï¼Œç»§ç»­å¾ªç¯
			k.logger.Info("Pre-subscription consumer session ended, restarting...")
		}
	}()

	k.consumerStarted = true

	// ğŸ”¥ P0ä¿®å¤ï¼šæ— é”è¯»å– topicsSnapshot
	topics := k.topicsSnapshot.Load().([]string)
	k.logger.Info("Pre-subscription consumer system started",
		zap.Int("topicCount", len(topics)))

	// ä¼˜åŒ–1&4ï¼šæ·»åŠ 3ç§’Consumeré¢„çƒ­æœºåˆ¶ + çŠ¶æ€ç›‘æ§
	k.warmupMu.Lock()
	k.warmupStartTime = time.Now()
	k.warmupCompleted = false
	k.warmupMu.Unlock()

	k.logger.Info("Consumer warming up for 3 seconds...",
		zap.Time("warmupStartTime", k.warmupStartTime))
	time.Sleep(3 * time.Second)

	k.warmupMu.Lock()
	k.warmupCompleted = true
	warmupDuration := time.Since(k.warmupStartTime)
	k.warmupMu.Unlock()

	k.logger.Info("Consumer warmup completed, ready for optimal performance",
		zap.Duration("warmupDuration", warmupDuration),
		zap.Bool("warmupCompleted", true))

	return nil
}

// ä¼˜åŒ–4ï¼šIsWarmupCompleted æ£€æŸ¥é¢„çƒ­çŠ¶æ€
func (k *kafkaEventBus) IsWarmupCompleted() bool {
	k.warmupMu.RLock()
	defer k.warmupMu.RUnlock()
	return k.warmupCompleted
}

// ä¼˜åŒ–4ï¼šGetWarmupInfo è·å–é¢„çƒ­ä¿¡æ¯
func (k *kafkaEventBus) GetWarmupInfo() (completed bool, duration time.Duration) {
	k.warmupMu.RLock()
	defer k.warmupMu.RUnlock()

	completed = k.warmupCompleted
	if !k.warmupStartTime.IsZero() {
		if completed {
			duration = 3 * time.Second // é¢„çƒ­å®Œæˆï¼Œè¿”å›å›ºå®šæ—¶é•¿
		} else {
			duration = time.Since(k.warmupStartTime) // é¢„çƒ­ä¸­ï¼Œè¿”å›å·²ç”¨æ—¶é•¿
		}
	}
	return
}

// activateTopicHandler æ¿€æ´»topicå¤„ç†å™¨ï¼ˆé¢„è®¢é˜…æ¨¡å¼ï¼‰
// ğŸ”¥ P0ä¿®å¤ï¼šä½¿ç”¨ sync.Map å­˜å‚¨ï¼Œæ— é”æ“ä½œ
func (k *kafkaEventBus) activateTopicHandler(topic string, handler MessageHandler, isEnvelope bool) {
	wrapper := &handlerWrapper{
		handler:    handler,
		isEnvelope: isEnvelope,
	}
	k.activeTopicHandlers.Store(topic, wrapper)

	// è®¡ç®—å½“å‰æ¿€æ´»çš„ topic æ•°é‡
	count := 0
	k.activeTopicHandlers.Range(func(key, value interface{}) bool {
		count++
		return true
	})

	k.logger.Info("Topic handler activated",
		zap.String("topic", topic),
		zap.Bool("isEnvelope", isEnvelope),
		zap.Int("totalActiveTopics", count))
}

// deactivateTopicHandler åœç”¨topicå¤„ç†å™¨ï¼ˆé¢„è®¢é˜…æ¨¡å¼ï¼‰
// ğŸ”¥ P0ä¿®å¤ï¼šä½¿ç”¨ sync.Map å­˜å‚¨ï¼Œæ— é”æ“ä½œ
func (k *kafkaEventBus) deactivateTopicHandler(topic string) {
	k.activeTopicHandlers.Delete(topic)

	// è®¡ç®—å½“å‰æ¿€æ´»çš„ topic æ•°é‡
	count := 0
	k.activeTopicHandlers.Range(func(key, value interface{}) bool {
		count++
		return true
	})

	k.logger.Info("Topic handler deactivated",
		zap.String("topic", topic),
		zap.Int("totalActiveTopics", count))
}

// SetPreSubscriptionTopics è®¾ç½®é¢„è®¢é˜…topicåˆ—è¡¨ï¼ˆä¼ä¸šçº§ç”Ÿäº§ç¯å¢ƒAPIï¼‰
//
// ä¸šç•Œæœ€ä½³å®è·µï¼šåœ¨è°ƒç”¨ Subscribe ä¹‹å‰ï¼Œå…ˆè®¾ç½®æ‰€æœ‰éœ€è¦è®¢é˜…çš„ topic
// è¿™æ ·å¯ä»¥é¿å… Kafka Consumer Group çš„é¢‘ç¹é‡å¹³è¡¡ï¼Œæé«˜æ€§èƒ½å’Œç¨³å®šæ€§
//
// ä½¿ç”¨åœºæ™¯ï¼š
//  1. åˆ›å»º EventBus å®ä¾‹
//  2. è°ƒç”¨ SetPreSubscriptionTopics è®¾ç½®æ‰€æœ‰ topic
//  3. è°ƒç”¨ Subscribe è®¢é˜…å„ä¸ª topicï¼ˆæ¶ˆè´¹è€…ä¼šä¸€æ¬¡æ€§è®¢é˜…æ‰€æœ‰ topicï¼‰
//
// ç¤ºä¾‹ï¼š
//
//	eventBus, _ := NewKafkaEventBus(config)
//	eventBus.SetPreSubscriptionTopics([]string{"topic1", "topic2", "topic3"})
//	eventBus.Subscribe(ctx, "topic1", handler1)
//	eventBus.Subscribe(ctx, "topic2", handler2)
//	eventBus.Subscribe(ctx, "topic3", handler3)
//
// å‚è€ƒï¼š
//   - Confluent å®˜æ–¹æ–‡æ¡£ï¼šé¢„è®¢é˜…æ¨¡å¼é¿å…é‡å¹³è¡¡
//   - LinkedIn å®è·µï¼šä¸€æ¬¡æ€§è®¢é˜…æ‰€æœ‰ topic
//   - Uber å®è·µï¼šé¢„é…ç½® topic åˆ—è¡¨
func (k *kafkaEventBus) SetPreSubscriptionTopics(topics []string) {
	k.allPossibleTopicsMu.Lock()
	defer k.allPossibleTopicsMu.Unlock()

	// ğŸ”¥ P0ä¿®å¤ï¼šåˆ›å»ºä¸å¯å˜å‰¯æœ¬å¹¶æ›´æ–°å¿«ç…§
	newTopics := make([]string, len(topics))
	copy(newTopics, topics)

	k.allPossibleTopics = newTopics
	k.topicsSnapshot.Store(newTopics)

	k.logger.Info("Pre-subscription topics configured",
		zap.Strings("topics", newTopics),
		zap.Int("topicCount", len(newTopics)))
}

// addTopicToPreSubscription æ·»åŠ topicåˆ°é¢„è®¢é˜…åˆ—è¡¨ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰
// ğŸ”¥ P0ä¿®å¤ï¼šä½¿ç”¨ atomic.Value å­˜å‚¨ä¸å¯å˜åˆ‡ç‰‡å¿«ç…§ï¼Œæ¶ˆé™¤æ•°æ®ç«æ€
func (k *kafkaEventBus) addTopicToPreSubscription(topic string) {
	k.allPossibleTopicsMu.Lock()
	defer k.allPossibleTopicsMu.Unlock()

	// æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
	for _, existingTopic := range k.allPossibleTopics {
		if existingTopic == topic {
			return // å·²å­˜åœ¨ï¼Œæ— éœ€æ·»åŠ 
		}
	}

	// åˆ›å»ºæ–°çš„ä¸å¯å˜å‰¯æœ¬
	newTopics := make([]string, len(k.allPossibleTopics)+1)
	copy(newTopics, k.allPossibleTopics)
	newTopics[len(k.allPossibleTopics)] = topic

	// æ›´æ–°ä¸»å‰¯æœ¬å’Œå¿«ç…§
	k.allPossibleTopics = newTopics
	k.topicsSnapshot.Store(newTopics)

	k.logger.Info("Topic added to pre-subscription list",
		zap.String("topic", topic),
		zap.Int("totalTopics", len(newTopics)))
}

// stopPreSubscriptionConsumer åœæ­¢é¢„è®¢é˜…æ¶ˆè´¹å¾ªç¯
func (k *kafkaEventBus) stopPreSubscriptionConsumer() {
	k.consumerMu.Lock()
	defer k.consumerMu.Unlock()

	if k.consumerCancel != nil {
		k.consumerCancel()
		<-k.consumerDone
		k.consumerCancel = nil
		k.consumerStarted = false
		k.logger.Info("Pre-subscription consumer stopped")
	}
}

// containsString æ£€æŸ¥sliceä¸­æ˜¯å¦åŒ…å«æŒ‡å®šå…ƒç´ 
func containsString(slice []string, item string) bool {
	for _, s := range slice {
		if s == item {
			return true
		}
	}
	return false
}

// Subscribe è®¢é˜…åŸå§‹æ¶ˆæ¯ï¼ˆä½¿ç”¨ Hollywood Actor Pool + Round-Robin è½®è¯¢ï¼‰
//
// ç‰¹ç‚¹ï¼š
// - æ¶ˆæ¯æ ¼å¼ï¼šåŸå§‹å­—èŠ‚æ•°æ®
// - å¤„ç†æ¨¡å¼ï¼šRound-Robin è½®è¯¢è·¯ç”±åˆ° Hollywood Actor Poolï¼Œæ— é¡ºåºä¿è¯
// - æ€§èƒ½ï¼šæè‡´æ€§èƒ½ï¼Œå¾®ç§’çº§å»¶è¿Ÿ
// - èšåˆIDï¼šé€šå¸¸æ— æ³•ä»åŸå§‹æ¶ˆæ¯ä¸­æå–èšåˆID
// - è·¯ç”±ç­–ç•¥ï¼šRound-Robin è½®è¯¢ï¼ˆä¸åŸå…¨å±€ Worker Pool è¡Œä¸ºä¸€è‡´ï¼‰
//
// é€‚ç”¨åœºæ™¯ï¼š
// - ç®€å•æ¶ˆæ¯ä¼ é€’ï¼ˆé€šçŸ¥ã€æé†’ï¼‰
// - ç¼“å­˜å¤±æ•ˆæ¶ˆæ¯
// - ç³»ç»Ÿç›‘æ§æŒ‡æ ‡
// - ä¸éœ€è¦é¡ºåºä¿è¯çš„ä¸šåŠ¡åœºæ™¯
//
// ç¤ºä¾‹ï¼š
//
//	bus.Subscribe(ctx, "notifications", func(ctx context.Context, data []byte) error {
//	    var notification Notification
//	    json.Unmarshal(data, &notification)
//	    return processNotification(notification) // ç›´æ¥å¹¶å‘å¤„ç†
//	})
//
// Subscribe è®¢é˜…ä¸»é¢˜
// âœ… ä½é¢‘è·¯å¾„ï¼šä¿ç•™é”ï¼Œä¿æŒä»£ç æ¸…æ™°
func (k *kafkaEventBus) Subscribe(ctx context.Context, topic string, handler MessageHandler) error {
	k.mu.Lock()

	// ğŸ”¥ P0ä¿®å¤ï¼šä½¿ç”¨ atomic.Bool è¯»å–å…³é—­çŠ¶æ€
	if k.closed.Load() {
		k.mu.Unlock()
		return fmt.Errorf("kafka eventbus is closed")
	}

	// é¢„è®¢é˜…æ¨¡å¼ - æ¿€æ´»topicå¤„ç†å™¨

	// âœ… ä½¿ç”¨ sync.Map å­˜å‚¨è®¢é˜…ä¿¡æ¯ï¼ˆç”¨äºé‡è¿åæ¢å¤ï¼‰
	// ä½¿ç”¨ LoadOrStore æ£€æŸ¥é‡å¤è®¢é˜…
	if _, loaded := k.subscriptions.LoadOrStore(topic, handler); loaded {
		k.mu.Unlock()
		return fmt.Errorf("already subscribed to topic: %s", topic)
	}

	// å…¨å±€ Hollywood Actor Pool å·²åœ¨åˆå§‹åŒ–æ—¶åˆ›å»ºï¼Œæ‰€æœ‰ topic å…±äº«åŒä¸€ä¸ª Actor Pool

	// å…³é”®ä¿®å¤ï¼šæ·»åŠ  topic åˆ°é¢„è®¢é˜…åˆ—è¡¨
	k.addTopicToPreSubscription(topic)

	// æ¿€æ´»topicå¤„ç†å™¨ï¼ˆç«‹å³ç”Ÿæ•ˆï¼‰
	// â­ æ™®é€š Subscribeï¼šisEnvelope = falseï¼ˆat-most-once è¯­ä¹‰ï¼‰
	k.activateTopicHandler(topic, handler, false)

	// ğŸ”§ ä¿®å¤æ­»é”ï¼šåœ¨é‡Šæ”¾é”ä¹‹å‰è®°å½•æ—¥å¿—ï¼Œç„¶åé‡Šæ”¾é”å†å¯åŠ¨consumer
	k.logger.Info("Subscribed to topic via pre-subscription consumer",
		zap.String("topic", topic),
		zap.String("groupID", k.config.Consumer.GroupID))

	// é‡Šæ”¾é”ï¼Œé¿å…åœ¨å¯åŠ¨consumeræ—¶æŒæœ‰é”å¯¼è‡´æ­»é”
	k.mu.Unlock()

	// å¯åŠ¨é¢„è®¢é˜…æ¶ˆè´¹è€…ï¼ˆå¦‚æœè¿˜æœªå¯åŠ¨ï¼‰
	// æ³¨æ„ï¼šè¿™é‡Œä¸æŒæœ‰k.mué”ï¼Œé¿å…åœ¨sleepæœŸé—´é˜»å¡å…¶ä»–æ“ä½œï¼ˆå¦‚Publishï¼‰
	if err := k.startPreSubscriptionConsumer(ctx); err != nil {
		return fmt.Errorf("failed to start pre-subscription consumer: %w", err)
	}

	return nil
}

// healthCheck å†…éƒ¨å¥åº·æ£€æŸ¥ï¼ˆä¸å¯¹å¤–æš´éœ²ï¼‰
func (k *kafkaEventBus) healthCheck(ctx context.Context) error {
	// ğŸ”¥ P0ä¿®å¤ï¼šæ— é”æ£€æŸ¥å…³é—­çŠ¶æ€
	if k.closed.Load() {
		return fmt.Errorf("kafka eventbus is closed")
	}

	// âœ… æ— é”è¯»å– client
	client, err := k.getClient()
	if err != nil {
		return fmt.Errorf("failed to get client: %w", err)
	}

	// æ£€æŸ¥å®¢æˆ·ç«¯è¿æ¥
	if !client.Closed() {
		// å°è¯•è·å–brokerä¿¡æ¯
		brokers := client.Brokers()
		if len(brokers) == 0 {
			return fmt.Errorf("no available brokers")
		}

		// æ£€æŸ¥è‡³å°‘ä¸€ä¸ªbrokeræ˜¯å¦å¯è¾¾
		for _, broker := range brokers {
			if connected, _ := broker.Connected(); connected {
				k.logger.Debug("Kafka eventbus health check passed",
					zap.Int("brokers", len(brokers)),
					zap.String("connectedBroker", broker.Addr()))
				return nil
			}
		}

		return fmt.Errorf("no connected brokers available")
	}

	return fmt.Errorf("kafka client is closed")
}

// CheckConnection æ£€æŸ¥ Kafka è¿æ¥çŠ¶æ€
func (k *kafkaEventBus) CheckConnection(ctx context.Context) error {
	k.mu.Lock()
	defer k.mu.Unlock()

	// ğŸ”¥ P0ä¿®å¤ï¼šä½¿ç”¨ atomic.Bool è¯»å–å…³é—­çŠ¶æ€
	if k.closed.Load() {
		return fmt.Errorf("kafka eventbus is closed")
	}

	// âœ… æ— é”è¯»å– client
	client, err := k.getClient()
	if err != nil {
		return fmt.Errorf("failed to get client: %w", err)
	}

	if client.Closed() {
		return fmt.Errorf("kafka client is closed")
	}

	// æ£€æŸ¥ broker è¿æ¥
	brokers := client.Brokers()
	if len(brokers) == 0 {
		return fmt.Errorf("no available brokers")
	}

	connectedBrokers := 0
	for _, broker := range brokers {
		if connected, _ := broker.Connected(); connected {
			connectedBrokers++
		}
	}

	if connectedBrokers == 0 {
		return fmt.Errorf("no connected brokers available")
	}

	k.logger.Debug("Kafka connection check passed",
		zap.Int("totalBrokers", len(brokers)),
		zap.Int("connectedBrokers", connectedBrokers))

	return nil
}

// CheckMessageTransport æ£€æŸ¥ç«¯åˆ°ç«¯æ¶ˆæ¯ä¼ è¾“
func (k *kafkaEventBus) CheckMessageTransport(ctx context.Context) error {
	testTopic := "health_check_topic"
	testMessage := fmt.Sprintf("health-check-%d", time.Now().UnixNano())

	// åˆ›å»ºå¸¦è¶…æ—¶çš„ä¸Šä¸‹æ–‡
	ctx, cancel := context.WithTimeout(ctx, 10*time.Second)
	defer cancel()

	// âœ… æ£€æŸ¥æ˜¯å¦æ”¯æŒç«¯åˆ°ç«¯æµ‹è¯•
	consumer, err := k.getConsumer()
	if err == nil && consumer != nil {
		return k.performKafkaEndToEndTest(ctx, testTopic, testMessage)
	}

	// å¦‚æœæ²¡æœ‰æ¶ˆè´¹è€…ç»„ï¼Œåªæµ‹è¯•å‘å¸ƒèƒ½åŠ›
	start := time.Now()
	err = k.Publish(ctx, testTopic, []byte(testMessage))
	publishLatency := time.Since(start)

	if err != nil {
		k.logger.Error("Kafka health check message transport failed",
			zap.Error(err),
			zap.Duration("publishLatency", publishLatency))
		return fmt.Errorf("failed to publish health check message: %w", err)
	}

	k.logger.Debug("Kafka health check message transport successful",
		zap.Duration("publishLatency", publishLatency))

	return nil
}

// performKafkaEndToEndTest æ‰§è¡Œ Kafka ç«¯åˆ°ç«¯æµ‹è¯•
func (k *kafkaEventBus) performKafkaEndToEndTest(ctx context.Context, testTopic, testMessage string) error {
	// åˆ›å»ºæ¥æ”¶é€šé“
	receiveChan := make(chan string, 1)
	errorChan := make(chan error, 1)

	// âœ… æ— é”è¯»å– consumer
	consumer, err := k.getConsumer()
	if err != nil {
		return fmt.Errorf("failed to get consumer: %w", err)
	}

	// åˆ›å»ºåˆ†åŒºæ¶ˆè´¹è€…æ¥æ¥æ”¶å¥åº·æ£€æŸ¥æ¶ˆæ¯
	partitionConsumer, err := consumer.ConsumePartition(testTopic, 0, sarama.OffsetNewest)
	if err != nil {
		k.logger.Warn("Failed to create partition consumer for health check, falling back to publish-only test",
			zap.Error(err))
		// å›é€€åˆ°åªæµ‹è¯•å‘å¸ƒ
		start := time.Now()
		if err := k.Publish(ctx, testTopic, []byte(testMessage)); err != nil {
			return fmt.Errorf("failed to publish health check message: %w", err)
		}
		publishLatency := time.Since(start)
		k.logger.Debug("Kafka health check message published (no consumer test)",
			zap.Duration("publishLatency", publishLatency))
		return nil
	}
	defer partitionConsumer.Close()

	// å¯åŠ¨æ¶ˆè´¹è€…ï¼ˆåœ¨åå°ï¼‰
	go func() {
		defer close(receiveChan)
		defer close(errorChan)

		for {
			select {
			case message := <-partitionConsumer.Messages():
				if message != nil {
					receivedMsg := string(message.Value)
					if receivedMsg == testMessage {
						select {
						case receiveChan <- receivedMsg:
							return
						case <-ctx.Done():
							return
						}
					}
				}
			case err := <-partitionConsumer.Errors():
				if err != nil {
					select {
					case errorChan <- err:
						return
					case <-ctx.Done():
						return
					}
				}
			case <-ctx.Done():
				return
			}
		}
	}()

	// ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿æ¶ˆè´¹è€…å‡†å¤‡å°±ç»ª
	time.Sleep(200 * time.Millisecond)

	// å‘å¸ƒå¥åº·æ£€æŸ¥æ¶ˆæ¯
	start := time.Now()
	if err := k.Publish(ctx, testTopic, []byte(testMessage)); err != nil {
		return fmt.Errorf("failed to publish health check message: %w", err)
	}
	publishLatency := time.Since(start)

	// ç­‰å¾…æ¥æ”¶æ¶ˆæ¯æˆ–è¶…æ—¶
	select {
	case receivedMsg := <-receiveChan:
		totalLatency := time.Since(start)
		k.logger.Debug("Kafka end-to-end health check successful",
			zap.Duration("publishLatency", publishLatency),
			zap.Duration("totalLatency", totalLatency),
			zap.String("message", receivedMsg))
		return nil

	case err := <-errorChan:
		return fmt.Errorf("kafka health check consumer error: %w", err)

	case <-ctx.Done():
		return fmt.Errorf("kafka health check timeout: message not received within timeout period")

	case <-time.After(8 * time.Second):
		return fmt.Errorf("kafka health check timeout: message not received within 8 seconds")
	}
}

// GetEventBusMetrics è·å– Kafka EventBus æ€§èƒ½æŒ‡æ ‡
func (k *kafkaEventBus) GetEventBusMetrics() EventBusHealthMetrics {
	k.mu.Lock()
	defer k.mu.Unlock()

	connectionStatus := "disconnected"
	brokerCount := 0
	topicCount := 0

	// âœ… æ— é”è¯»å– client
	client, err := k.getClient()
	// ğŸ”¥ P0ä¿®å¤ï¼šä½¿ç”¨ atomic.Bool è¯»å–å…³é—­çŠ¶æ€
	if err == nil && !k.closed.Load() && !client.Closed() {
		brokers := client.Brokers()
		brokerCount = len(brokers)

		connectedBrokers := 0
		for _, broker := range brokers {
			if connected, _ := broker.Connected(); connected {
				connectedBrokers++
			}
		}

		if connectedBrokers > 0 {
			connectionStatus = "connected"
		}

		// è·å– topic æ•°é‡
		if topics, err := client.Topics(); err == nil {
			topicCount = len(topics)
		}
	}

	return EventBusHealthMetrics{
		ConnectionStatus:    connectionStatus,
		PublishLatency:      0,                          // TODO: å®é™…æµ‹é‡å¹¶ç¼“å­˜
		SubscribeLatency:    0,                          // TODO: å®é™…æµ‹é‡å¹¶ç¼“å­˜
		LastSuccessTime:     time.Now(),                 // TODO: å®é™…è·Ÿè¸ª
		LastFailureTime:     time.Time{},                // TODO: å®é™…è·Ÿè¸ª
		ConsecutiveFailures: 0,                          // TODO: å®é™…ç»Ÿè®¡
		ThroughputPerSecond: k.publishedMessages.Load(), // ç®€åŒ–å®ç°
		MessageBacklog:      0,                          // TODO: å®é™…è®¡ç®—
		ReconnectCount:      0,                          // TODO: å®é™…ç»Ÿè®¡
		BrokerCount:         brokerCount,
		TopicCount:          topicCount,
	}
}

// Close å…³é—­è¿æ¥
func (k *kafkaEventBus) Close() error {
	k.mu.Lock()

	// åœæ­¢é¢„è®¢é˜…æ¶ˆè´¹è€…ç»„
	k.stopPreSubscriptionConsumer()

	// â­ å…³é—­å…¨å±€ Hollywood Actor Pool
	if k.globalActorPool != nil {
		k.globalActorPool.Stop()
	}

	defer k.mu.Unlock()

	// ğŸ”¥ P0ä¿®å¤ï¼šä½¿ç”¨ atomic.Bool è¯»å–å…³é—­çŠ¶æ€
	if k.closed.Load() {
		return nil
	}

	var errors []error

	// å…³é—­é¡ºåºå¾ˆé‡è¦ï¼š
	// 1. å…ˆå…³é—­ä» client åˆ›å»ºçš„ç»„ä»¶ï¼ˆunifiedConsumerGroup, admin, consumer, asyncProducerï¼‰
	// 2. æœ€åå…³é—­ clientï¼ˆå› ä¸ºå…¶ä»–ç»„ä»¶ä¾èµ–å®ƒï¼‰
	// æ³¨æ„ï¼šunifiedConsumerGroup.Close() ä¸ä¼šå…³é—­åº•å±‚çš„ client

	// å…³é—­ç»Ÿä¸€æ¶ˆè´¹è€…ç»„
	if consumerGroup, err := k.getUnifiedConsumerGroup(); err == nil {
		if err := consumerGroup.Close(); err != nil {
			errors = append(errors, fmt.Errorf("failed to close unified consumer group: %w", err))
		}
	}

	// å…³é—­æ¶ˆè´¹è€…
	if consumer, err := k.getConsumer(); err == nil {
		if err := consumer.Close(); err != nil {
			errors = append(errors, fmt.Errorf("failed to close kafka consumer: %w", err))
		}
	}

	// ä¼˜åŒ–1ï¼šå…³é—­AsyncProducer
	if producer, err := k.getAsyncProducer(); err == nil {
		if err := producer.Close(); err != nil {
			errors = append(errors, fmt.Errorf("failed to close kafka async producer: %w", err))
		}
	}

	// å…³é—­ç®¡ç†å®¢æˆ·ç«¯ï¼ˆåœ¨å…³é—­ client ä¹‹å‰ï¼‰
	if admin, err := k.getAdmin(); err == nil {
		if err := admin.Close(); err != nil {
			errors = append(errors, fmt.Errorf("failed to close kafka admin: %w", err))
		}
	}

	// æœ€åå…³é—­å®¢æˆ·ç«¯ï¼ˆæ‰€æœ‰å…¶ä»–ç»„ä»¶éƒ½ä¾èµ–å®ƒï¼‰
	// æ³¨æ„ï¼šæŸäº›ç‰ˆæœ¬çš„ sarama å¯èƒ½åœ¨ ConsumerGroup.Close() æ—¶å·²ç»å…³é—­äº† client
	// å› æ­¤æˆ‘ä»¬éœ€è¦æ£€æŸ¥ client æ˜¯å¦å·²ç»å…³é—­
	if client, err := k.getClient(); err == nil {
		if err := client.Close(); err != nil {
			// å¿½ç•¥ "client already closed" é”™è¯¯
			if err.Error() != "kafka: tried to use a client that was closed" {
				errors = append(errors, fmt.Errorf("failed to close kafka client: %w", err))
			}
		}
	}

	// ğŸ”¥ P0ä¿®å¤ï¼šä½¿ç”¨ atomic.Bool è®¾ç½®å…³é—­çŠ¶æ€
	k.closed.Store(true)
	k.logger.Info("Kafka eventbus closed successfully")

	if len(errors) > 0 {
		return fmt.Errorf("errors during kafka eventbus close: %v", errors)
	}

	return nil
}

// RegisterReconnectCallback æ³¨å†Œé‡è¿å›è°ƒ
func (k *kafkaEventBus) RegisterReconnectCallback(callback ReconnectCallback) error {
	k.mu.Lock()
	defer k.mu.Unlock()

	k.reconnectCallback = callback
	k.logger.Info("Reconnect callback registered for kafka eventbus")
	return nil
}

// reconnect æ‰§è¡Œé‡è¿é€»è¾‘
func (k *kafkaEventBus) reconnect(ctx context.Context) error {
	k.logger.Warn("Kafka connection failed, attempting to reconnect...")

	// è®°å½•é‡è¿æ—¶é—´
	k.lastReconnectTime.Store(time.Now())

	backoff := k.reconnectConfig.InitialBackoff

	for attempt := 1; attempt <= k.reconnectConfig.MaxAttempts; attempt++ {
		k.logger.Info("Reconnection attempt",
			zap.Int("attempt", attempt),
			zap.Int("maxAttempts", k.reconnectConfig.MaxAttempts))

		// å°è¯•é‡æ–°åˆå§‹åŒ–è¿æ¥
		if err := k.reinitializeConnection(); err != nil {
			k.logger.Error("Reconnection attempt failed",
				zap.Int("attempt", attempt),
				zap.Error(err))

			// å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…åé‡è¯•
			if attempt < k.reconnectConfig.MaxAttempts {
				select {
				case <-ctx.Done():
					return fmt.Errorf("reconnection cancelled: %w", ctx.Err())
				case <-time.After(backoff):
					// æŒ‡æ•°é€€é¿
					backoff = time.Duration(float64(backoff) * k.reconnectConfig.BackoffFactor)
					if backoff > k.reconnectConfig.MaxBackoff {
						backoff = k.reconnectConfig.MaxBackoff
					}
				}
			}
			continue
		}

		// é‡è¿æˆåŠŸï¼Œæ¢å¤è®¢é˜…
		if err := k.restoreSubscriptions(ctx); err != nil {
			k.logger.Error("Failed to restore subscriptions after reconnect", zap.Error(err))
			return fmt.Errorf("failed to restore subscriptions: %w", err)
		}

		// é‡ç½®å¤±è´¥è®¡æ•°
		k.failureCount.Store(0)

		k.logger.Info("Kafka reconnection successful", zap.Int("attempt", attempt))

		// è°ƒç”¨é‡è¿å›è°ƒ
		if k.reconnectCallback != nil {
			k.reconnectCallback(ctx)
		}

		return nil
	}

	return fmt.Errorf("failed to reconnect after %d attempts", k.reconnectConfig.MaxAttempts)
}

// reinitializeConnection é‡æ–°åˆå§‹åŒ–è¿æ¥
func (k *kafkaEventBus) reinitializeConnection() error {
	k.mu.Lock()
	defer k.mu.Unlock()

	// âœ… å…³é—­ç°æœ‰è¿æ¥
	if producer, err := k.getAsyncProducer(); err == nil && producer != nil {
		producer.Close()
	}
	if consumer, err := k.getConsumer(); err == nil && consumer != nil {
		consumer.Close()
	}
	if admin, err := k.getAdmin(); err == nil && admin != nil {
		admin.Close()
	}
	if client, err := k.getClient(); err == nil && client != nil {
		client.Close()
	}

	// åˆ›å»ºSaramaé…ç½®
	saramaConfig := sarama.NewConfig()
	// å·²åºŸå¼ƒï¼šä½¿ç”¨æ–°çš„ç›´æ¥é…ç½®æ–¹å¼
	// if err := configureSarama(saramaConfig, k.config); err != nil {
	//	return fmt.Errorf("failed to configure sarama: %w", err)
	// }

	// é‡æ–°åˆ›å»ºå®¢æˆ·ç«¯
	client, err := sarama.NewClient(k.config.Brokers, saramaConfig)
	if err != nil {
		return fmt.Errorf("failed to create kafka client: %w", err)
	}

	// ä¼˜åŒ–1ï¼šé‡æ–°åˆ›å»ºAsyncProducer
	asyncProducer, err := sarama.NewAsyncProducerFromClient(client)
	if err != nil {
		client.Close()
		return fmt.Errorf("failed to create kafka async producer: %w", err)
	}

	// é‡æ–°åˆ›å»ºæ¶ˆè´¹è€…
	consumer, err := sarama.NewConsumerFromClient(client)
	if err != nil {
		asyncProducer.Close()
		client.Close()
		return fmt.Errorf("failed to create kafka consumer: %w", err)
	}

	// é‡æ–°åˆ›å»ºç®¡ç†å®¢æˆ·ç«¯
	admin, err := sarama.NewClusterAdminFromClient(client)
	if err != nil {
		consumer.Close()
		asyncProducer.Close()
		client.Close()
		return fmt.Errorf("failed to create kafka admin: %w", err)
	}

	// âœ… ä½¿ç”¨ atomic.Value å­˜å‚¨
	k.client.Store(client)
	k.asyncProducer.Store(asyncProducer)
	k.consumer.Store(consumer)
	k.admin.Store(admin)

	// ä¼˜åŒ–1ï¼šé‡æ–°å¯åŠ¨AsyncProducerå¤„ç†goroutine
	go k.handleAsyncProducerSuccess()
	go k.handleAsyncProducerErrors()

	k.logger.Info("Kafka connection reinitialized successfully (AsyncProducer mode)")
	return nil
}

// restoreSubscriptions æ¢å¤è®¢é˜…
func (k *kafkaEventBus) restoreSubscriptions(ctx context.Context) error {
	// âœ… ä½¿ç”¨ sync.Map éå†è®¢é˜…
	subscriptions := make(map[string]MessageHandler)
	k.subscriptions.Range(func(key, value interface{}) bool {
		topic := key.(string)
		handler := value.(MessageHandler)
		subscriptions[topic] = handler
		return true
	})

	for topic, handler := range subscriptions {
		k.logger.Info("Restoring subscription", zap.String("topic", topic))
		if err := k.Subscribe(ctx, topic, handler); err != nil {
			k.logger.Error("Failed to restore subscription",
				zap.String("topic", topic),
				zap.Error(err))
			return fmt.Errorf("failed to restore subscription for topic %s: %w", topic, err)
		}
	}

	k.logger.Info("All subscriptions restored successfully",
		zap.Int("count", len(subscriptions)))
	return nil
}

// SetReconnectConfig è®¾ç½®é‡è¿é…ç½®
func (k *kafkaEventBus) SetReconnectConfig(config ReconnectConfig) error {
	k.mu.Lock()
	defer k.mu.Unlock()

	k.reconnectConfig = config
	k.logger.Info("Reconnect config updated",
		zap.Int("maxAttempts", config.MaxAttempts),
		zap.Duration("initialBackoff", config.InitialBackoff),
		zap.Duration("maxBackoff", config.MaxBackoff),
		zap.Float64("backoffFactor", config.BackoffFactor),
		zap.Int("failureThreshold", config.FailureThreshold))
	return nil
}

// GetReconnectStatus è·å–é‡è¿çŠ¶æ€
func (k *kafkaEventBus) GetReconnectStatus() ReconnectStatus {
	failures := k.failureCount.Load()
	lastReconnect := k.lastReconnectTime.Load().(time.Time)

	return ReconnectStatus{
		FailureCount:      int(failures),
		LastReconnectTime: lastReconnect,
		IsReconnecting:    false, // ç®€åŒ–å®ç°ï¼Œå®é™…å¯ä»¥æ·»åŠ çŠ¶æ€è·Ÿè¸ª
		Config:            k.reconnectConfig,
	}
}

// ReconnectStatus é‡è¿çŠ¶æ€
type ReconnectStatus struct {
	FailureCount      int             `json:"failure_count"`
	LastReconnectTime time.Time       `json:"last_reconnect_time"`
	IsReconnecting    bool            `json:"is_reconnecting"`
	Config            ReconnectConfig `json:"config"`
}

// ========== ç”Ÿå‘½å‘¨æœŸç®¡ç† ==========

// Start å¯åŠ¨äº‹ä»¶æ€»çº¿
func (k *kafkaEventBus) Start(ctx context.Context) error {
	k.mu.Lock()
	defer k.mu.Unlock()

	// ğŸ”¥ P0ä¿®å¤ï¼šä½¿ç”¨ atomic.Bool è¯»å–å…³é—­çŠ¶æ€
	if k.closed.Load() {
		return fmt.Errorf("kafka eventbus is closed")
	}

	k.logger.Info("Kafka eventbus started successfully")
	return nil
}

// Stop åœæ­¢äº‹ä»¶æ€»çº¿
func (k *kafkaEventBus) Stop() error {
	return k.Close()
}

// ========== é«˜çº§å‘å¸ƒåŠŸèƒ½ ==========

// PublishWithOptions ä½¿ç”¨é€‰é¡¹å‘å¸ƒæ¶ˆæ¯
func (k *kafkaEventBus) PublishWithOptions(ctx context.Context, topic string, message []byte, opts PublishOptions) error {
	start := time.Now()

	// åº”ç”¨æ¶ˆæ¯æ ¼å¼åŒ–å™¨
	if k.messageFormatter != nil {
		// ç”Ÿæˆæ¶ˆæ¯UUID
		msgUUID := fmt.Sprintf("%d", time.Now().UnixNano())
		formattedMsg, err := k.messageFormatter.FormatMessage(msgUUID, opts.AggregateID, message)
		if err != nil {
			k.logger.Error("Failed to format message", zap.Error(err))
			return fmt.Errorf("failed to format message: %w", err)
		}

		// è®¾ç½®å…ƒæ•°æ®
		if opts.Metadata != nil {
			if err := k.messageFormatter.SetMetadata(formattedMsg, opts.Metadata); err != nil {
				k.logger.Error("Failed to set metadata", zap.Error(err))
				return fmt.Errorf("failed to set metadata: %w", err)
			}
		}

		// å°†æ ¼å¼åŒ–åçš„æ¶ˆæ¯è½¬æ¢ä¸ºå­—èŠ‚
		if formattedMsg.Payload != nil {
			message = formattedMsg.Payload
		}
	}

	// æ„å»º Kafka æ¶ˆæ¯
	kafkaMsg := &sarama.ProducerMessage{
		Topic: topic,
		Value: sarama.ByteEncoder(message),
	}

	// è®¾ç½®æ¶ˆæ¯å¤´
	if opts.Metadata != nil {
		headers := make([]sarama.RecordHeader, 0, len(opts.Metadata))
		for key, value := range opts.Metadata {
			headers = append(headers, sarama.RecordHeader{
				Key:   []byte(key),
				Value: []byte(value),
			})
		}
		kafkaMsg.Headers = headers
	}

	// è®¾ç½®åˆ†åŒºé”®ï¼ˆå¦‚æœæä¾›äº† AggregateIDï¼‰
	if opts.AggregateID != nil {
		var aggregateKey string
		if k.messageFormatter != nil {
			aggregateKey = k.messageFormatter.ExtractAggregateID(opts.AggregateID)
		} else {
			aggregateKey = fmt.Sprintf("%v", opts.AggregateID)
		}
		if aggregateKey != "" {
			kafkaMsg.Key = sarama.StringEncoder(aggregateKey)
		}
	}

	// âœ… æ— é”è¯»å– asyncProducer
	producer, err := k.getAsyncProducer()
	if err != nil {
		k.errorCount.Add(1)
		return fmt.Errorf("failed to get async producer: %w", err)
	}

	// ä¼˜åŒ–1ï¼šä½¿ç”¨AsyncProducerå¼‚æ­¥å‘å¸ƒ
	select {
	case producer.Input() <- kafkaMsg:
		// æ¶ˆæ¯å·²æäº¤åˆ°å‘é€é˜Ÿåˆ—
		duration := time.Since(start)
		k.logger.Debug("Message with options queued for async publishing",
			zap.String("topic", topic),
			zap.Duration("duration", duration))
		return nil
	case <-time.After(100 * time.Millisecond):
		// å‘é€é˜Ÿåˆ—æ»¡ï¼Œåº”ç”¨èƒŒå‹
		k.logger.Warn("Async producer input queue full for message with options",
			zap.String("topic", topic))
		// é˜»å¡ç­‰å¾…
		producer.Input() <- kafkaMsg
		return nil
	}
}

// SetMessageFormatter è®¾ç½®æ¶ˆæ¯æ ¼å¼åŒ–å™¨
func (k *kafkaEventBus) SetMessageFormatter(formatter MessageFormatter) error {
	k.mu.Lock()
	defer k.mu.Unlock()

	k.messageFormatter = formatter
	k.logger.Info("Message formatter set for kafka eventbus")
	return nil
}

// RegisterPublishCallback æ³¨å†Œå‘å¸ƒå›è°ƒ
func (k *kafkaEventBus) RegisterPublishCallback(callback PublishCallback) error {
	k.mu.Lock()
	defer k.mu.Unlock()

	k.publishCallback = callback
	k.logger.Debug("Publish callback registered for kafka eventbus")
	return nil
}

// ========== é«˜çº§è®¢é˜…åŠŸèƒ½ ==========

// SubscribeWithOptions ä½¿ç”¨é€‰é¡¹è®¢é˜…æ¶ˆæ¯
func (k *kafkaEventBus) SubscribeWithOptions(ctx context.Context, topic string, handler MessageHandler, opts SubscribeOptions) error {
	// åŒ…è£…å¤„ç†å™¨ä»¥æ”¯æŒä¼ä¸šç‰¹æ€§
	wrappedHandler := func(ctx context.Context, message []byte) error {
		start := time.Now()

		// åº”ç”¨æµé‡æ§åˆ¶
		if k.rateLimiter != nil {
			if !k.rateLimiter.Allow() {
				k.logger.Warn("Message dropped due to rate limiting", zap.String("topic", topic))
				return fmt.Errorf("rate limit exceeded")
			}
		}

		// ç›´æ¥å¤„ç†æ¶ˆæ¯
		err := handler(ctx, message)

		// è®°å½•æŒ‡æ ‡
		duration := time.Since(start)
		if err != nil {
			k.errorCount.Add(1)
			k.logger.Error("Message processing failed",
				zap.String("topic", topic),
				zap.Error(err),
				zap.Duration("duration", duration))

			// è°ƒç”¨é”™è¯¯å¤„ç†å™¨
			if k.errorHandler != nil {
				go func() {
					errorAction := k.errorHandler.HandleError(ctx, err, message, topic)
					k.logger.Info("Error handler executed",
						zap.String("action", string(errorAction.Action)),
						zap.Bool("deadLetter", errorAction.DeadLetter),
						zap.Bool("skipMessage", errorAction.SkipMessage))
				}()
			}
		} else {
			k.consumedMessages.Add(1)
			k.logger.Debug("Message processed successfully",
				zap.String("topic", topic),
				zap.Duration("duration", duration))
		}

		return err
	}

	// ä½¿ç”¨åŒ…è£…åçš„å¤„ç†å™¨è®¢é˜…
	return k.Subscribe(ctx, topic, wrappedHandler)
}

// RegisterSubscriberBacklogCallback æ³¨å†Œè®¢é˜…ç«¯ç§¯å‹å›è°ƒ
func (k *kafkaEventBus) RegisterSubscriberBacklogCallback(callback BacklogStateCallback) error {
	if k.backlogDetector != nil {
		return k.backlogDetector.RegisterCallback(callback)
	}
	k.logger.Info("Subscriber backlog callback registered (detector not available)")
	return nil
}

// StartSubscriberBacklogMonitoring å¯åŠ¨è®¢é˜…ç«¯ç§¯å‹ç›‘æ§
func (k *kafkaEventBus) StartSubscriberBacklogMonitoring(ctx context.Context) error {
	if k.backlogDetector != nil {
		return k.backlogDetector.Start(ctx)
	}
	k.logger.Info("Subscriber backlog monitoring not available")
	return nil
}

// StopSubscriberBacklogMonitoring åœæ­¢è®¢é˜…ç«¯ç§¯å‹ç›‘æ§
func (k *kafkaEventBus) StopSubscriberBacklogMonitoring() error {
	if k.backlogDetector != nil {
		return k.backlogDetector.Stop()
	}
	k.logger.Info("Subscriber backlog monitoring not available")
	return nil
}

// RegisterPublisherBacklogCallback æ³¨å†Œå‘é€ç«¯ç§¯å‹å›è°ƒ
func (k *kafkaEventBus) RegisterPublisherBacklogCallback(callback PublisherBacklogCallback) error {
	if k.publisherBacklogDetector != nil {
		return k.publisherBacklogDetector.RegisterCallback(callback)
	}
	k.logger.Debug("Publisher backlog callback registered (detector not available)")
	return nil
}

// StartPublisherBacklogMonitoring å¯åŠ¨å‘é€ç«¯ç§¯å‹ç›‘æ§
func (k *kafkaEventBus) StartPublisherBacklogMonitoring(ctx context.Context) error {
	if k.publisherBacklogDetector != nil {
		return k.publisherBacklogDetector.Start(ctx)
	}
	k.logger.Debug("Publisher backlog monitoring not available (not configured)")
	return nil
}

// StopPublisherBacklogMonitoring åœæ­¢å‘é€ç«¯ç§¯å‹ç›‘æ§
func (k *kafkaEventBus) StopPublisherBacklogMonitoring() error {
	if k.publisherBacklogDetector != nil {
		return k.publisherBacklogDetector.Stop()
	}
	k.logger.Debug("Publisher backlog monitoring not available (not configured)")
	return nil
}

// StartAllBacklogMonitoring æ ¹æ®é…ç½®å¯åŠ¨æ‰€æœ‰ç§¯å‹ç›‘æ§
func (k *kafkaEventBus) StartAllBacklogMonitoring(ctx context.Context) error {
	var errs []error

	// å¯åŠ¨è®¢é˜…ç«¯ç§¯å‹ç›‘æ§
	if err := k.StartSubscriberBacklogMonitoring(ctx); err != nil {
		errs = append(errs, fmt.Errorf("failed to start subscriber backlog monitoring: %w", err))
	}

	// å¯åŠ¨å‘é€ç«¯ç§¯å‹ç›‘æ§
	if err := k.StartPublisherBacklogMonitoring(ctx); err != nil {
		errs = append(errs, fmt.Errorf("failed to start publisher backlog monitoring: %w", err))
	}

	if len(errs) > 0 {
		return fmt.Errorf("failed to start some backlog monitoring: %v", errs)
	}

	k.logger.Info("All backlog monitoring started successfully")
	return nil
}

// StopAllBacklogMonitoring åœæ­¢æ‰€æœ‰ç§¯å‹ç›‘æ§
func (k *kafkaEventBus) StopAllBacklogMonitoring() error {
	var errs []error

	// åœæ­¢è®¢é˜…ç«¯ç§¯å‹ç›‘æ§
	if err := k.StopSubscriberBacklogMonitoring(); err != nil {
		errs = append(errs, fmt.Errorf("failed to stop subscriber backlog monitoring: %w", err))
	}

	// åœæ­¢å‘é€ç«¯ç§¯å‹ç›‘æ§
	if err := k.StopPublisherBacklogMonitoring(); err != nil {
		errs = append(errs, fmt.Errorf("failed to stop publisher backlog monitoring: %w", err))
	}

	if len(errs) > 0 {
		return fmt.Errorf("failed to stop some backlog monitoring: %v", errs)
	}

	k.logger.Info("All backlog monitoring stopped successfully")
	return nil
}

// SetMessageRouter è®¾ç½®æ¶ˆæ¯è·¯ç”±å™¨
func (k *kafkaEventBus) SetMessageRouter(router MessageRouter) error {
	k.mu.Lock()
	defer k.mu.Unlock()

	k.messageRouter = router
	k.logger.Info("Message router set for kafka eventbus")
	return nil
}

// SetErrorHandler è®¾ç½®é”™è¯¯å¤„ç†å™¨
func (k *kafkaEventBus) SetErrorHandler(handler ErrorHandler) error {
	k.mu.Lock()
	defer k.mu.Unlock()

	k.errorHandler = handler
	k.logger.Info("Error handler set for kafka eventbus")
	return nil
}

// RegisterSubscriptionCallback æ³¨å†Œè®¢é˜…å›è°ƒ
func (k *kafkaEventBus) RegisterSubscriptionCallback(callback SubscriptionCallback) error {
	k.logger.Info("Subscription callback registered for kafka eventbus")
	return nil
}

// ========== ç»Ÿä¸€å¥åº·æ£€æŸ¥å’Œç›‘æ§ ==========

// StartHealthCheck å¯åŠ¨å¥åº·æ£€æŸ¥
func (k *kafkaEventBus) StartHealthCheck(ctx context.Context) error {
	k.mu.Lock()
	defer k.mu.Unlock()

	// å¦‚æœå·²ç»å¯åŠ¨ï¼Œå…ˆåœæ­¢ä¹‹å‰çš„
	if k.healthCheckCancel != nil {
		k.healthCheckCancel()
		if k.healthCheckDone != nil {
			<-k.healthCheckDone // ç­‰å¾…ä¹‹å‰çš„å¥åº·æ£€æŸ¥å®Œå…¨åœæ­¢
		}
	}

	// åˆ›å»ºæ–°çš„æ§åˆ¶ context
	healthCtx, cancel := context.WithCancel(ctx)
	k.healthCheckCancel = cancel
	k.healthCheckDone = make(chan struct{})

	// å¯åŠ¨å¥åº·æ£€æŸ¥åç¨‹
	go func() {
		defer close(k.healthCheckDone)

		ticker := time.NewTicker(2 * time.Minute) // é»˜è®¤å¥åº·æ£€æŸ¥é—´éš”
		defer ticker.Stop()

		for {
			select {
			case <-healthCtx.Done():
				k.logger.Info("Health check stopped for kafka eventbus")
				return
			case <-ticker.C:
				if err := k.healthCheck(healthCtx); err != nil {
					k.logger.Error("Health check failed", zap.Error(err))

					// å¢åŠ å¤±è´¥è®¡æ•°
					failures := k.failureCount.Add(1)
					k.logger.Warn("Health check failure count increased",
						zap.Int32("failures", failures),
						zap.Int("threshold", k.reconnectConfig.FailureThreshold))

					// æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘é‡è¿
					if int(failures) >= k.reconnectConfig.FailureThreshold {
						k.logger.Warn("Health check failure threshold reached, triggering reconnection",
							zap.Int32("failures", failures),
							zap.Int("threshold", k.reconnectConfig.FailureThreshold))

						// æ‰§è¡Œé‡è¿
						if reconnectErr := k.reconnect(healthCtx); reconnectErr != nil {
							k.logger.Error("Automatic reconnection failed", zap.Error(reconnectErr))
						} else {
							k.logger.Info("Automatic reconnection successful")
						}
					}
				} else {
					// å¥åº·æ£€æŸ¥æˆåŠŸï¼Œé‡ç½®å¤±è´¥è®¡æ•°
					if k.failureCount.Load() > 0 {
						k.failureCount.Store(0)
						k.logger.Debug("Health check passed, failure count reset")
					}
				}
			}
		}
	}()

	k.logger.Info("Health check started for kafka eventbus")
	return nil
}

// StopHealthCheck åœæ­¢å¥åº·æ£€æŸ¥
func (k *kafkaEventBus) StopHealthCheck() error {
	k.mu.Lock()
	defer k.mu.Unlock()

	if k.healthCheckCancel != nil {
		// å–æ¶ˆå¥åº·æ£€æŸ¥ context
		k.healthCheckCancel()

		// ç­‰å¾…å¥åº·æ£€æŸ¥ goroutine å®Œå…¨åœæ­¢
		if k.healthCheckDone != nil {
			<-k.healthCheckDone
		}

		// æ¸…ç†èµ„æº
		k.healthCheckCancel = nil
		k.healthCheckDone = nil

		k.logger.Info("Health check stopped for kafka eventbus")
	} else {
		k.logger.Debug("Health check was not running")
	}

	return nil
}

// GetHealthStatus è·å–å¥åº·çŠ¶æ€
func (k *kafkaEventBus) GetHealthStatus() HealthCheckStatus {
	// âœ… æ— é”æ£€æŸ¥ client æ˜¯å¦å¯ç”¨
	_, clientErr := k.getClient()
	// ğŸ”¥ P0ä¿®å¤ï¼šä½¿ç”¨ atomic.Bool è¯»å–å…³é—­çŠ¶æ€
	isHealthy := !k.closed.Load() && clientErr == nil

	return HealthCheckStatus{
		IsHealthy:           isHealthy,
		ConsecutiveFailures: 0,
		LastSuccessTime:     time.Now(),
		LastFailureTime:     time.Time{},
		IsRunning:           !k.closed.Load(),
		EventBusType:        "kafka",
		Source:              "kafka-eventbus",
	}
}

// RegisterHealthCheckCallback æ³¨å†Œå¥åº·æ£€æŸ¥å›è°ƒ
func (k *kafkaEventBus) RegisterHealthCheckCallback(callback HealthCheckCallback) error {
	k.logger.Info("Health check callback registered for kafka eventbus")
	return nil
}

// StartHealthCheckSubscriber å¯åŠ¨å¥åº·æ£€æŸ¥æ¶ˆæ¯è®¢é˜…ç›‘æ§
func (k *kafkaEventBus) StartHealthCheckSubscriber(ctx context.Context) error {
	k.mu.Lock()

	if k.healthCheckSubscriber != nil {
		k.mu.Unlock()
		return nil // å·²ç»å¯åŠ¨
	}

	// ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä¿å­˜çš„å¥åº·æ£€æŸ¥é…ç½®ï¼ˆå¦‚æœæœªé…ç½®ï¼Œåˆ™ä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
	// ä¸ StartHealthCheckPublisher ä¿æŒä¸€è‡´
	config := k.healthCheckConfig
	if !config.Enabled {
		config = GetDefaultHealthCheckConfig()
	}
	k.healthCheckSubscriber = NewHealthCheckSubscriber(config, k, "kafka-eventbus", "kafka")

	// ğŸ”§ ä¿®å¤æ­»é”ï¼šåœ¨è°ƒç”¨ Start ä¹‹å‰é‡Šæ”¾é”
	// Start æ–¹æ³•å†…éƒ¨ä¼šè°ƒç”¨ Subscribeï¼Œè€Œ Subscribe ä¹Ÿéœ€è¦è·å– k.mu é”
	// å¦‚æœä¸é‡Šæ”¾é”ï¼Œä¼šå¯¼è‡´æ­»é”
	subscriber := k.healthCheckSubscriber
	k.mu.Unlock()

	// å¯åŠ¨ç›‘æ§å™¨ï¼ˆä¸æŒæœ‰é”ï¼‰
	if err := subscriber.Start(ctx); err != nil {
		// å¯åŠ¨å¤±è´¥ï¼Œéœ€è¦æ¸…ç†
		k.mu.Lock()
		k.healthCheckSubscriber = nil
		k.mu.Unlock()
		return fmt.Errorf("failed to start health check subscriber: %w", err)
	}

	k.logger.Info("Health check subscriber started for kafka eventbus")
	return nil
}

// StopHealthCheckSubscriber åœæ­¢å¥åº·æ£€æŸ¥æ¶ˆæ¯è®¢é˜…ç›‘æ§
func (k *kafkaEventBus) StopHealthCheckSubscriber() error {
	k.mu.Lock()
	defer k.mu.Unlock()

	if k.healthCheckSubscriber == nil {
		return nil
	}

	if err := k.healthCheckSubscriber.Stop(); err != nil {
		k.logger.Error("Failed to stop health check subscriber", zap.Error(err))
		return err
	}

	k.healthCheckSubscriber = nil
	k.logger.Info("Health check subscriber stopped for kafka eventbus")
	return nil
}

// RegisterHealthCheckAlertCallback æ³¨å†Œå¥åº·æ£€æŸ¥å‘Šè­¦å›è°ƒ
func (k *kafkaEventBus) RegisterHealthCheckAlertCallback(callback HealthCheckAlertCallback) error {
	k.mu.Lock()
	defer k.mu.Unlock()

	if k.healthCheckSubscriber == nil {
		return fmt.Errorf("health check subscriber not started")
	}

	return k.healthCheckSubscriber.RegisterAlertCallback(callback)
}

// GetHealthCheckSubscriberStats è·å–å¥åº·æ£€æŸ¥è®¢é˜…ç›‘æ§ç»Ÿè®¡ä¿¡æ¯
func (k *kafkaEventBus) GetHealthCheckSubscriberStats() HealthCheckSubscriberStats {
	k.mu.Lock()
	defer k.mu.Unlock()

	if k.healthCheckSubscriber == nil {
		return HealthCheckSubscriberStats{}
	}

	return k.healthCheckSubscriber.GetStats()
}

// GetConnectionState è·å–è¿æ¥çŠ¶æ€
func (k *kafkaEventBus) GetConnectionState() ConnectionState {
	// âœ… æ— é”æ£€æŸ¥ client æ˜¯å¦å¯ç”¨
	_, clientErr := k.getClient()
	// ğŸ”¥ P0ä¿®å¤ï¼šä½¿ç”¨ atomic.Bool è¯»å–å…³é—­çŠ¶æ€
	isConnected := !k.closed.Load() && clientErr == nil

	return ConnectionState{
		IsConnected:       isConnected,
		LastConnectedTime: time.Now(),
		ReconnectCount:    0,
		LastError:         "",
	}
}

// GetMetrics è·å–ç›‘æ§æŒ‡æ ‡
func (k *kafkaEventBus) GetMetrics() Metrics {
	return Metrics{
		MessagesPublished: k.publishedMessages.Load(),
		MessagesConsumed:  k.consumedMessages.Load(),
		PublishErrors:     k.errorCount.Load(),
		ConsumeErrors:     0,
		ConnectionErrors:  0,
		LastHealthCheck:   time.Now(),
		HealthCheckStatus: "healthy",
		ActiveConnections: 1,
		MessageBacklog:    0,
	}
}

// ========== æ–¹æ¡ˆAï¼šEnvelope æ”¯æŒ ==========

// PublishEnvelope å‘å¸ƒEnvelopeæ¶ˆæ¯ï¼ˆæ–¹æ¡ˆAï¼‰
func (k *kafkaEventBus) PublishEnvelope(ctx context.Context, topic string, envelope *Envelope) error {
	// ğŸ”¥ P0ä¿®å¤ï¼šé«˜é¢‘è·¯å¾„ä¼˜åŒ– - æ— é”æ£€æŸ¥å…³é—­çŠ¶æ€
	if k.closed.Load() {
		return fmt.Errorf("kafka eventbus is closed")
	}

	// æ ¡éªŒEnvelope
	if err := envelope.Validate(); err != nil {
		return fmt.Errorf("invalid envelope: %w", err)
	}

	// å‘å¸ƒç«¯æµé‡æ§åˆ¶
	if k.publishRateLimiter != nil {
		if err := k.publishRateLimiter.Wait(ctx); err != nil {
			k.errorCount.Add(1)
			return fmt.Errorf("publisher rate limit error: %w", err)
		}
	}

	// è®¢é˜…ç«¯æµé‡æ§åˆ¶ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰
	if k.rateLimiter != nil {
		if err := k.rateLimiter.Wait(ctx); err != nil {
			k.errorCount.Add(1)
			return fmt.Errorf("subscriber rate limit error: %w", err)
		}
	}

	// åºåˆ—åŒ–Envelope
	envelopeBytes, err := envelope.ToBytes()
	if err != nil {
		k.errorCount.Add(1)
		return fmt.Errorf("failed to serialize envelope: %w", err)
	}

	// åˆ›å»ºKafkaæ¶ˆæ¯ï¼ˆæ–¹æ¡ˆA + ä¼ è¾“å±‚é•œåƒï¼‰
	msg := &sarama.ProducerMessage{
		Topic: topic,
		Key:   sarama.StringEncoder(envelope.AggregateID), // é•œåƒåˆ°Kafka Key
		Value: sarama.ByteEncoder(envelopeBytes),
		Headers: []sarama.RecordHeader{
			{Key: []byte("X-Event-ID"), Value: []byte(envelope.EventID)}, // â† æ·»åŠ  EventID
			{Key: []byte("X-Aggregate-ID"), Value: []byte(envelope.AggregateID)},
			{Key: []byte("X-Event-Version"), Value: []byte(fmt.Sprintf("%d", envelope.EventVersion))},
			{Key: []byte("X-Event-Type"), Value: []byte(envelope.EventType)},
		},
	}

	// æ·»åŠ å¯é€‰å­—æ®µåˆ°Header
	if envelope.TraceID != "" {
		msg.Headers = append(msg.Headers, sarama.RecordHeader{
			Key: []byte("X-Trace-ID"), Value: []byte(envelope.TraceID),
		})
	}
	if envelope.CorrelationID != "" {
		msg.Headers = append(msg.Headers, sarama.RecordHeader{
			Key: []byte("X-Correlation-ID"), Value: []byte(envelope.CorrelationID),
		})
	}
	if envelope.TenantID != "" {
		msg.Headers = append(msg.Headers, sarama.RecordHeader{
			Key: []byte("X-Tenant-ID"), Value: []byte(envelope.TenantID), // â† ç§Ÿæˆ·IDï¼ˆå¤šç§Ÿæˆ·æ”¯æŒï¼Œç”¨äºOutbox ACKè·¯ç”±ï¼‰
		})
	}

	// âœ… æ— é”è¯»å– asyncProducer
	producer, err := k.getAsyncProducer()
	if err != nil {
		k.errorCount.Add(1)
		return fmt.Errorf("failed to get async producer: %w", err)
	}

	// ä¼˜åŒ–1ï¼šä½¿ç”¨AsyncProducerå¼‚æ­¥å‘é€
	select {
	case producer.Input() <- msg:
		// æ¶ˆæ¯å·²æäº¤åˆ°å‘é€é˜Ÿåˆ—
		k.logger.Debug("Envelope message queued for async publishing",
			zap.String("topic", topic),
			zap.String("eventID", envelope.EventID),
			zap.String("aggregateID", envelope.AggregateID),
			zap.String("eventType", envelope.EventType),
			zap.Int64("eventVersion", envelope.EventVersion))
		return nil
	case <-time.After(100 * time.Millisecond):
		// å‘é€é˜Ÿåˆ—æ»¡ï¼Œåº”ç”¨èƒŒå‹
		k.logger.Warn("Async producer input queue full for envelope message",
			zap.String("topic", topic),
			zap.String("eventID", envelope.EventID),
			zap.String("aggregateID", envelope.AggregateID))
		// é˜»å¡ç­‰å¾…
		producer.Input() <- msg
		return nil
	}
}

// SubscribeEnvelope è®¢é˜…Envelopeæ¶ˆæ¯ï¼ˆä½¿ç”¨ Hollywood Actor Pool + æ™ºèƒ½è·¯ç”±ï¼‰
//
// ç‰¹ç‚¹ï¼š
// - æ¶ˆæ¯æ ¼å¼ï¼šEnvelopeåŒ…è£…æ ¼å¼ï¼ˆåŒ…å«èšåˆIDã€äº‹ä»¶ç±»å‹ã€ç‰ˆæœ¬ç­‰å…ƒæ•°æ®ï¼‰
// - å¤„ç†æ¨¡å¼ï¼šæŒ‰èšåˆIDè·¯ç”±åˆ° Hollywood Actor Poolï¼ŒåŒèšåˆIDä¸¥æ ¼é¡ºåºå¤„ç†
// - æ€§èƒ½ï¼šé¡ºåºä¿è¯ï¼Œæ¯«ç§’çº§å»¶è¿Ÿ
// - èšåˆIDï¼šä»Envelope.AggregateIDå­—æ®µæå–
// - è·¯ç”±ç­–ç•¥ï¼š
//   - æœ‰èšåˆIDï¼šä¸€è‡´æ€§å“ˆå¸Œè·¯ç”±åˆ°å›ºå®š Actorï¼ˆä¿è¯é¡ºåºï¼‰
//   - æ— èšåˆIDï¼šRound-Robin è½®è¯¢è·¯ç”±ï¼ˆä¿è¯å¹¶å‘ï¼‰
//
// æ ¸å¿ƒæœºåˆ¶ï¼š
// 1. æ¶ˆæ¯å¿…é¡»æ˜¯Envelopeæ ¼å¼ï¼ŒåŒ…å«AggregateID
// 2. ExtractAggregateIDæˆåŠŸæå–èšåˆID
// 3. ä½¿ç”¨ä¸€è‡´æ€§å“ˆå¸Œå°†ç›¸åŒèšåˆIDè·¯ç”±åˆ°å›ºå®š Actor
// 4. ç¡®ä¿åŒä¸€èšåˆçš„äº‹ä»¶ä¸¥æ ¼æŒ‰åºå¤„ç†
//
// é€‚ç”¨åœºæ™¯ï¼š
// - é¢†åŸŸäº‹ä»¶å¤„ç†ï¼ˆè®¢å•çŠ¶æ€å˜æ›´ã€ç”¨æˆ·è¡Œä¸ºï¼‰
// - äº‹ä»¶æº¯æºï¼ˆEvent Sourcingï¼‰
// - èšåˆç®¡ç†ï¼ˆDDDèšåˆæ ¹ï¼‰
// - éœ€è¦é¡ºåºä¿è¯çš„ä¸šåŠ¡åœºæ™¯
//
// ç¤ºä¾‹ï¼š
//
//	bus.SubscribeEnvelope(ctx, "orders.events", func(ctx context.Context, env *Envelope) error {
//	    // env.AggregateID = "order-123"
//	    // åŒä¸€è®¢å•çš„æ‰€æœ‰äº‹ä»¶ä¼šè·¯ç”±åˆ°åŒä¸€ä¸ª Actorï¼Œç¡®ä¿é¡ºåºå¤„ç†
//	    return processDomainEvent(env)
//	})
func (k *kafkaEventBus) SubscribeEnvelope(ctx context.Context, topic string, handler EnvelopeHandler) error {
	// åŒ…è£…EnvelopeHandlerä¸ºMessageHandler
	wrappedHandler := func(ctx context.Context, message []byte) error {
		// å°è¯•è§£æä¸ºEnvelope
		envelope, err := FromBytes(message)
		if err != nil {
			k.logger.Error("Failed to parse envelope message",
				zap.String("topic", topic),
				zap.Error(err))
			return fmt.Errorf("failed to parse envelope: %w", err)
		}

		// è°ƒç”¨ä¸šåŠ¡å¤„ç†å™¨
		return handler(ctx, envelope)
	}

	// â­ é‡è¦ï¼šä¸èƒ½ç›´æ¥è°ƒç”¨ Subscribeï¼Œå› ä¸ºéœ€è¦è®¾ç½® isEnvelope = true
	// å¤åˆ¶ Subscribe çš„é€»è¾‘ï¼Œä½†ä½¿ç”¨ isEnvelope = true
	k.mu.Lock()

	// ğŸ”¥ P0ä¿®å¤ï¼šä½¿ç”¨ atomic.Bool è¯»å–å…³é—­çŠ¶æ€
	if k.closed.Load() {
		k.mu.Unlock()
		return fmt.Errorf("kafka eventbus is closed")
	}

	// âœ… ä½¿ç”¨ sync.Map å­˜å‚¨è®¢é˜…ä¿¡æ¯ï¼ˆç”¨äºé‡è¿åæ¢å¤ï¼‰
	// ä½¿ç”¨ LoadOrStore æ£€æŸ¥é‡å¤è®¢é˜…
	if _, loaded := k.subscriptions.LoadOrStore(topic, wrappedHandler); loaded {
		k.mu.Unlock()
		return fmt.Errorf("already subscribed to topic: %s", topic)
	}

	// å…³é”®ä¿®å¤ï¼šæ·»åŠ  topic åˆ°é¢„è®¢é˜…åˆ—è¡¨
	k.addTopicToPreSubscription(topic)

	// æ¿€æ´»topicå¤„ç†å™¨ï¼ˆç«‹å³ç”Ÿæ•ˆï¼‰
	// â­ SubscribeEnvelopeï¼šisEnvelope = trueï¼ˆat-least-once è¯­ä¹‰ï¼‰
	k.activateTopicHandler(topic, wrappedHandler, true)

	// ğŸ”§ ä¿®å¤æ­»é”ï¼šåœ¨é‡Šæ”¾é”ä¹‹å‰è®°å½•æ—¥å¿—ï¼Œç„¶åé‡Šæ”¾é”å†å¯åŠ¨consumer
	k.logger.Info("Subscribed to envelope topic via pre-subscription consumer",
		zap.String("topic", topic),
		zap.String("groupID", k.config.Consumer.GroupID))

	// é‡Šæ”¾é”ï¼Œé¿å…åœ¨å¯åŠ¨consumeræ—¶æŒæœ‰é”å¯¼è‡´æ­»é”
	k.mu.Unlock()

	// å¯åŠ¨é¢„è®¢é˜…æ¶ˆè´¹è€…ï¼ˆå¦‚æœè¿˜æœªå¯åŠ¨ï¼‰
	if err := k.startPreSubscriptionConsumer(ctx); err != nil {
		return fmt.Errorf("failed to start pre-subscription consumer: %w", err)
	}

	return nil
}

// ========== æ–°çš„åˆ†ç¦»å¼å¥åº·æ£€æŸ¥æ¥å£å®ç° ==========

// StartHealthCheckPublisher å¯åŠ¨å¥åº·æ£€æŸ¥å‘å¸ƒå™¨
func (k *kafkaEventBus) StartHealthCheckPublisher(ctx context.Context) error {
	k.mu.Lock()
	defer k.mu.Unlock()

	if k.healthChecker != nil {
		return nil // å·²ç»å¯åŠ¨
	}

	// ä½¿ç”¨ä¿å­˜çš„å¥åº·æ£€æŸ¥é…ç½®ï¼ˆå¦‚æœæœªé…ç½®ï¼Œåˆ™ä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
	config := k.healthCheckConfig
	if !config.Enabled {
		config = GetDefaultHealthCheckConfig()
	}

	k.healthChecker = NewHealthChecker(config, k, "kafka-eventbus", "kafka")

	// å¯åŠ¨å¥åº·æ£€æŸ¥å‘å¸ƒå™¨
	if err := k.healthChecker.Start(ctx); err != nil {
		k.healthChecker = nil
		return fmt.Errorf("failed to start health check publisher: %w", err)
	}

	k.logger.Info("Health check publisher started for kafka eventbus",
		zap.Duration("interval", config.Publisher.Interval),
		zap.String("topic", config.Publisher.Topic))
	return nil
}

// StopHealthCheckPublisher åœæ­¢å¥åº·æ£€æŸ¥å‘å¸ƒå™¨
func (k *kafkaEventBus) StopHealthCheckPublisher() error {
	k.mu.Lock()
	defer k.mu.Unlock()

	if k.healthChecker == nil {
		return nil
	}

	if err := k.healthChecker.Stop(); err != nil {
		return fmt.Errorf("failed to stop health check publisher: %w", err)
	}

	k.healthChecker = nil
	k.logger.Info("Health check publisher stopped for kafka eventbus")
	return nil
}

// GetHealthCheckPublisherStatus è·å–å¥åº·æ£€æŸ¥å‘å¸ƒå™¨çŠ¶æ€
func (k *kafkaEventBus) GetHealthCheckPublisherStatus() HealthCheckStatus {
	k.mu.Lock()
	defer k.mu.Unlock()

	if k.healthChecker == nil {
		return HealthCheckStatus{
			IsHealthy:           false,
			ConsecutiveFailures: 0,
			LastSuccessTime:     time.Time{},
			LastFailureTime:     time.Now(),
			IsRunning:           false,
			EventBusType:        "kafka",
			Source:              "kafka-eventbus",
		}
	}

	return k.healthChecker.GetStatus()
}

// RegisterHealthCheckPublisherCallback æ³¨å†Œå¥åº·æ£€æŸ¥å‘å¸ƒå™¨å›è°ƒ
func (k *kafkaEventBus) RegisterHealthCheckPublisherCallback(callback HealthCheckCallback) error {
	k.mu.Lock()
	defer k.mu.Unlock()

	if k.healthChecker == nil {
		return fmt.Errorf("health check publisher not started")
	}

	return k.healthChecker.RegisterCallback(callback)
}

// RegisterHealthCheckSubscriberCallback æ³¨å†Œå¥åº·æ£€æŸ¥è®¢é˜…å™¨å›è°ƒ
func (k *kafkaEventBus) RegisterHealthCheckSubscriberCallback(callback HealthCheckAlertCallback) error {
	return k.RegisterHealthCheckAlertCallback(callback)
}

// StartAllHealthCheck æ ¹æ®é…ç½®å¯åŠ¨æ‰€æœ‰å¥åº·æ£€æŸ¥
func (k *kafkaEventBus) StartAllHealthCheck(ctx context.Context) error {
	// è¿™é‡Œå¯ä»¥æ ¹æ®é…ç½®å†³å®šå¯åŠ¨å“ªäº›å¥åº·æ£€æŸ¥
	// ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬å¯åŠ¨å‘å¸ƒå™¨å’Œè®¢é˜…å™¨
	if err := k.StartHealthCheckPublisher(ctx); err != nil {
		return fmt.Errorf("failed to start health check publisher: %w", err)
	}

	if err := k.StartHealthCheckSubscriber(ctx); err != nil {
		return fmt.Errorf("failed to start health check subscriber: %w", err)
	}

	return nil
}

// StopAllHealthCheck åœæ­¢æ‰€æœ‰å¥åº·æ£€æŸ¥
func (k *kafkaEventBus) StopAllHealthCheck() error {
	var errs []error

	if err := k.StopHealthCheckPublisher(); err != nil {
		errs = append(errs, fmt.Errorf("failed to stop health check publisher: %w", err))
	}

	if err := k.StopHealthCheckSubscriber(); err != nil {
		errs = append(errs, fmt.Errorf("failed to stop health check subscriber: %w", err))
	}

	if len(errs) > 0 {
		return fmt.Errorf("errors stopping health checks: %v", errs)
	}

	return nil
}

// ========== ä¸»é¢˜æŒä¹…åŒ–ç®¡ç†å®ç° ==========

// ConfigureTopic é…ç½®ä¸»é¢˜çš„æŒä¹…åŒ–ç­–ç•¥å’Œå…¶ä»–é€‰é¡¹ï¼ˆå¹‚ç­‰æ“ä½œï¼‰
func (k *kafkaEventBus) ConfigureTopic(ctx context.Context, topic string, options TopicOptions) error {
	start := time.Now()

	// éªŒè¯ä¸»é¢˜åç§°
	if err := ValidateTopicName(topic); err != nil {
		return err
	}

	// âœ… ä½é¢‘è·¯å¾„ï¼šä¿ç•™é”ï¼Œä¿æŒä»£ç æ¸…æ™°
	k.mu.Lock()
	// ğŸ”¥ P0ä¿®å¤ï¼šä½¿ç”¨ atomic.Bool è¯»å–å…³é—­çŠ¶æ€
	if k.closed.Load() {
		k.mu.Unlock()
		return fmt.Errorf("kafka eventbus is closed")
	}
	k.mu.Unlock()

	// æ£€æŸ¥æ˜¯å¦å·²æœ‰é…ç½®
	_, exists := k.topicConfigs.Load(topic)
	// ç¼“å­˜é…ç½®
	k.topicConfigs.Store(topic, options)

	// æ ¹æ®ç­–ç•¥å†³å®šæ˜¯å¦éœ€è¦åŒæ­¥åˆ°æ¶ˆæ¯ä¸­é—´ä»¶
	shouldCreate, shouldUpdate := shouldCreateOrUpdate(k.topicConfigStrategy, exists)

	var action string
	var err error
	var mismatches []TopicConfigMismatch

	switch {
	case k.topicConfigStrategy == StrategySkip:
		// è·³è¿‡æ¨¡å¼ï¼šä¸æ£€æŸ¥
		action = "skipped"

	case k.topicConfigStrategy == StrategyValidateOnly:
		// éªŒè¯æ¨¡å¼ï¼šåªéªŒè¯ï¼Œä¸ä¿®æ”¹
		action = "validated"
		if exists {
			actualConfig, validateErr := k.getActualTopicConfig(ctx, topic)
			if validateErr == nil {
				mismatches = compareTopicOptions(topic, options, actualConfig)
				if len(mismatches) > 0 {
					err = handleConfigMismatches(mismatches, k.topicConfigOnMismatch)
				}
			}
		}

	case shouldCreate:
		// åˆ›å»ºæ¨¡å¼ï¼šåˆ›å»ºæ–°é…ç½®
		action = "created"
		err = k.ensureKafkaTopicIdempotent(ctx, topic, options, false)

	case shouldUpdate:
		// æ›´æ–°æ¨¡å¼ï¼šæ›´æ–°ç°æœ‰é…ç½®
		action = "updated"
		// å…ˆéªŒè¯é…ç½®å·®å¼‚
		actualConfig, validateErr := k.getActualTopicConfig(ctx, topic)
		if validateErr == nil {
			mismatches = compareTopicOptions(topic, options, actualConfig)
		}
		// æ‰§è¡Œæ›´æ–°
		err = k.ensureKafkaTopicIdempotent(ctx, topic, options, true)

	default:
		// é»˜è®¤ï¼šåˆ›å»ºæˆ–æ›´æ–°
		action = "configured"
		err = k.ensureKafkaTopicIdempotent(ctx, topic, options, exists)
	}

	duration := time.Since(start)

	// è®°å½•ç»“æœ
	if err != nil {
		k.logger.Error("Topic configuration failed",
			zap.String("topic", topic),
			zap.String("action", action),
			zap.String("strategy", string(k.topicConfigStrategy)),
			zap.Error(err),
			zap.Duration("duration", duration))
		return fmt.Errorf("failed to configure topic %s: %w", topic, err)
	}

	k.logger.Info("Topic configured successfully",
		zap.String("topic", topic),
		zap.String("action", action),
		zap.String("strategy", string(k.topicConfigStrategy)),
		zap.String("persistenceMode", string(options.PersistenceMode)),
		zap.Duration("retentionTime", options.RetentionTime),
		zap.Int64("maxSize", options.MaxSize),
		zap.Int("mismatches", len(mismatches)),
		zap.Duration("duration", duration))

	return nil
}

// SetTopicPersistence è®¾ç½®ä¸»é¢˜æ˜¯å¦æŒä¹…åŒ–ï¼ˆç®€åŒ–æ¥å£ï¼‰
func (k *kafkaEventBus) SetTopicPersistence(ctx context.Context, topic string, persistent bool) error {
	mode := TopicEphemeral
	if persistent {
		mode = TopicPersistent
	}

	options := DefaultTopicOptions()
	options.PersistenceMode = mode

	return k.ConfigureTopic(ctx, topic, options)
}

// GetTopicConfig è·å–ä¸»é¢˜çš„å½“å‰é…ç½®
func (k *kafkaEventBus) GetTopicConfig(topic string) (TopicOptions, error) {
	// ğŸ”¥ é«˜é¢‘è·¯å¾„ï¼šæ— é”è¯»å–é…ç½®
	if configAny, exists := k.topicConfigs.Load(topic); exists {
		return configAny.(TopicOptions), nil
	}

	// è¿”å›é»˜è®¤é…ç½®
	return DefaultTopicOptions(), nil
}

// ListConfiguredTopics åˆ—å‡ºæ‰€æœ‰å·²é…ç½®çš„ä¸»é¢˜
func (k *kafkaEventBus) ListConfiguredTopics() []string {
	topics := make([]string, 0)
	k.topicConfigs.Range(func(key, value interface{}) bool {
		topics = append(topics, key.(string))
		return true
	})

	return topics
}

// RemoveTopicConfig ç§»é™¤ä¸»é¢˜é…ç½®ï¼ˆæ¢å¤ä¸ºé»˜è®¤è¡Œä¸ºï¼‰
func (k *kafkaEventBus) RemoveTopicConfig(topic string) error {
	k.topicConfigs.Delete(topic)

	k.logger.Info("Topic configuration removed", zap.String("topic", topic))
	return nil
}

// ensureKafkaTopic ç¡®ä¿Kafkaä¸»é¢˜å­˜åœ¨å¹¶é…ç½®æ­£ç¡®
func (k *kafkaEventBus) ensureKafkaTopic(topic string, options TopicOptions) error {
	// âœ… æ— é”è¯»å– admin
	admin, err := k.getAdmin()
	if err != nil {
		return fmt.Errorf("Kafka admin client not available: %w", err)
	}

	// æ£€æŸ¥ä¸»é¢˜æ˜¯å¦å·²å­˜åœ¨
	metadata, err := admin.DescribeTopics([]string{topic})
	if err != nil {
		// ä¸»é¢˜ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°ä¸»é¢˜
		return k.createKafkaTopic(topic, options)
	}

	// æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°ä¸»é¢˜
	if len(metadata) == 0 {
		return k.createKafkaTopic(topic, options)
	}

	// ä¸»é¢˜å·²å­˜åœ¨ï¼Œå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ é…ç½®æ›´æ–°é€»è¾‘
	k.logger.Info("Kafka topic already exists", zap.String("topic", topic))
	return nil
}

// createKafkaTopic åˆ›å»ºæ–°çš„Kafkaä¸»é¢˜
func (k *kafkaEventBus) createKafkaTopic(topic string, options TopicOptions) error {
	// âœ… æ— é”è¯»å– admin
	admin, err := k.getAdmin()
	if err != nil {
		return fmt.Errorf("Kafka admin client not available: %w", err)
	}

	// ğŸ”¥ æ€§èƒ½ä¼˜åŒ–ï¼šæ”¯æŒå¤šåˆ†åŒºé…ç½®
	numPartitions := int32(1) // é»˜è®¤1ä¸ªåˆ†åŒº
	if options.Partitions > 0 {
		numPartitions = int32(options.Partitions)
	}

	// ğŸ”¥ é«˜å¯ç”¨ä¼˜åŒ–ï¼šæ”¯æŒå‰¯æœ¬å› å­é…ç½®
	replicationFactor := int16(1) // é»˜è®¤1ä¸ªå‰¯æœ¬
	if options.ReplicationFactor > 0 {
		replicationFactor = int16(options.ReplicationFactor)
	} else if options.Replicas > 0 {
		replicationFactor = int16(options.Replicas)
	}

	// æ„å»ºä¸»é¢˜é…ç½®
	topicDetail := &sarama.TopicDetail{
		NumPartitions:     numPartitions,
		ReplicationFactor: replicationFactor,
		ConfigEntries:     make(map[string]*string),
	}

	// æ ¹æ®æŒä¹…åŒ–æ¨¡å¼è®¾ç½®é…ç½®
	if options.IsPersistent(true) { // Kafkaé»˜è®¤æ˜¯æŒä¹…åŒ–çš„
		// æŒä¹…åŒ–é…ç½®
		if options.RetentionTime > 0 {
			retentionMs := fmt.Sprintf("%d", options.RetentionTime.Milliseconds())
			topicDetail.ConfigEntries["retention.ms"] = &retentionMs
		}
		if options.MaxSize > 0 {
			retentionBytes := fmt.Sprintf("%d", options.MaxSize)
			topicDetail.ConfigEntries["retention.bytes"] = &retentionBytes
		}
		// è®¾ç½®ä¸ºæŒä¹…åŒ–å­˜å‚¨
		cleanupPolicy := "delete"
		topicDetail.ConfigEntries["cleanup.policy"] = &cleanupPolicy
	} else {
		// éæŒä¹…åŒ–é…ç½®ï¼ˆçŸ­æœŸä¿ç•™ï¼‰
		shortRetention := "60000" // 1åˆ†é’Ÿ
		topicDetail.ConfigEntries["retention.ms"] = &shortRetention
		cleanupPolicy := "delete"
		topicDetail.ConfigEntries["cleanup.policy"] = &cleanupPolicy
	}

	// ğŸ”¥ æ–°å¢ï¼šåº”ç”¨ topic çº§åˆ«çš„å‹ç¼©é…ç½®
	if options.Compression != "" && options.Compression != "none" {
		compressionType := options.Compression
		topicDetail.ConfigEntries["compression.type"] = &compressionType
		k.logger.Debug("Applying topic-level compression",
			zap.String("topic", topic),
			zap.String("compression", compressionType))
	}

	// åˆ›å»ºä¸»é¢˜
	err = admin.CreateTopic(topic, topicDetail, false)
	if err != nil {
		return fmt.Errorf("failed to create Kafka topic %s: %w", topic, err)
	}

	k.logger.Info("Created Kafka topic",
		zap.String("topic", topic),
		zap.Int("partitions", int(topicDetail.NumPartitions)),
		zap.Int("replicas", int(topicDetail.ReplicationFactor)),
		zap.Bool("persistent", options.IsPersistent(true)),
		zap.String("compression", options.Compression))

	return nil
}

// ensureKafkaTopicIdempotent å¹‚ç­‰åœ°ç¡®ä¿Kafkaä¸»é¢˜å­˜åœ¨ï¼ˆæ”¯æŒåˆ›å»ºå’Œæ›´æ–°ï¼‰
func (k *kafkaEventBus) ensureKafkaTopicIdempotent(ctx context.Context, topic string, options TopicOptions, allowUpdate bool) error {
	// âœ… æ— é”è¯»å– admin
	admin, err := k.getAdmin()
	if err != nil {
		return fmt.Errorf("Kafka admin client not available: %w", err)
	}

	// æ£€æŸ¥ä¸»é¢˜æ˜¯å¦å·²å­˜åœ¨
	metadata, err := admin.DescribeTopics([]string{topic})

	if err != nil || len(metadata) == 0 {
		// ä¸»é¢˜ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°ä¸»é¢˜
		k.logger.Info("Creating new Kafka topic", zap.String("topic", topic))
		return k.createKafkaTopic(topic, options)
	}

	// ä¸»é¢˜å·²å­˜åœ¨
	k.logger.Info("Kafka topic already exists", zap.String("topic", topic))

	// å¦‚æœå…è®¸æ›´æ–°ï¼Œæ›´æ–°ä¸»é¢˜é…ç½®
	if allowUpdate {
		configEntries := make(map[string]*string)

		// æ„å»ºé…ç½®æ›´æ–°
		if options.RetentionTime > 0 {
			retentionMs := fmt.Sprintf("%d", options.RetentionTime.Milliseconds())
			configEntries["retention.ms"] = &retentionMs
		}
		if options.MaxSize > 0 {
			retentionBytes := fmt.Sprintf("%d", options.MaxSize)
			configEntries["retention.bytes"] = &retentionBytes
		}

		// ğŸ”¥ æ–°å¢ï¼šæ›´æ–°å‹ç¼©é…ç½®
		if options.Compression != "" && options.Compression != "none" {
			compressionType := options.Compression
			configEntries["compression.type"] = &compressionType
		}

		if len(configEntries) > 0 {
			k.logger.Info("Updating Kafka topic configuration",
				zap.String("topic", topic),
				zap.String("compression", options.Compression))

			err := admin.AlterConfig(sarama.TopicResource, topic, configEntries, false)
			if err != nil {
				k.logger.Warn("Failed to update topic config, using existing config",
					zap.String("topic", topic),
					zap.Error(err))
				// ä¸è¿”å›é”™è¯¯ï¼Œä½¿ç”¨ç°æœ‰é…ç½®
			}
		}
	}

	return nil
}

// getActualTopicConfig è·å–ä¸»é¢˜åœ¨Kafkaä¸­çš„å®é™…é…ç½®
func (k *kafkaEventBus) getActualTopicConfig(ctx context.Context, topic string) (TopicOptions, error) {
	// âœ… æ— é”è¯»å– admin
	admin, err := k.getAdmin()
	if err != nil {
		return TopicOptions{}, fmt.Errorf("Kafka admin client not available: %w", err)
	}

	// è·å–ä¸»é¢˜é…ç½®
	configs, err := admin.DescribeConfig(sarama.ConfigResource{
		Type: sarama.TopicResource,
		Name: topic,
	})
	if err != nil {
		return TopicOptions{}, fmt.Errorf("failed to describe topic config: %w", err)
	}

	// è§£æé…ç½®
	actualConfig := TopicOptions{
		PersistenceMode: TopicPersistent, // Kafkaé»˜è®¤æŒä¹…åŒ–
	}

	for _, entry := range configs {
		switch entry.Name {
		case "retention.ms":
			if entry.Value != "" {
				if ms, err := time.ParseDuration(entry.Value + "ms"); err == nil {
					actualConfig.RetentionTime = ms
				}
			}
		case "retention.bytes":
			if entry.Value != "" {
				if bytes, err := fmt.Sscanf(entry.Value, "%d", &actualConfig.MaxSize); err == nil {
					_ = bytes
				}
			}
		}
	}

	// âœ… æ— é”è¯»å– admin
	admin, adminErr := k.getAdmin()
	if adminErr == nil {
		// è·å–åˆ†åŒºå’Œå‰¯æœ¬ä¿¡æ¯
		metadata, err := admin.DescribeTopics([]string{topic})
		if err == nil && len(metadata) > 0 {
			topicMeta := metadata[0]
			if len(topicMeta.Partitions) > 0 {
				actualConfig.Replicas = len(topicMeta.Partitions[0].Replicas)
			}
		}
	}

	return actualConfig, nil
}

// GetPublishResultChannel è·å–å¼‚æ­¥å‘å¸ƒç»“æœé€šé“
// ç”¨äºOutbox Processorç›‘å¬å‘å¸ƒç»“æœå¹¶æ›´æ–°OutboxçŠ¶æ€
//
// âœ… Kafka å®ç°è¯´æ˜ï¼š
// - PublishEnvelope() ä¼šåœ¨ Envelope ä¸­ç”Ÿæˆ EventID å¹¶æ·»åŠ åˆ° Kafka Message Header
// - handleAsyncProducerSuccess() ä» Header æå– EventID å¹¶å‘é€æˆåŠŸç»“æœåˆ° publishResultChan
// - handleAsyncProducerErrors() ä» Header æå– EventID å¹¶å‘é€å¤±è´¥ç»“æœåˆ° publishResultChan
// - Outbox Processor å¯é€šè¿‡æ­¤é€šé“è·å– ACK ç»“æœ
//
// âš ï¸ æ³¨æ„ï¼šåªæœ‰é€šè¿‡ PublishEnvelope() å‘å¸ƒçš„æ¶ˆæ¯æ‰ä¼šå‘é€ ACK ç»“æœ
//
//	é€šè¿‡ Publish() å‘å¸ƒçš„æ™®é€šæ¶ˆæ¯ä¸ä¼šå‘é€ ACK ç»“æœ
func (k *kafkaEventBus) GetPublishResultChannel() <-chan *PublishResult {
	return k.publishResultChan
}

// SetTopicConfigStrategy è®¾ç½®ä¸»é¢˜é…ç½®ç­–ç•¥
func (k *kafkaEventBus) SetTopicConfigStrategy(strategy TopicConfigStrategy) {
	// âœ… ä½é¢‘è·¯å¾„ï¼šä¿ç•™é”
	k.mu.Lock()
	defer k.mu.Unlock()
	k.topicConfigStrategy = strategy
	k.logger.Info("Topic config strategy updated", zap.String("strategy", string(strategy)))
}

// GetTopicConfigStrategy è·å–å½“å‰ä¸»é¢˜é…ç½®ç­–ç•¥
func (k *kafkaEventBus) GetTopicConfigStrategy() TopicConfigStrategy {
	// âœ… ä½é¢‘è·¯å¾„ï¼šä¿ç•™é”
	k.mu.Lock()
	defer k.mu.Unlock()
	return k.topicConfigStrategy
}

// ==========================================================================
// å¤šç§Ÿæˆ· ACK æ”¯æŒ
// ==========================================================================

// sendResultToChannel å‘é€ ACK ç»“æœåˆ°ç§Ÿæˆ·ä¸“å±é€šé“æˆ–å…¨å±€é€šé“
func (k *kafkaEventBus) sendResultToChannel(result *PublishResult) {
	// ä¼˜å…ˆå‘é€åˆ°ç§Ÿæˆ·ä¸“å±é€šé“
	if result.TenantID != "" {
		k.tenantChannelsMu.RLock()
		tenantChan, exists := k.tenantPublishResultChans[result.TenantID]
		k.tenantChannelsMu.RUnlock()

		if exists {
			select {
			case tenantChan <- result:
				// æˆåŠŸå‘é€åˆ°ç§Ÿæˆ·é€šé“
				return
			default:
				// ç§Ÿæˆ·é€šé“æ»¡ï¼Œè®°å½•è­¦å‘Š
				k.logger.Warn("Tenant ACK channel full, falling back to global channel",
					zap.String("tenantID", result.TenantID),
					zap.String("eventID", result.EventID),
					zap.String("topic", result.Topic))
			}
		} else {
			// ç§Ÿæˆ·æœªæ³¨å†Œï¼Œè®°å½•è­¦å‘Š
			k.logger.Warn("Tenant not registered, falling back to global channel",
				zap.String("tenantID", result.TenantID),
				zap.String("eventID", result.EventID))
		}
	}

	// é™çº§ï¼šå‘é€åˆ°å…¨å±€é€šé“ï¼ˆå‘åå…¼å®¹ï¼‰
	select {
	case k.publishResultChan <- result:
		// æˆåŠŸå‘é€åˆ°å…¨å±€é€šé“
	default:
		// å…¨å±€é€šé“ä¹Ÿæ»¡ï¼Œè®°å½•é”™è¯¯
		k.logger.Error("Both tenant and global ACK channels full, dropping result",
			zap.String("tenantID", result.TenantID),
			zap.String("eventID", result.EventID),
			zap.String("topic", result.Topic),
			zap.Bool("success", result.Success))
	}
}

// RegisterTenant æ³¨å†Œç§Ÿæˆ·ï¼ˆåˆ›å»ºç§Ÿæˆ·ä¸“å±çš„ ACK Channelï¼‰
func (k *kafkaEventBus) RegisterTenant(tenantID string, bufferSize int) error {
	if tenantID == "" {
		return fmt.Errorf("tenantID cannot be empty")
	}

	if bufferSize <= 0 {
		bufferSize = 100000 // é»˜è®¤ç¼“å†²åŒºå¤§å°
	}

	k.tenantChannelsMu.Lock()
	defer k.tenantChannelsMu.Unlock()

	// å»¶è¿Ÿåˆå§‹åŒ– map
	if k.tenantPublishResultChans == nil {
		k.tenantPublishResultChans = make(map[string]chan *PublishResult)
	}

	// æ£€æŸ¥ç§Ÿæˆ·æ˜¯å¦å·²æ³¨å†Œ
	if _, exists := k.tenantPublishResultChans[tenantID]; exists {
		return fmt.Errorf("tenant %s already registered", tenantID)
	}

	// åˆ›å»ºç§Ÿæˆ·ä¸“å± ACK Channel
	k.tenantPublishResultChans[tenantID] = make(chan *PublishResult, bufferSize)

	k.logger.Info("Tenant ACK channel registered",
		zap.String("tenantID", tenantID),
		zap.Int("bufferSize", bufferSize))

	return nil
}

// UnregisterTenant æ³¨é”€ç§Ÿæˆ·ï¼ˆå…³é—­å¹¶æ¸…ç†ç§Ÿæˆ·çš„ ACK Channelï¼‰
func (k *kafkaEventBus) UnregisterTenant(tenantID string) error {
	if tenantID == "" {
		return fmt.Errorf("tenantID cannot be empty")
	}

	k.tenantChannelsMu.Lock()
	defer k.tenantChannelsMu.Unlock()

	// æ£€æŸ¥ç§Ÿæˆ·æ˜¯å¦å·²æ³¨å†Œ
	ch, exists := k.tenantPublishResultChans[tenantID]
	if !exists {
		return fmt.Errorf("tenant %s not registered", tenantID)
	}

	// å…³é—­å¹¶åˆ é™¤ç§Ÿæˆ· Channel
	close(ch)
	delete(k.tenantPublishResultChans, tenantID)

	k.logger.Info("Tenant ACK channel unregistered",
		zap.String("tenantID", tenantID))

	return nil
}

// GetTenantPublishResultChannel è·å–ç§Ÿæˆ·ä¸“å±çš„å¼‚æ­¥å‘å¸ƒç»“æœé€šé“
func (k *kafkaEventBus) GetTenantPublishResultChannel(tenantID string) <-chan *PublishResult {
	if tenantID == "" {
		// è¿”å›å…¨å±€é€šé“ï¼ˆå‘åå…¼å®¹ï¼‰
		return k.publishResultChan
	}

	k.tenantChannelsMu.RLock()
	defer k.tenantChannelsMu.RUnlock()

	if ch, exists := k.tenantPublishResultChans[tenantID]; exists {
		return ch
	}

	// ç§Ÿæˆ·æœªæ³¨å†Œï¼Œè¿”å› nil
	k.logger.Warn("Tenant not registered, returning nil channel",
		zap.String("tenantID", tenantID))
	return nil
}

// GetRegisteredTenants è·å–æ‰€æœ‰å·²æ³¨å†Œçš„ç§Ÿæˆ·IDåˆ—è¡¨
func (k *kafkaEventBus) GetRegisteredTenants() []string {
	k.tenantChannelsMu.RLock()
	defer k.tenantChannelsMu.RUnlock()

	tenants := make([]string, 0, len(k.tenantPublishResultChans))
	for tenantID := range k.tenantPublishResultChans {
		tenants = append(tenants, tenantID)
	}

	return tenants
}
