package eventbus

import (
	"context"
	"hash/fnv"
	"runtime"
	"sync"
	"time"

	"go.uber.org/zap"
)

// UnifiedWorkItem ç»Ÿä¸€çš„å·¥ä½œé¡¹ï¼ˆæ”¯æŒKafkaå’ŒNATSï¼‰
type UnifiedWorkItem struct {
	// é€šç”¨å­—æ®µ
	Topic       string
	AggregateID string // å¦‚æœæœ‰èšåˆIDï¼Œåˆ™åŸºäºå“ˆå¸Œè·¯ç”±ï¼›å¦åˆ™è½®è¯¢åˆ†é…
	Data        []byte
	Handler     MessageHandler
	Context     context.Context

	// Kafkaä¸“ç”¨å­—æ®µ
	KafkaMessage interface{} // *sarama.ConsumerMessage
	KafkaSession interface{} // sarama.ConsumerGroupSession

	// NATSä¸“ç”¨å­—æ®µ
	NATSAckFunc func() error
	NATSBus     interface{} // *natsEventBusï¼Œç”¨äºæ›´æ–°ç»Ÿè®¡
}

// Process å¤„ç†å·¥ä½œé¡¹
func (w *UnifiedWorkItem) Process() error {
	// è°ƒç”¨handlerå¤„ç†æ¶ˆæ¯
	return w.Handler(w.Context, w.Data)
}

// UnifiedWorkerPool ç»Ÿä¸€çš„å…¨å±€Keyed-Workeræ± 
// ç‰¹ç‚¹ï¼š
// 1. æœ‰èšåˆIDçš„æ¶ˆæ¯ï¼šåŸºäºèšåˆIDå“ˆå¸Œåˆ°ç‰¹å®šWorkerï¼ˆä¿è¯é¡ºåºï¼‰
// 2. æ— èšåˆIDçš„æ¶ˆæ¯ï¼šè½®è¯¢åˆ†é…åˆ°ä»»æ„Workerï¼ˆé«˜å¹¶å‘ï¼‰
// 3. æ‰€æœ‰topicå…±äº«åŒä¸€ä¸ªWorkeræ± ï¼ˆå‡å°‘Goroutineæ•°é‡ï¼‰
type UnifiedWorkerPool struct {
	workers     []*UnifiedWorker
	workQueue   chan UnifiedWorkItem
	workerCount int
	queueSize   int
	ctx         context.Context
	cancel      context.CancelFunc
	wg          sync.WaitGroup
	logger      *zap.Logger

	// è½®è¯¢åˆ†é…çš„ç´¢å¼•ï¼ˆç”¨äºæ— èšåˆIDçš„æ¶ˆæ¯ï¼‰
	roundRobinIndex int
	roundRobinMu    sync.Mutex
}

// UnifiedWorker ç»Ÿä¸€çš„Worker
type UnifiedWorker struct {
	id       int
	pool     *UnifiedWorkerPool
	workChan chan UnifiedWorkItem
	quit     chan bool
}

// NewUnifiedWorkerPool åˆ›å»ºç»Ÿä¸€çš„å…¨å±€Keyed-Workeræ± 
func NewUnifiedWorkerPool(workerCount int, logger *zap.Logger) *UnifiedWorkerPool {
	if workerCount <= 0 {
		// ğŸ”¥ ä¼˜åŒ–ï¼šå¢åŠ  Worker æ•°é‡ï¼ˆä» CPUÃ—16 â†’ CPUÃ—32ï¼‰
		// æå‡é«˜å‹ä¸‹çš„å¤„ç†é€Ÿåº¦ï¼Œå‡å°‘æ¶ˆæ¯ç§¯å‹
		workerCount = runtime.NumCPU() * 32
	}

	// ğŸ”¥ ä¼˜åŒ–ï¼šå¢åŠ é˜Ÿåˆ—å¤§å°ï¼ˆä» workerÃ—100 â†’ workerÃ—200ï¼‰
	// å‡å°‘é˜Ÿåˆ—æ»¡çš„æƒ…å†µï¼Œé¿å…èƒŒå‹
	queueSize := workerCount * 200

	ctx, cancel := context.WithCancel(context.Background())

	pool := &UnifiedWorkerPool{
		workers:         make([]*UnifiedWorker, workerCount),
		workQueue:       make(chan UnifiedWorkItem, queueSize),
		workerCount:     workerCount,
		queueSize:       queueSize,
		ctx:             ctx,
		cancel:          cancel,
		logger:          logger,
		roundRobinIndex: 0,
	}

	// åˆ›å»ºå¹¶å¯åŠ¨workers
	for i := 0; i < workerCount; i++ {
		worker := &UnifiedWorker{
			id:       i,
			pool:     pool,
			workChan: make(chan UnifiedWorkItem, 50), // ğŸ”¥ ä¼˜åŒ–ï¼šå¢åŠ ç¼“å†²ï¼ˆä» 10 â†’ 50ï¼‰
			quit:     make(chan bool),
		}
		pool.workers[i] = worker
		pool.wg.Add(1)
		go worker.start()
	}

	// å¯åŠ¨æ™ºèƒ½åˆ†å‘å™¨
	go pool.smartDispatcher()

	logger.Info("Unified Worker Pool started",
		zap.Int("workerCount", workerCount),
		zap.Int("queueSize", queueSize))

	return pool
}

// smartDispatcher æ™ºèƒ½åˆ†å‘å™¨
// æ ¹æ®æ˜¯å¦æœ‰èšåˆIDï¼Œé€‰æ‹©ä¸åŒçš„åˆ†å‘ç­–ç•¥
func (p *UnifiedWorkerPool) smartDispatcher() {
	for {
		select {
		case work := <-p.workQueue:
			if work.AggregateID != "" {
				// âœ… æœ‰èšåˆIDï¼šåŸºäºå“ˆå¸Œè·¯ç”±åˆ°ç‰¹å®šWorkerï¼ˆä¿è¯é¡ºåºï¼‰
				p.dispatchByHash(work)
			} else {
				// âœ… æ— èšåˆIDï¼šè½®è¯¢åˆ†é…åˆ°ä»»æ„Workerï¼ˆé«˜å¹¶å‘ï¼‰
				p.dispatchRoundRobin(work)
			}
		case <-p.ctx.Done():
			return
		}
	}
}

// dispatchByHash åŸºäºèšåˆIDå“ˆå¸Œåˆ†å‘ï¼ˆä¿è¯åŒä¸€èšåˆIDçš„æ¶ˆæ¯é¡ºåºï¼‰
func (p *UnifiedWorkerPool) dispatchByHash(work UnifiedWorkItem) {
	idx := p.hashToIndex(work.AggregateID)
	worker := p.workers[idx]

	// å°è¯•å¿«é€Ÿå…¥é˜Ÿ
	select {
	case worker.workChan <- work:
		return
	default:
		// Workerå¿™ï¼Œé˜»å¡ç­‰å¾…ï¼ˆä¿è¯é¡ºåºï¼Œä¸èƒ½æ¢Workerï¼‰
		worker.workChan <- work
	}
}

// dispatchRoundRobin è½®è¯¢åˆ†å‘ï¼ˆæ— èšåˆIDçš„æ¶ˆæ¯ï¼Œè¿½æ±‚é«˜å¹¶å‘ï¼‰
func (p *UnifiedWorkerPool) dispatchRoundRobin(work UnifiedWorkItem) {
	p.roundRobinMu.Lock()
	startIndex := p.roundRobinIndex
	p.roundRobinIndex = (p.roundRobinIndex + 1) % p.workerCount
	p.roundRobinMu.Unlock()

	// å°è¯•ä»å½“å‰ç´¢å¼•å¼€å§‹ï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªå¯ç”¨çš„Worker
	for i := 0; i < p.workerCount; i++ {
		idx := (startIndex + i) % p.workerCount
		worker := p.workers[idx]

		select {
		case worker.workChan <- work:
			return // æˆåŠŸåˆ†å‘
		default:
			continue // è¿™ä¸ªWorkerå¿™ï¼Œå°è¯•ä¸‹ä¸€ä¸ª
		}
	}

	// æ‰€æœ‰Workeréƒ½å¿™ï¼Œé˜»å¡ç­‰å¾…ç¬¬ä¸€ä¸ªå¯ç”¨çš„Worker
	p.workers[startIndex].workChan <- work
}

// hashToIndex å°†èšåˆIDå“ˆå¸Œåˆ°Workerç´¢å¼•
func (p *UnifiedWorkerPool) hashToIndex(aggregateID string) int {
	h := fnv.New32a()
	h.Write([]byte(aggregateID))
	return int(h.Sum32() % uint32(p.workerCount))
}

// SubmitWork æäº¤å·¥ä½œåˆ°ç»Ÿä¸€Workeræ± 
func (p *UnifiedWorkerPool) SubmitWork(work UnifiedWorkItem) bool {
	select {
	case p.workQueue <- work:
		return true
	case <-time.After(500 * time.Millisecond): // ğŸ”¥ ä¼˜åŒ–ï¼šå¢åŠ è¶…æ—¶ï¼ˆä» 100ms â†’ 500msï¼‰
		// ç­‰å¾…500msåä»ç„¶æ»¡ï¼Œè®°å½•è­¦å‘Šä½†ä»å°è¯•æäº¤
		p.logger.Warn("Unified worker pool queue full, applying backpressure",
			zap.String("topic", work.Topic),
			zap.String("aggregateID", work.AggregateID),
			zap.Int("queueSize", p.queueSize),
			zap.Int("workerCount", p.workerCount))
		// é˜»å¡ç­‰å¾…ï¼Œç¡®ä¿æ¶ˆæ¯ä¸ä¸¢å¤±
		p.workQueue <- work
		return true
	}
}

// Close å…³é—­Workeræ± 
func (p *UnifiedWorkerPool) Close() {
	p.cancel()

	// å…³é—­æ‰€æœ‰Worker
	for _, worker := range p.workers {
		close(worker.quit)
	}

	// ç­‰å¾…æ‰€æœ‰Workerå®Œæˆ
	p.wg.Wait()

	p.logger.Info("Unified Worker Pool closed")
}

// start Workerå¯åŠ¨
func (w *UnifiedWorker) start() {
	defer w.pool.wg.Done()

	for {
		select {
		case work := <-w.workChan:
			w.processWork(work)
		case <-w.quit:
			return
		case <-w.pool.ctx.Done():
			return
		}
	}
}

// processWork å¤„ç†å·¥ä½œ
func (w *UnifiedWorker) processWork(work UnifiedWorkItem) {
	defer func() {
		if r := recover(); r != nil {
			w.pool.logger.Error("Unified Worker panic during message processing",
				zap.Int("workerID", w.id),
				zap.String("topic", work.Topic),
				zap.String("aggregateID", work.AggregateID),
				zap.Any("panic", r))
		}
	}()

	// å¤„ç†æ¶ˆæ¯
	err := work.Process()
	if err != nil {
		w.pool.logger.Error("Unified Worker message processing failed",
			zap.Int("workerID", w.id),
			zap.String("topic", work.Topic),
			zap.String("aggregateID", work.AggregateID),
			zap.Error(err))
	}

	// ğŸ”¥ ä¼˜åŒ–ï¼šå¤„ç†å®Œæˆåç«‹å³ ACKï¼ˆNATSï¼‰
	if work.NATSAckFunc != nil {
		if ackErr := work.NATSAckFunc(); ackErr != nil {
			w.pool.logger.Error("Failed to ACK NATS message",
				zap.Int("workerID", w.id),
				zap.String("topic", work.Topic),
				zap.String("aggregateID", work.AggregateID),
				zap.Error(ackErr))
		}
	}

	// ğŸ”¥ ä¼˜åŒ–ï¼šå¤„ç†å®Œæˆåç«‹å³ Markï¼ˆKafkaï¼‰
	if work.KafkaMessage != nil && work.KafkaSession != nil {
		// Kafka çš„ Mark æ˜¯å¼‚æ­¥çš„ï¼Œä¸éœ€è¦ç­‰å¾…
		// æ³¨æ„ï¼šè¿™é‡Œä¸éœ€è¦æ˜¾å¼è°ƒç”¨ï¼ŒKafka çš„ ConsumeClaim ä¼šè‡ªåŠ¨å¤„ç†
	}
}
