package eventbus

import (
	"context"
	"encoding/json"
	"fmt"
	"runtime"
	"sync/atomic"
	"testing"
	"time"
)

// ComprehensiveMetrics å…¨é¢æ€§èƒ½æŒ‡æ ‡
type ComprehensiveMetrics struct {
	// åŸºæœ¬ä¿¡æ¯
	System    string
	Pressure  string
	Messages  int
	StartTime time.Time
	EndTime   time.Time
	Duration  time.Duration

	// æ¶ˆæ¯æŒ‡æ ‡
	Sent        int64
	Received    int64
	Errors      int64
	SuccessRate float64

	// æ€§èƒ½æŒ‡æ ‡
	SendRate       float64 // å‘é€é€Ÿç‡ msg/s
	Throughput     float64 // ååé‡ msg/s
	FirstLatency   time.Duration
	AverageLatency time.Duration
	P95Latency     time.Duration
	P99Latency     time.Duration

	// èµ„æºå ç”¨
	InitialGoroutines int
	PeakGoroutines    int
	GoroutineDelta    int
	InitialMemoryMB   float64
	PeakMemoryMB      float64
	MemoryDeltaMB     float64
	CPUUsagePercent   float64

	// ç½‘ç»œæŒ‡æ ‡
	BytesSent      int64
	BytesReceived  int64
	NetworkLatency time.Duration
}

// TestComprehensivePerformanceBenchmark å…¨é¢æ€§èƒ½åŸºå‡†æµ‹è¯•
func TestComprehensivePerformanceBenchmark(t *testing.T) {
	t.Logf("ğŸš€ COMPREHENSIVE PERFORMANCE BENCHMARK")
	t.Logf("ğŸ“Š Testing Kafka vs NATS with detailed metrics and resource monitoring")

	// æµ‹è¯•åœºæ™¯
	scenarios := []struct {
		name     string
		messages int
		timeout  time.Duration
	}{
		{"Light", 100, 30 * time.Second},
		{"Medium", 500, 60 * time.Second},
		{"Heavy", 1000, 90 * time.Second},
		{"Extreme", 2000, 120 * time.Second},
	}

	allResults := make(map[string][]*ComprehensiveMetrics)
	allResults["Kafka"] = make([]*ComprehensiveMetrics, 0)
	allResults["NATS"] = make([]*ComprehensiveMetrics, 0)

	for _, scenario := range scenarios {
		t.Logf("\nğŸ¯ ===== %s Load Test (%d messages) =====", scenario.name, scenario.messages)

		// 1. Kafkaæµ‹è¯•
		t.Logf("ğŸ”´ Kafka (RedPanda) Performance Test...")
		kafkaMetrics := runComprehensiveKafkaTest(t, scenario.name, scenario.messages, scenario.timeout)
		allResults["Kafka"] = append(allResults["Kafka"], kafkaMetrics)

		// æ¸…ç†é—´éš”
		time.Sleep(5 * time.Second)
		runtime.GC() // å¼ºåˆ¶åƒåœ¾å›æ”¶

		// 2. NATSæµ‹è¯•
		t.Logf("ğŸ”µ NATS Basic Performance Test...")
		natsMetrics := runComprehensiveNATSTest(t, scenario.name, scenario.messages, scenario.timeout)
		allResults["NATS"] = append(allResults["NATS"], natsMetrics)

		// å›åˆåˆ†æ
		analyzeRoundResults(t, scenario.name, kafkaMetrics, natsMetrics)

		// æ¸…ç†é—´éš”
		time.Sleep(5 * time.Second)
		runtime.GC()
	}

	// ç”Ÿæˆå…¨é¢æŠ¥å‘Š
	generateComprehensiveReport(t, allResults)
}

// runComprehensiveKafkaTest è¿è¡ŒKafkaå…¨é¢æµ‹è¯•
func runComprehensiveKafkaTest(t *testing.T, pressure string, messageCount int, timeout time.Duration) *ComprehensiveMetrics {
	config := &KafkaConfig{
		Brokers:  []string{"localhost:29094"},
		ClientID: fmt.Sprintf("benchmark-kafka-%s", pressure),
		Producer: ProducerConfig{
			MaxMessageBytes: 1024 * 1024,
			RequiredAcks:    1,
			Timeout:         5 * time.Second,
		},
		Consumer: ConsumerConfig{
			GroupID:            fmt.Sprintf("benchmark-kafka-%s-group", pressure),
			AutoOffsetReset:    "earliest",
			SessionTimeout:     6 * time.Second,
			HeartbeatInterval:  2 * time.Second,
			MaxProcessingTime:  5 * time.Second,
			FetchMaxWait:       100 * time.Millisecond,
			MaxPollRecords:     1000,
			EnableAutoCommit:   true,
			AutoCommitInterval: 500 * time.Millisecond,
		},
	}

	eventBus, err := NewKafkaEventBus(config)
	if err != nil {
		t.Fatalf("Kafka failed: %v", err)
	}
	defer eventBus.Close()

	// è®¾ç½®é¢„è®¢é˜…
	kafkaBus := eventBus.(*kafkaEventBus)
	testTopic := fmt.Sprintf("benchmark.kafka.%s.test", pressure)
	kafkaBus.allPossibleTopics = []string{testTopic}

	return runComprehensiveTest(t, eventBus, "Kafka", pressure, messageCount, timeout, testTopic)
}

// runComprehensiveNATSTest è¿è¡ŒNATSå…¨é¢æµ‹è¯•
func runComprehensiveNATSTest(t *testing.T, pressure string, messageCount int, timeout time.Duration) *ComprehensiveMetrics {
	config := &NATSConfig{
		URLs:                []string{"nats://localhost:4223"},
		ClientID:            fmt.Sprintf("benchmark-nats-%s", pressure),
		MaxReconnects:       3,
		ReconnectWait:       1 * time.Second,
		ConnectionTimeout:   5 * time.Second,
		HealthCheckInterval: 30 * time.Second,
		JetStream: JetStreamConfig{
			Enabled: false, // ä½¿ç”¨åŸºæœ¬NATSé¿å…é…ç½®é—®é¢˜
		},
	}

	eventBus, err := NewNATSEventBus(config)
	if err != nil {
		t.Logf("âŒ NATS failed: %v", err)
		return &ComprehensiveMetrics{
			System:      "NATS",
			Pressure:    pressure,
			Messages:    messageCount,
			SuccessRate: 0,
			Errors:      int64(messageCount),
		}
	}
	defer eventBus.Close()

	testTopic := fmt.Sprintf("benchmark.nats.%s.test", pressure)
	return runComprehensiveTest(t, eventBus, "NATS", pressure, messageCount, timeout, testTopic)
}

// runComprehensiveTest è¿è¡Œå…¨é¢æµ‹è¯•
func runComprehensiveTest(t *testing.T, eventBus EventBus, system, pressure string, messageCount int, timeout time.Duration, topic string) *ComprehensiveMetrics {
	ctx, cancel := context.WithTimeout(context.Background(), timeout)
	defer cancel()

	// åˆå§‹åŒ–æŒ‡æ ‡
	metrics := &ComprehensiveMetrics{
		System:    system,
		Pressure:  pressure,
		Messages:  messageCount,
		StartTime: time.Now(),
	}

	// è®°å½•åˆå§‹èµ„æºçŠ¶æ€
	metrics.InitialGoroutines = runtime.NumGoroutine()
	var m runtime.MemStats
	runtime.ReadMemStats(&m)
	metrics.InitialMemoryMB = float64(m.Alloc) / 1024 / 1024

	// æ¶ˆæ¯è®¡æ•°å™¨å’Œå»¶è¿Ÿè®°å½•
	var receivedCount int64
	var errors int64
	var firstMessageTime time.Time
	latencies := make([]time.Duration, 0, messageCount)
	var totalBytes int64

	// æ¶ˆæ¯å¤„ç†å™¨
	handler := func(ctx context.Context, message []byte) error {
		receiveTime := time.Now()
		count := atomic.AddInt64(&receivedCount, 1)
		atomic.AddInt64(&totalBytes, int64(len(message)))

		if count == 1 {
			firstMessageTime = receiveTime
		}

		// è§£ææ¶ˆæ¯è·å–å‘é€æ—¶é—´
		var msg map[string]interface{}
		if err := json.Unmarshal(message, &msg); err == nil {
			if sendTimeNano, ok := msg["sendTime"].(float64); ok {
				sendTime := time.Unix(0, int64(sendTimeNano))
				latency := receiveTime.Sub(sendTime)
				latencies = append(latencies, latency)
			}
		}

		return nil
	}

	// è®¢é˜…
	err := eventBus.Subscribe(ctx, topic, handler)
	if err != nil {
		t.Logf("âŒ %s subscribe failed: %v", system, err)
		metrics.Errors = int64(messageCount)
		return metrics
	}

	// é¢„çƒ­
	time.Sleep(3 * time.Second)

	t.Logf("âš¡ %s sending %d messages...", system, messageCount)

	// å‘é€æ¶ˆæ¯å¹¶è®°å½•æ€§èƒ½
	sendStart := time.Now()
	var sentBytes int64

	for i := 0; i < messageCount; i++ {
		sendTime := time.Now()
		message := map[string]interface{}{
			"id":       fmt.Sprintf("%s-%s-%d", system, pressure, i),
			"content":  fmt.Sprintf("%s %s benchmark message %d", system, pressure, i),
			"sendTime": sendTime.UnixNano(),
			"index":    i,
			"system":   system,
			"pressure": pressure,
		}

		messageBytes, _ := json.Marshal(message)
		atomic.AddInt64(&sentBytes, int64(len(messageBytes)))

		err := eventBus.Publish(ctx, topic, messageBytes)
		if err != nil {
			atomic.AddInt64(&errors, 1)
		} else {
			atomic.AddInt64(&metrics.Sent, 1)
		}
	}

	sendDuration := time.Since(sendStart)
	metrics.SendRate = float64(messageCount) / sendDuration.Seconds()
	metrics.BytesSent = sentBytes

	t.Logf("ğŸ“¤ %s sent %d messages in %.2fs (%.1f msg/s)",
		system, messageCount, sendDuration.Seconds(), metrics.SendRate)

	// ç­‰å¾…å¤„ç†å®Œæˆ
	waitTime := 15 * time.Second
	t.Logf("â³ %s waiting %.0fs for processing...", system, waitTime.Seconds())
	time.Sleep(waitTime)

	// è®°å½•ç»“æŸæ—¶é—´å’Œæœ€ç»ˆçŠ¶æ€
	metrics.EndTime = time.Now()
	metrics.Duration = metrics.EndTime.Sub(metrics.StartTime)
	metrics.Received = atomic.LoadInt64(&receivedCount)
	metrics.Errors = atomic.LoadInt64(&errors)
	metrics.BytesReceived = atomic.LoadInt64(&totalBytes)

	// è®¡ç®—æˆåŠŸç‡å’Œååé‡
	metrics.SuccessRate = float64(metrics.Received) / float64(messageCount) * 100
	metrics.Throughput = float64(metrics.Received) / metrics.Duration.Seconds()

	// è®¡ç®—å»¶è¿ŸæŒ‡æ ‡
	if !firstMessageTime.IsZero() {
		metrics.FirstLatency = firstMessageTime.Sub(sendStart)
	}

	if len(latencies) > 0 {
		// è®¡ç®—å¹³å‡å»¶è¿Ÿ
		var totalLatency time.Duration
		for _, lat := range latencies {
			totalLatency += lat
		}
		metrics.AverageLatency = totalLatency / time.Duration(len(latencies))

		// è®¡ç®—P95å’ŒP99å»¶è¿Ÿï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
		if len(latencies) >= 20 {
			p95Index := int(float64(len(latencies)) * 0.95)
			p99Index := int(float64(len(latencies)) * 0.99)
			metrics.P95Latency = latencies[p95Index]
			metrics.P99Latency = latencies[p99Index]
		}
	}

	// è®°å½•å³°å€¼èµ„æºä½¿ç”¨
	metrics.PeakGoroutines = runtime.NumGoroutine()
	metrics.GoroutineDelta = metrics.PeakGoroutines - metrics.InitialGoroutines

	runtime.ReadMemStats(&m)
	metrics.PeakMemoryMB = float64(m.Alloc) / 1024 / 1024
	metrics.MemoryDeltaMB = metrics.PeakMemoryMB - metrics.InitialMemoryMB

	// è®¡ç®—ç½‘ç»œå»¶è¿Ÿï¼ˆé¦–æ¶ˆæ¯å»¶è¿Ÿä½œä¸ºç½‘ç»œå»¶è¿Ÿçš„è¿‘ä¼¼ï¼‰
	metrics.NetworkLatency = metrics.FirstLatency

	t.Logf("ğŸ“Š %s Results: %d/%d (%.1f%%), %.1f msg/s throughput, %v first latency",
		system, metrics.Received, messageCount, metrics.SuccessRate,
		metrics.Throughput, metrics.FirstLatency)

	return metrics
}

// analyzeRoundResults åˆ†æå›åˆç»“æœ
func analyzeRoundResults(t *testing.T, pressure string, kafka, nats *ComprehensiveMetrics) {
	t.Logf("\nğŸ“ˆ %s Load Analysis:", pressure)

	// æ€§èƒ½å¯¹æ¯”
	t.Logf("ğŸ”´ Kafka Performance:")
	t.Logf("   ğŸ“Š Success: %.1f%% (%d/%d)", kafka.SuccessRate, kafka.Received, kafka.Messages)
	t.Logf("   ğŸš€ Send Rate: %.1f msg/s", kafka.SendRate)
	t.Logf("   ğŸ“ˆ Throughput: %.1f msg/s", kafka.Throughput)
	t.Logf("   â±ï¸  First Latency: %v", kafka.FirstLatency)
	t.Logf("   ğŸ“Š Avg Latency: %v", kafka.AverageLatency)
	t.Logf("   ğŸ”§ Goroutines: %d (+%d)", kafka.PeakGoroutines, kafka.GoroutineDelta)
	t.Logf("   ğŸ’¾ Memory: %.2f MB (+%.2f MB)", kafka.PeakMemoryMB, kafka.MemoryDeltaMB)
	t.Logf("   ğŸŒ Network: %.2f MB sent, %.2f MB received",
		float64(kafka.BytesSent)/1024/1024, float64(kafka.BytesReceived)/1024/1024)

	t.Logf("ğŸ”µ NATS Performance:")
	t.Logf("   ğŸ“Š Success: %.1f%% (%d/%d)", nats.SuccessRate, nats.Received, nats.Messages)
	t.Logf("   ğŸš€ Send Rate: %.1f msg/s", nats.SendRate)
	t.Logf("   ğŸ“ˆ Throughput: %.1f msg/s", nats.Throughput)
	t.Logf("   â±ï¸  First Latency: %v", nats.FirstLatency)
	t.Logf("   ğŸ“Š Avg Latency: %v", nats.AverageLatency)
	t.Logf("   ğŸ”§ Goroutines: %d (+%d)", nats.PeakGoroutines, nats.GoroutineDelta)
	t.Logf("   ğŸ’¾ Memory: %.2f MB (+%.2f MB)", nats.PeakMemoryMB, nats.MemoryDeltaMB)
	t.Logf("   ğŸŒ Network: %.2f MB sent, %.2f MB received",
		float64(nats.BytesSent)/1024/1024, float64(nats.BytesReceived)/1024/1024)

	// ç›´æ¥å¯¹æ¯”
	t.Logf("âš”ï¸ Direct Comparison:")

	// æˆåŠŸç‡å¯¹æ¯”
	if kafka.SuccessRate > nats.SuccessRate {
		t.Logf("   âœ… Kafka wins in reliability: %.1f%% vs %.1f%%", kafka.SuccessRate, nats.SuccessRate)
	} else if nats.SuccessRate > kafka.SuccessRate {
		t.Logf("   âœ… NATS wins in reliability: %.1f%% vs %.1f%%", nats.SuccessRate, kafka.SuccessRate)
	} else {
		t.Logf("   ğŸ¤ Tie in reliability: %.1f%%", kafka.SuccessRate)
	}

	// ååé‡å¯¹æ¯”
	throughputRatio := nats.Throughput / kafka.Throughput
	if throughputRatio > 1.1 {
		t.Logf("   âœ… NATS wins in throughput: %.1fx faster (%.1f vs %.1f msg/s)",
			throughputRatio, nats.Throughput, kafka.Throughput)
	} else if throughputRatio < 0.9 {
		t.Logf("   âœ… Kafka wins in throughput: %.1fx faster (%.1f vs %.1f msg/s)",
			1/throughputRatio, kafka.Throughput, nats.Throughput)
	} else {
		t.Logf("   ğŸ¤ Similar throughput: NATS %.1f vs Kafka %.1f msg/s",
			nats.Throughput, kafka.Throughput)
	}

	// å»¶è¿Ÿå¯¹æ¯”
	if kafka.FirstLatency > 0 && nats.FirstLatency > 0 {
		latencyRatio := float64(kafka.FirstLatency.Nanoseconds()) / float64(nats.FirstLatency.Nanoseconds())
		if latencyRatio > 1.5 {
			t.Logf("   âœ… NATS wins in latency: %.1fx faster (%v vs %v)",
				latencyRatio, nats.FirstLatency, kafka.FirstLatency)
		} else if latencyRatio < 0.67 {
			t.Logf("   âœ… Kafka wins in latency: %.1fx faster (%v vs %v)",
				1/latencyRatio, kafka.FirstLatency, nats.FirstLatency)
		} else {
			t.Logf("   ğŸ¤ Similar latency: NATS %v vs Kafka %v",
				nats.FirstLatency, kafka.FirstLatency)
		}
	}

	// èµ„æºæ•ˆç‡å¯¹æ¯”
	memoryRatio := kafka.MemoryDeltaMB / nats.MemoryDeltaMB
	if memoryRatio > 1.2 && nats.MemoryDeltaMB > 0 {
		t.Logf("   âœ… NATS wins in memory efficiency: %.1fx less memory (%.2f vs %.2f MB)",
			memoryRatio, nats.MemoryDeltaMB, kafka.MemoryDeltaMB)
	} else if memoryRatio < 0.8 && kafka.MemoryDeltaMB > 0 {
		t.Logf("   âœ… Kafka wins in memory efficiency: %.1fx less memory (%.2f vs %.2f MB)",
			1/memoryRatio, kafka.MemoryDeltaMB, nats.MemoryDeltaMB)
	}

	goroutineRatio := float64(kafka.GoroutineDelta) / float64(nats.GoroutineDelta)
	if goroutineRatio > 1.2 && nats.GoroutineDelta > 0 {
		t.Logf("   âœ… NATS wins in goroutine efficiency: %.1fx fewer goroutines (%d vs %d)",
			goroutineRatio, nats.GoroutineDelta, kafka.GoroutineDelta)
	} else if goroutineRatio < 0.8 && kafka.GoroutineDelta > 0 {
		t.Logf("   âœ… Kafka wins in goroutine efficiency: %.1fx fewer goroutines (%d vs %d)",
			1/goroutineRatio, kafka.GoroutineDelta, nats.GoroutineDelta)
	}
}

// generateComprehensiveReport ç”Ÿæˆå…¨é¢æŠ¥å‘Š
func generateComprehensiveReport(t *testing.T, allResults map[string][]*ComprehensiveMetrics) {
	t.Logf("\nğŸ† ===== COMPREHENSIVE PERFORMANCE BENCHMARK REPORT =====")

	// 1. è¯¦ç»†æ€§èƒ½è¡¨æ ¼
	generatePerformanceTable(t, allResults)

	// 2. èµ„æºå ç”¨åˆ†æ
	generateResourceAnalysis(t, allResults)

	// 3. æ€§èƒ½è¶‹åŠ¿åˆ†æ
	generateTrendAnalysis(t, allResults)

	// 4. ç»¼åˆè¯„åˆ†å’Œæ¨è
	generateFinalRecommendations(t, allResults)
}

// generatePerformanceTable ç”Ÿæˆæ€§èƒ½è¡¨æ ¼
func generatePerformanceTable(t *testing.T, allResults map[string][]*ComprehensiveMetrics) {
	t.Logf("\nğŸ“Š Detailed Performance Metrics Table:")
	t.Logf("%-10s | %-6s | %-8s | %-10s | %-12s | %-10s | %-10s | %-12s",
		"Load", "System", "Messages", "Success", "Send Rate", "Throughput", "Latency", "Avg Latency")
	t.Logf("%-10s-+-%-6s-+-%-8s-+-%-10s-+-%-12s-+-%-10s-+-%-10s-+-%-12s",
		"----------", "------", "--------", "----------", "------------", "----------", "----------", "------------")

	loads := []string{"Light", "Medium", "Heavy", "Extreme"}
	systems := []string{"Kafka", "NATS"}

	for i, load := range loads {
		for _, system := range systems {
			if i < len(allResults[system]) {
				result := allResults[system][i]
				t.Logf("%-10s | %-6s | %8d | %9.1f%% | %10.1f/s | %8.1f/s | %8.1fms | %10.1fms",
					load, system, result.Messages, result.SuccessRate,
					result.SendRate, result.Throughput,
					float64(result.FirstLatency.Nanoseconds())/1000000,
					float64(result.AverageLatency.Nanoseconds())/1000000)
			}
		}
		if i < len(loads)-1 {
			t.Logf("%-10s-+-%-6s-+-%-8s-+-%-10s-+-%-12s-+-%-10s-+-%-10s-+-%-12s",
				"", "", "", "", "", "", "", "")
		}
	}
}

// generateResourceAnalysis ç”Ÿæˆèµ„æºå ç”¨åˆ†æ
func generateResourceAnalysis(t *testing.T, allResults map[string][]*ComprehensiveMetrics) {
	t.Logf("\nğŸ’¾ Resource Usage Analysis:")
	t.Logf("%-10s | %-6s | %-12s | %-12s | %-15s | %-15s",
		"Load", "System", "Goroutines", "Memory(MB)", "Network Sent", "Network Recv")
	t.Logf("%-10s-+-%-6s-+-%-12s-+-%-12s-+-%-15s-+-%-15s",
		"----------", "------", "------------", "------------", "---------------", "---------------")

	loads := []string{"Light", "Medium", "Heavy", "Extreme"}
	systems := []string{"Kafka", "NATS"}

	for i, load := range loads {
		for _, system := range systems {
			if i < len(allResults[system]) {
				result := allResults[system][i]
				t.Logf("%-10s | %-6s | %8d(+%d) | %8.2f(+%.2f) | %11.2f MB | %11.2f MB",
					load, system, result.PeakGoroutines, result.GoroutineDelta,
					result.PeakMemoryMB, result.MemoryDeltaMB,
					float64(result.BytesSent)/1024/1024,
					float64(result.BytesReceived)/1024/1024)
			}
		}
		if i < len(loads)-1 {
			t.Logf("%-10s-+-%-6s-+-%-12s-+-%-12s-+-%-15s-+-%-15s",
				"", "", "", "", "", "")
		}
	}

	// èµ„æºæ•ˆç‡åˆ†æ
	t.Logf("\nğŸ” Resource Efficiency Analysis:")

	kafkaResults := allResults["Kafka"]
	natsResults := allResults["NATS"]

	if len(kafkaResults) > 0 && len(natsResults) > 0 {
		// è®¡ç®—å¹³å‡èµ„æºä½¿ç”¨
		var kafkaAvgMem, natsAvgMem float64
		var kafkaAvgGoroutines, natsAvgGoroutines float64

		for i := 0; i < len(kafkaResults) && i < len(natsResults); i++ {
			kafkaAvgMem += kafkaResults[i].MemoryDeltaMB
			natsAvgMem += natsResults[i].MemoryDeltaMB
			kafkaAvgGoroutines += float64(kafkaResults[i].GoroutineDelta)
			natsAvgGoroutines += float64(natsResults[i].GoroutineDelta)
		}

		count := float64(len(kafkaResults))
		kafkaAvgMem /= count
		natsAvgMem /= count
		kafkaAvgGoroutines /= count
		natsAvgGoroutines /= count

		t.Logf("ğŸ“Š Average Resource Usage:")
		t.Logf("   ğŸ”´ Kafka: %.2f MB memory, %.1f goroutines", kafkaAvgMem, kafkaAvgGoroutines)
		t.Logf("   ğŸ”µ NATS: %.2f MB memory, %.1f goroutines", natsAvgMem, natsAvgGoroutines)

		// æ•ˆç‡å¯¹æ¯”
		if natsAvgMem > 0 && kafkaAvgMem > 0 {
			memEfficiency := kafkaAvgMem / natsAvgMem
			if memEfficiency > 1.2 {
				t.Logf("   âœ… NATS is %.1fx more memory efficient", memEfficiency)
			} else if memEfficiency < 0.8 {
				t.Logf("   âœ… Kafka is %.1fx more memory efficient", 1/memEfficiency)
			} else {
				t.Logf("   ğŸ¤ Similar memory efficiency")
			}
		}

		if natsAvgGoroutines > 0 && kafkaAvgGoroutines > 0 {
			goroutineEfficiency := kafkaAvgGoroutines / natsAvgGoroutines
			if goroutineEfficiency > 1.2 {
				t.Logf("   âœ… NATS uses %.1fx fewer goroutines", goroutineEfficiency)
			} else if goroutineEfficiency < 0.8 {
				t.Logf("   âœ… Kafka uses %.1fx fewer goroutines", 1/goroutineEfficiency)
			} else {
				t.Logf("   ğŸ¤ Similar goroutine usage")
			}
		}
	}
}

// generateTrendAnalysis ç”Ÿæˆæ€§èƒ½è¶‹åŠ¿åˆ†æ
func generateTrendAnalysis(t *testing.T, allResults map[string][]*ComprehensiveMetrics) {
	t.Logf("\nğŸ“ˆ Performance Trend Analysis:")

	loads := []string{"Light", "Medium", "Heavy", "Extreme"}
	systems := []string{"Kafka", "NATS"}

	for _, system := range systems {
		results := allResults[system]
		if len(results) < 2 {
			continue
		}

		t.Logf("ğŸ”„ %s Performance Scaling:", system)

		// åˆ†æååé‡è¶‹åŠ¿
		t.Logf("   ğŸ“Š Throughput Scaling:")
		for i, load := range loads {
			if i < len(results) {
				result := results[i]
				efficiency := result.Throughput / float64(result.Messages) * 100
				t.Logf("     %s: %.1f msg/s (%.2f%% efficiency)",
					load, result.Throughput, efficiency)
			}
		}

		// åˆ†æå»¶è¿Ÿè¶‹åŠ¿
		t.Logf("   â±ï¸  Latency Scaling:")
		for i, load := range loads {
			if i < len(results) {
				result := results[i]
				t.Logf("     %s: First=%.1fms, Avg=%.1fms",
					load,
					float64(result.FirstLatency.Nanoseconds())/1000000,
					float64(result.AverageLatency.Nanoseconds())/1000000)
			}
		}

		// åˆ†æèµ„æºæ‰©å±•æ€§
		t.Logf("   ğŸ”§ Resource Scaling:")
		for i, load := range loads {
			if i < len(results) {
				result := results[i]
				memPerMsg := result.MemoryDeltaMB / float64(result.Messages) * 1024 // KB per message
				goroutinePerMsg := float64(result.GoroutineDelta) / float64(result.Messages)
				t.Logf("     %s: %.2f KB/msg memory, %.4f goroutines/msg",
					load, memPerMsg, goroutinePerMsg)
			}
		}
	}

	// æ‰©å±•æ€§å¯¹æ¯”
	t.Logf("\nğŸš€ Scalability Comparison:")
	kafkaResults := allResults["Kafka"]
	natsResults := allResults["NATS"]

	if len(kafkaResults) >= 2 && len(natsResults) >= 2 {
		// æ¯”è¾ƒä»è½»è´Ÿè½½åˆ°é‡è´Ÿè½½çš„æ€§èƒ½å˜åŒ–
		kafkaLightThroughput := kafkaResults[0].Throughput
		kafkaHeavyThroughput := kafkaResults[len(kafkaResults)-1].Throughput
		kafkaScaling := kafkaHeavyThroughput / kafkaLightThroughput

		natsLightThroughput := natsResults[0].Throughput
		natsHeavyThroughput := natsResults[len(natsResults)-1].Throughput
		natsScaling := natsHeavyThroughput / natsLightThroughput

		t.Logf("   ğŸ“Š Throughput Scaling Factor:")
		t.Logf("     ğŸ”´ Kafka: %.2fx (%.1f â†’ %.1f msg/s)",
			kafkaScaling, kafkaLightThroughput, kafkaHeavyThroughput)
		t.Logf("     ğŸ”µ NATS: %.2fx (%.1f â†’ %.1f msg/s)",
			natsScaling, natsLightThroughput, natsHeavyThroughput)

		if natsScaling > kafkaScaling*1.1 {
			t.Logf("   âœ… NATS scales better under load")
		} else if kafkaScaling > natsScaling*1.1 {
			t.Logf("   âœ… Kafka scales better under load")
		} else {
			t.Logf("   ğŸ¤ Similar scaling characteristics")
		}
	}
}

// generateFinalRecommendations ç”Ÿæˆæœ€ç»ˆæ¨è
func generateFinalRecommendations(t *testing.T, allResults map[string][]*ComprehensiveMetrics) {
	t.Logf("\nğŸ† Final Performance Evaluation & Recommendations:")

	kafkaResults := allResults["Kafka"]
	natsResults := allResults["NATS"]

	if len(kafkaResults) == 0 || len(natsResults) == 0 {
		t.Logf("âŒ Insufficient data for comprehensive evaluation")
		return
	}

	// è®¡ç®—ç»¼åˆè¯„åˆ†
	kafkaScore := calculateOverallScore(kafkaResults)
	natsScore := calculateOverallScore(natsResults)

	t.Logf("ğŸ“Š Overall Performance Scores:")
	t.Logf("   ğŸ”´ Kafka: %.1f/100", kafkaScore)
	t.Logf("   ğŸ”µ NATS: %.1f/100", natsScore)

	// åˆ†ç±»è¯„åˆ†
	t.Logf("\nğŸ“ˆ Category Breakdown:")

	// å¯é æ€§è¯„åˆ†
	kafkaReliability := calculateReliabilityScore(kafkaResults)
	natsReliability := calculateReliabilityScore(natsResults)
	t.Logf("   ğŸ›¡ï¸  Reliability:")
	t.Logf("     ğŸ”´ Kafka: %.1f/25", kafkaReliability)
	t.Logf("     ğŸ”µ NATS: %.1f/25", natsReliability)

	// æ€§èƒ½è¯„åˆ†
	kafkaPerformance := calculatePerformanceScore(kafkaResults)
	natsPerformance := calculatePerformanceScore(natsResults)
	t.Logf("   ğŸš€ Performance:")
	t.Logf("     ğŸ”´ Kafka: %.1f/35", kafkaPerformance)
	t.Logf("     ğŸ”µ NATS: %.1f/35", natsPerformance)

	// èµ„æºæ•ˆç‡è¯„åˆ†
	kafkaEfficiency := calculateEfficiencyScore(kafkaResults)
	natsEfficiency := calculateEfficiencyScore(natsResults)
	t.Logf("   ğŸ’¾ Resource Efficiency:")
	t.Logf("     ğŸ”´ Kafka: %.1f/25", kafkaEfficiency)
	t.Logf("     ğŸ”µ NATS: %.1f/25", natsEfficiency)

	// æ‰©å±•æ€§è¯„åˆ†
	kafkaScalability := calculateScalabilityScore(kafkaResults)
	natsScalability := calculateScalabilityScore(natsResults)
	t.Logf("   ğŸ“Š Scalability:")
	t.Logf("     ğŸ”´ Kafka: %.1f/15", kafkaScalability)
	t.Logf("     ğŸ”µ NATS: %.1f/15", natsScalability)

	// å®£å¸ƒè·èƒœè€…
	t.Logf("\nğŸ† FINAL VERDICT:")
	if kafkaScore > natsScore {
		t.Logf("   ğŸ¥‡ WINNER: ğŸ”´ KAFKA (RedPanda)")
		t.Logf("   ğŸ“Š Score: %.1f vs %.1f", kafkaScore, natsScore)
		t.Logf("   ğŸ‰ Kafka dominates with superior overall performance!")
	} else if natsScore > kafkaScore {
		t.Logf("   ğŸ¥‡ WINNER: ğŸ”µ NATS")
		t.Logf("   ğŸ“Š Score: %.1f vs %.1f", natsScore, kafkaScore)
		t.Logf("   ğŸ‰ NATS conquers with exceptional performance!")
	} else {
		t.Logf("   ğŸ¤ RESULT: TIE")
		t.Logf("   ğŸ“Š Score: %.1f vs %.1f", kafkaScore, natsScore)
		t.Logf("   ğŸ‰ Both systems are equally matched!")
	}

	// ä½¿ç”¨åœºæ™¯æ¨è
	t.Logf("\nğŸ’¡ Use Case Recommendations:")

	t.Logf("ğŸ”´ Choose Kafka (RedPanda) when:")
	t.Logf("   âœ… You need enterprise-grade reliability (%.1f%% avg success rate)",
		calculateAverageSuccessRate(kafkaResults))
	t.Logf("   âœ… You require mature ecosystem and tooling")
	t.Logf("   âœ… You have complex data processing pipelines")
	t.Logf("   âœ… You need guaranteed message persistence")
	t.Logf("   âœ… You're building data-intensive applications")

	t.Logf("ğŸ”µ Choose NATS when:")
	t.Logf("   âœ… You need maximum performance (%.1f msg/s avg throughput)",
		calculateAverageThroughput(natsResults))
	t.Logf("   âœ… You require ultra-low latency (%.1fms avg latency)",
		calculateAverageLatency(natsResults))
	t.Logf("   âœ… You want simple deployment and operations")
	t.Logf("   âœ… You're building real-time applications")
	t.Logf("   âœ… You need efficient resource utilization")

	// æ€§èƒ½åŸºå‡†å»ºè®®
	t.Logf("\nğŸ“‹ Performance Benchmarks for Your Environment:")
	t.Logf("ğŸ”´ Kafka Expected Performance:")
	t.Logf("   ğŸ“Š Throughput: %.1f-%.1f msg/s",
		getMinThroughput(kafkaResults), getMaxThroughput(kafkaResults))
	t.Logf("   â±ï¸  Latency: %.1f-%.1fms",
		getMinLatency(kafkaResults), getMaxLatency(kafkaResults))
	t.Logf("   ğŸ’¾ Memory: %.2f-%.2f MB per test",
		getMinMemory(kafkaResults), getMaxMemory(kafkaResults))

	t.Logf("ğŸ”µ NATS Expected Performance:")
	t.Logf("   ğŸ“Š Throughput: %.1f-%.1f msg/s",
		getMinThroughput(natsResults), getMaxThroughput(natsResults))
	t.Logf("   â±ï¸  Latency: %.1f-%.1fms",
		getMinLatency(natsResults), getMaxLatency(natsResults))
	t.Logf("   ğŸ’¾ Memory: %.2f-%.2f MB per test",
		getMinMemory(natsResults), getMaxMemory(natsResults))

	t.Logf("\nğŸ¯ COMPREHENSIVE BENCHMARK COMPLETED!")
	t.Logf("ğŸ“Š Total tests run: %d scenarios Ã— 2 systems = %d tests",
		len(kafkaResults), len(kafkaResults)*2)
	t.Logf("â±ï¸  Total test duration: ~%.1f minutes",
		float64(len(kafkaResults)*4*60)/60) // ä¼°ç®—
	t.Logf("ğŸ”¬ Metrics collected: Performance, Reliability, Resource Usage, Scalability")
}
