package eventbus

import (
	"context"
	"fmt"
	"sync"
	"sync/atomic"
	"time"

	"github.com/ChenBigdata421/jxt-core/sdk/pkg/logger"
)

// memoryEventBus å†…å­˜äº‹ä»¶æ€»çº¿å®ç°ï¼ˆç”¨äºæµ‹è¯•å’Œå¼€å‘ï¼‰
type memoryEventBus struct {
	subscribers map[string][]*handlerWrapper // â­ ä¿®æ”¹ï¼šå­˜å‚¨ handlerWrapper è€Œé MessageHandler
	mu          sync.RWMutex
	closed      bool
	metrics     *Metrics

	// â­ æ–°å¢ï¼šHollywood Actor Poolï¼ˆç»Ÿä¸€æ¶æ„ï¼‰
	globalActorPool *HollywoodActorPool

	// â­ æ–°å¢ï¼šRound-Robin è®¡æ•°å™¨ï¼ˆç”¨äºæ— èšåˆIDçš„æ¶ˆæ¯ï¼‰
	roundRobinCounter atomic.Uint64
}

// memoryPublisher å†…å­˜å‘å¸ƒå™¨
type memoryPublisher struct {
	eventBus              *memoryEventBus
	topicConfigStrategy   TopicConfigStrategy
	topicConfigStrategyMu sync.RWMutex
}

// memorySubscriber å†…å­˜è®¢é˜…å™¨
type memorySubscriber struct {
	eventBus              *memoryEventBus
	topicConfigStrategy   TopicConfigStrategy
	topicConfigStrategyMu sync.RWMutex
}

// NewMemoryEventBus åˆ›å»ºå†…å­˜äº‹ä»¶æ€»çº¿
func NewMemoryEventBus() EventBus {
	bus := &memoryEventBus{
		subscribers: make(map[string][]*handlerWrapper),
		metrics: &Metrics{
			LastHealthCheck:   time.Now(),
			HealthCheckStatus: "healthy",
		},
	}

	// â­ åˆå§‹åŒ– Hollywood Actor Pool
	// â­ ä½¿ç”¨å”¯ä¸€çš„ namespace é¿å… Prometheus æŒ‡æ ‡å†²çª
	// â­ æ³¨æ„ï¼šPrometheus æŒ‡æ ‡åç§°åªèƒ½åŒ…å«å­—æ¯ã€æ•°å­—å’Œä¸‹åˆ’çº¿ï¼Œä¸”ä¸èƒ½ä»¥æ•°å­—å¼€å¤´
	namespace := fmt.Sprintf("memory_eventbus_n%d", time.Now().UnixNano())

	poolConfig := HollywoodActorPoolConfig{
		PoolSize:    256,
		InboxSize:   1000,
		MaxRestarts: 3,
	}

	metricsCollector := NewPrometheusActorPoolMetricsCollector(namespace)
	pool := NewHollywoodActorPool(poolConfig, metricsCollector)

	bus.globalActorPool = pool
	logger.Info("Memory EventBus using Hollywood Actor Pool",
		"poolSize", poolConfig.PoolSize,
		"inboxSize", poolConfig.InboxSize,
		"maxRestarts", poolConfig.MaxRestarts,
		"namespace", namespace)

	return &eventBusManager{
		publisher: &memoryPublisher{
			eventBus:            bus,
			topicConfigStrategy: StrategyCreateOrUpdate, // é»˜è®¤ç­–ç•¥
		},
		subscriber: &memorySubscriber{
			eventBus:            bus,
			topicConfigStrategy: StrategyCreateOrUpdate, // é»˜è®¤ç­–ç•¥
		},
		metrics: bus.metrics,
		healthStatus: &HealthStatus{
			Overall:   "healthy",
			Timestamp: time.Now(),
			Infrastructure: InfrastructureHealth{
				EventBus: EventBusHealthMetrics{
					ConnectionStatus: "connected",
				},
			},
			Details: make(map[string]interface{}),
		},
		// ğŸš€ åˆå§‹åŒ–å¼‚æ­¥å‘å¸ƒç»“æœé€šé“ï¼ˆç¼“å†²åŒºå¤§å°ï¼š10000ï¼‰
		publishResultChan: make(chan *PublishResult, 10000),
		// ğŸ”§ åˆå§‹åŒ–ä¸»é¢˜é…ç½®æ˜ å°„
		topicConfigs: make(map[string]TopicOptions),
		// â­ ä¿®å¤ï¼šåˆå§‹åŒ– metricsCollectorï¼ˆNewMemoryEventBus ä¸æ¥å—é…ç½®ï¼Œä½¿ç”¨ NoOpï¼‰
		metricsCollector: &NoOpMetricsCollector{},
	}
}

// Publish å‘å¸ƒæ¶ˆæ¯
func (m *memoryEventBus) Publish(ctx context.Context, topic string, message []byte) error {
	m.mu.RLock()
	if m.closed {
		m.mu.RUnlock()
		return fmt.Errorf("memory eventbus is closed")
	}

	handlers, exists := m.subscribers[topic]
	if !exists || len(handlers) == 0 {
		m.mu.RUnlock()
		logger.Debug("No subscribers for topic", "topic", topic)
		return nil
	}

	// ğŸ”§ ä¿®å¤å¹¶å‘å®‰å…¨é—®é¢˜ï¼šåˆ›å»ºhandlersçš„å‰¯æœ¬ï¼Œé¿å…åœ¨å¼‚æ­¥goroutineä¸­ä½¿ç”¨å¯èƒ½è¢«ä¿®æ”¹çš„åˆ‡ç‰‡
	handlersCopy := make([]*handlerWrapper, len(handlers))
	copy(handlersCopy, handlers)
	m.mu.RUnlock()

	// â­ ä½¿ç”¨ Hollywood Actor Pool å¤„ç†æ¶ˆæ¯
	return m.publishWithActorPool(ctx, topic, message, handlersCopy)
}

// publishWithActorPool ä½¿ç”¨ Hollywood Actor Pool å¤„ç†æ¶ˆæ¯
func (m *memoryEventBus) publishWithActorPool(ctx context.Context, topic string, message []byte, handlers []*handlerWrapper) error {
	// æå– aggregateIDï¼ˆå¦‚æœæ˜¯ Envelopeï¼‰
	// â­ ä½¿ç”¨ä¸ Kafka/NATS ä¸€è‡´çš„æå–é€»è¾‘
	aggregateID, _ := ExtractAggregateID(message, nil, nil, "")

	// â­ ç¡®å®š routingKeyï¼ˆæ‰€æœ‰ handler å…±äº«åŒä¸€ä¸ª routingKeyï¼‰
	// ç­–ç•¥ï¼š
	// - æœ‰èšåˆIDï¼šä½¿ç”¨ aggregateIDï¼ˆä¿è¯åŒä¸€èšåˆçš„æ¶ˆæ¯é¡ºåºï¼‰
	// - æ— èšåˆIDï¼šä½¿ç”¨ Round-Robinï¼ˆæœ€å¤§å¹¶å‘ï¼Œä½†åŒä¸€æ¡æ¶ˆæ¯çš„æ‰€æœ‰ handler è·¯ç”±åˆ°åŒä¸€ Actorï¼‰
	routingKey := aggregateID
	if routingKey == "" {
		// æ— èšåˆIDï¼šä½¿ç”¨ Round-Robinï¼ˆæ¯æ¡æ¶ˆæ¯é€’å¢ä¸€æ¬¡ï¼‰
		index := m.roundRobinCounter.Add(1)
		routingKey = fmt.Sprintf("rr-%d", index)
	}

	// å¯¹æ¯ä¸ª handler æäº¤åˆ° Actor Pool
	for _, wrapper := range handlers {
		// åˆ›å»º AggregateMessage
		aggMsg := &AggregateMessage{
			Topic:       topic,
			Value:       message,
			AggregateID: routingKey, // â­ æ‰€æœ‰ handler ä½¿ç”¨ç›¸åŒçš„ routingKey
			Context:     ctx,
			Done:        make(chan error, 1),
			Handler:     wrapper.handler,
			IsEnvelope:  wrapper.isEnvelope, // â­ è®¾ç½® Envelope æ ‡è®°
		}

		// æäº¤åˆ° Actor Pool
		if err := m.globalActorPool.ProcessMessage(ctx, aggMsg); err != nil {
			logger.Error("Failed to submit message to actor pool", "error", err)
			m.metrics.ConsumeErrors++
			continue
		}

		// â­ ä¿®å¤ï¼šä½¿ç”¨ time.NewTimer é¿å…æ³„æ¼
		// å¼‚æ­¥ç­‰å¾…ç»“æœï¼ˆä¸é˜»å¡å‘å¸ƒè€…ï¼‰
		go func(msg *AggregateMessage, isEnvelope bool) {
			timer := time.NewTimer(30 * time.Second)
			defer timer.Stop()

			select {
			case err := <-msg.Done:
				if err != nil {
					// âš ï¸ Memory EventBus é™åˆ¶ï¼šæ— æ³•å®ç° at-least-once è¯­ä¹‰
					// åŸå› ï¼šç¼ºä¹æŒä¹…åŒ–æœºåˆ¶ï¼Œæ¶ˆæ¯å¤„ç†å¤±è´¥åæ— æ³•é‡æ–°æŠ•é€’
					// é‡è¯•ä¼šå¯¼è‡´æ— é™å¾ªç¯ï¼ˆå¦‚æœ handler æ€»æ˜¯å¤±è´¥ï¼‰
					if isEnvelope {
						logger.Warn("Envelope message processing failed (at-most-once semantics)", "topic", topic, "error", err)
					} else {
						logger.Error("Regular message handler failed", "topic", topic, "error", err)
					}
					m.metrics.ConsumeErrors++
				} else {
					m.metrics.MessagesConsumed++
				}
			case <-timer.C:
				logger.Error("Message processing timeout", "topic", topic)
				m.metrics.ConsumeErrors++
			}
		}(aggMsg, wrapper.isEnvelope)
	}

	m.metrics.MessagesPublished++
	logger.Debug("Message published to memory eventbus via actor pool", "topic", topic, "handlers", len(handlers), "routingKey", routingKey)
	return nil
}

// Subscribe è®¢é˜…æ¶ˆæ¯
func (m *memoryEventBus) Subscribe(ctx context.Context, topic string, handler MessageHandler) error {
	m.mu.Lock()
	defer m.mu.Unlock()

	if m.closed {
		return fmt.Errorf("memory eventbus is closed")
	}

	if handler == nil {
		return fmt.Errorf("handler cannot be nil")
	}

	// â­ æ™®é€š Subscribeï¼šisEnvelope = falseï¼ˆat-most-once è¯­ä¹‰ï¼‰
	wrapper := &handlerWrapper{
		handler:    handler,
		isEnvelope: false,
	}
	m.subscribers[topic] = append(m.subscribers[topic], wrapper)
	logger.Info("Subscribed to topic in memory eventbus", "topic", topic, "totalSubscribers", len(m.subscribers[topic]))
	return nil
}

// SubscribeEnvelope è®¢é˜… Envelope æ¶ˆæ¯
func (m *memoryEventBus) SubscribeEnvelope(ctx context.Context, topic string, handler EnvelopeHandler) error {
	// åŒ…è£…EnvelopeHandlerä¸ºMessageHandler
	wrappedHandler := func(ctx context.Context, message []byte) error {
		// å°è¯•è§£æä¸ºEnvelope
		envelope, err := FromBytes(message)
		if err != nil {
			logger.Error("Failed to parse envelope message", "topic", topic, "error", err)
			return fmt.Errorf("failed to parse envelope: %w", err)
		}

		// è°ƒç”¨ä¸šåŠ¡å¤„ç†å™¨
		return handler(ctx, envelope)
	}

	m.mu.Lock()
	defer m.mu.Unlock()

	if m.closed {
		return fmt.Errorf("memory eventbus is closed")
	}

	if handler == nil {
		return fmt.Errorf("handler cannot be nil")
	}

	// â­ SubscribeEnvelopeï¼šisEnvelope = trueï¼ˆat-least-once è¯­ä¹‰ï¼‰
	wrapper := &handlerWrapper{
		handler:    wrappedHandler,
		isEnvelope: true,
	}
	m.subscribers[topic] = append(m.subscribers[topic], wrapper)
	logger.Info("Subscribed to envelope topic in memory eventbus", "topic", topic, "totalSubscribers", len(m.subscribers[topic]))
	return nil
}

// HealthCheck å¥åº·æ£€æŸ¥
func (m *memoryEventBus) HealthCheck(ctx context.Context) error {
	m.mu.RLock()
	defer m.mu.RUnlock()

	if m.closed {
		return fmt.Errorf("memory eventbus is closed")
	}

	m.metrics.LastHealthCheck = time.Now()
	m.metrics.HealthCheckStatus = "healthy"
	m.metrics.ActiveConnections = 1 // å†…å­˜å®ç°å§‹ç»ˆæœ‰ä¸€ä¸ªè¿æ¥

	logger.Debug("Memory eventbus health check passed")
	return nil
}

// Close å…³é—­
func (m *memoryEventBus) Close() error {
	m.mu.Lock()
	defer m.mu.Unlock()

	if m.closed {
		return nil
	}

	m.closed = true

	// â­ å…³é—­ Hollywood Actor Pool
	if m.globalActorPool != nil {
		m.globalActorPool.Stop()
	}

	m.subscribers = make(map[string][]*handlerWrapper)
	logger.Info("Memory eventbus closed")
	return nil
}

// RegisterReconnectCallback æ³¨å†Œé‡è¿å›è°ƒï¼ˆå†…å­˜å®ç°ä¸éœ€è¦é‡è¿ï¼‰
func (m *memoryEventBus) RegisterReconnectCallback(callback func(ctx context.Context) error) error {
	logger.Debug("Reconnect callback registered for memory eventbus (no-op)")
	return nil
}

// memoryPublisher å®ç°

// Publish å‘å¸ƒæ¶ˆæ¯
func (p *memoryPublisher) Publish(ctx context.Context, topic string, message []byte) error {
	return p.eventBus.Publish(ctx, topic, message)
}

// Close å…³é—­å‘å¸ƒå™¨
func (p *memoryPublisher) Close() error {
	logger.Debug("Memory publisher closed")
	return nil
}

// memorySubscriber å®ç°

// Subscribe è®¢é˜…æ¶ˆæ¯
func (s *memorySubscriber) Subscribe(ctx context.Context, topic string, handler MessageHandler) error {
	return s.eventBus.Subscribe(ctx, topic, handler)
}

// Close å…³é—­è®¢é˜…å™¨
func (s *memorySubscriber) Close() error {
	logger.Debug("Memory subscriber closed")
	return nil
}

// æ›´æ–° eventBusManager çš„ initMemory æ–¹æ³•
func (m *eventBusManager) initMemory() (EventBus, error) {
	bus := &memoryEventBus{
		subscribers: make(map[string][]*handlerWrapper),
		metrics: &Metrics{
			LastHealthCheck:   time.Now(),
			HealthCheckStatus: "healthy",
		},
	}

	// â­ åˆå§‹åŒ– Hollywood Actor Pool
	// â­ ä½¿ç”¨å”¯ä¸€çš„ namespace é¿å… Prometheus æŒ‡æ ‡å†²çª
	// â­ æ³¨æ„ï¼šPrometheus æŒ‡æ ‡åç§°åªèƒ½åŒ…å«å­—æ¯ã€æ•°å­—å’Œä¸‹åˆ’çº¿ï¼Œä¸”ä¸èƒ½ä»¥æ•°å­—å¼€å¤´
	namespace := fmt.Sprintf("memory_eventbus_n%d", time.Now().UnixNano())

	poolConfig := HollywoodActorPoolConfig{
		PoolSize:    256,
		InboxSize:   1000,
		MaxRestarts: 3,
	}

	metricsCollector := NewPrometheusActorPoolMetricsCollector(namespace)
	pool := NewHollywoodActorPool(poolConfig, metricsCollector)

	bus.globalActorPool = pool
	logger.Info("Memory EventBus using Hollywood Actor Pool",
		"poolSize", poolConfig.PoolSize,
		"inboxSize", poolConfig.InboxSize,
		"maxRestarts", poolConfig.MaxRestarts,
		"namespace", namespace)

	m.publisher = &memoryPublisher{
		eventBus:            bus,
		topicConfigStrategy: StrategyCreateOrUpdate, // é»˜è®¤ç­–ç•¥
	}
	m.subscriber = &memorySubscriber{
		eventBus:            bus,
		topicConfigStrategy: StrategyCreateOrUpdate, // é»˜è®¤ç­–ç•¥
	}
	m.metrics = bus.metrics
	m.healthStatus = &HealthStatus{
		Overall:   "healthy",
		Timestamp: time.Now(),
		Infrastructure: InfrastructureHealth{
			EventBus: EventBusHealthMetrics{
				ConnectionStatus: "connected",
			},
		},
		Details: map[string]interface{}{"type": "memory"},
	}

	// â­ ä¿®å¤ï¼šç¡®ä¿ metricsCollector è¢«ä¿ç•™ï¼ˆNewEventBus å·²ç»åˆå§‹åŒ–äº†ï¼‰
	// å¦‚æœ metricsCollector ä¸º nilï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰ï¼Œåˆ™ä½¿ç”¨ NoOp
	// æ³¨æ„ï¼šNewEventBus åœ¨è°ƒç”¨ initMemory ä¹‹å‰å·²ç»è®¾ç½®äº† m.metricsCollector
	// æ‰€ä»¥è¿™é‡Œä¸éœ€è¦é‡æ–°è®¾ç½®ï¼Œé™¤éå®ƒä¸º nil
	if m.metricsCollector == nil {
		m.metricsCollector = &NoOpMetricsCollector{}
		logger.Warn("metricsCollector was nil in initMemory, using NoOp")
	}

	logger.Info("Memory eventbus initialized successfully")
	return m, nil
}

// SetTopicConfigStrategy è®¾ç½®ä¸»é¢˜é…ç½®ç­–ç•¥ï¼ˆmemoryPublisherï¼‰
func (m *memoryPublisher) SetTopicConfigStrategy(strategy TopicConfigStrategy) {
	m.topicConfigStrategyMu.Lock()
	defer m.topicConfigStrategyMu.Unlock()
	m.topicConfigStrategy = strategy
	logger.Debug("Memory publisher topic config strategy updated", "strategy", string(strategy))
}

// GetTopicConfigStrategy è·å–ä¸»é¢˜é…ç½®ç­–ç•¥ï¼ˆmemoryPublisherï¼‰
func (m *memoryPublisher) GetTopicConfigStrategy() TopicConfigStrategy {
	m.topicConfigStrategyMu.RLock()
	defer m.topicConfigStrategyMu.RUnlock()
	return m.topicConfigStrategy
}

// SetTopicConfigStrategy è®¾ç½®ä¸»é¢˜é…ç½®ç­–ç•¥ï¼ˆmemorySubscriberï¼‰
func (m *memorySubscriber) SetTopicConfigStrategy(strategy TopicConfigStrategy) {
	m.topicConfigStrategyMu.Lock()
	defer m.topicConfigStrategyMu.Unlock()
	m.topicConfigStrategy = strategy
	logger.Debug("Memory subscriber topic config strategy updated", "strategy", string(strategy))
}

// GetTopicConfigStrategy è·å–ä¸»é¢˜é…ç½®ç­–ç•¥ï¼ˆmemorySubscriberï¼‰
func (m *memorySubscriber) GetTopicConfigStrategy() TopicConfigStrategy {
	m.topicConfigStrategyMu.RLock()
	defer m.topicConfigStrategyMu.RUnlock()
	return m.topicConfigStrategy
}
