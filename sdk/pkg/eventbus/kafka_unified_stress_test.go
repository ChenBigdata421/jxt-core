package eventbus

import (
	"context"
	"encoding/json"
	"fmt"
	"math/rand"
	"runtime"
	"strings"
	"sync"
	"sync/atomic"
	"testing"
	"time"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
)

// ğŸ”¥ Kafka UnifiedConsumer å‹åŠ›æµ‹è¯•
// éªŒè¯åœ¨é«˜å¹¶å‘ã€å¤§æ•°æ®é‡åœºæ™¯ä¸‹çš„æ€§èƒ½å’Œç¨³å®šæ€§

// StressTestConfig å‹åŠ›æµ‹è¯•é…ç½®
type StressTestConfig struct {
	TopicCount        int           // topicæ•°é‡
	ProducerCount     int           // ç”Ÿäº§è€…æ•°é‡
	ConsumerCount     int           // æ¶ˆè´¹è€…æ•°é‡ï¼ˆworkeræ± å¤§å°ï¼‰
	MessagesPerSecond int           // æ¯ç§’æ¶ˆæ¯æ•°
	TestDuration      time.Duration // æµ‹è¯•æŒç»­æ—¶é—´
	MessageSize       int           // æ¶ˆæ¯å¤§å°(å­—èŠ‚)
	AggregateCount    int           // èšåˆIDæ•°é‡
}

// StressTestMetrics å‹åŠ›æµ‹è¯•æŒ‡æ ‡
type StressTestMetrics struct {
	// å®æ—¶ç»Ÿè®¡
	MessagesSent      int64 // å‘é€æ¶ˆæ¯æ•°
	MessagesReceived  int64 // æ¥æ”¶æ¶ˆæ¯æ•°
	SendErrors        int64 // å‘é€é”™è¯¯æ•°
	ProcessErrors     int64 // å¤„ç†é”™è¯¯æ•°
	OrderViolations   int64 // é¡ºåºè¿åæ•°
	
	// æ€§èƒ½ç»Ÿè®¡
	MaxSendLatency    int64 // æœ€å¤§å‘é€å»¶è¿Ÿ(Î¼s)
	MaxProcessLatency int64 // æœ€å¤§å¤„ç†å»¶è¿Ÿ(Î¼s)
	TotalSendLatency  int64 // æ€»å‘é€å»¶è¿Ÿ
	TotalProcessLatency int64 // æ€»å¤„ç†å»¶è¿Ÿ
	LatencyCount      int64 // å»¶è¿Ÿè®¡æ•°
	
	// ç³»ç»Ÿèµ„æº
	MaxMemoryUsage    uint64 // æœ€å¤§å†…å­˜ä½¿ç”¨(å­—èŠ‚)
	MaxGoroutines     int    // æœ€å¤§åç¨‹æ•°
	
	// æ—¶é—´æˆ³
	StartTime         time.Time
	EndTime           time.Time
	
	// èšåˆå¤„ç†è·Ÿè¸ª
	AggregateSequences sync.Map // map[string]int64 - è·Ÿè¸ªæ¯ä¸ªèšåˆçš„åºåˆ—å·
}

// TestKafkaUnifiedConsumerStressTest ä¸»å‹åŠ›æµ‹è¯•
func TestKafkaUnifiedConsumerStressTest(t *testing.T) {
	if testing.Short() {
		t.Skip("Skipping stress test in short mode")
	}

	config := StressTestConfig{
		TopicCount:        10,
		ProducerCount:     5,
		ConsumerCount:     20,
		MessagesPerSecond: 5000,
		TestDuration:      60 * time.Second,
		MessageSize:       1024, // 1KB
		AggregateCount:    200,
	}

	metrics := &StressTestMetrics{
		StartTime: time.Now(),
	}

	t.Logf("ğŸ”¥ å¼€å§‹Kafka UnifiedConsumerå‹åŠ›æµ‹è¯•")
	t.Logf("ğŸ“Š é…ç½®: %d topics, %d producers, %d consumers, %d msg/s, %d seconds",
		config.TopicCount, config.ProducerCount, config.ConsumerCount, 
		config.MessagesPerSecond, int(config.TestDuration.Seconds()))

	// åˆ›å»ºé«˜æ€§èƒ½Kafkaé…ç½®
	kafkaConfig := createHighPerformanceKafkaConfig()
	eventBus, err := NewKafkaEventBus(kafkaConfig)
	require.NoError(t, err)
	defer eventBus.Close()

	ctx, cancel := context.WithTimeout(context.Background(), config.TestDuration+30*time.Second)
	defer cancel()

	// å¯åŠ¨ç³»ç»Ÿç›‘æ§
	stopMonitoring := startSystemMonitoring(metrics)
	defer stopMonitoring()

	// è®¾ç½®æ¶ˆè´¹è€…
	err = setupStressTestConsumers(ctx, eventBus, config, metrics)
	require.NoError(t, err)

	// ç­‰å¾…æ¶ˆè´¹è€…å‡†å¤‡å°±ç»ª
	time.Sleep(3 * time.Second)

	// å¯åŠ¨ç”Ÿäº§è€…
	err = runStressTestProducers(ctx, eventBus, config, metrics)
	require.NoError(t, err)

	// ç­‰å¾…æµ‹è¯•å®Œæˆ
	time.Sleep(config.TestDuration)
	cancel() // åœæ­¢ç”Ÿäº§è€…

	// ç­‰å¾…å‰©ä½™æ¶ˆæ¯å¤„ç†å®Œæˆ
	waitForRemainingMessages(metrics, 30*time.Second, t)

	// åˆ†æå‹åŠ›æµ‹è¯•ç»“æœ
	analyzeStressTestResults(config, metrics, t)
}

// createHighPerformanceKafkaConfig åˆ›å»ºé«˜æ€§èƒ½Kafkaé…ç½®
func createHighPerformanceKafkaConfig() *KafkaConfig {
	return &KafkaConfig{
		Brokers: []string{"localhost:9092"},
		Producer: ProducerConfig{
			RequiredAcks:     -1, // WaitForAllï¼Œå¹‚ç­‰æ€§ç”Ÿäº§è€…è¦æ±‚
			Compression:      "snappy",
			FlushFrequency:   5 * time.Millisecond,  // æ›´é¢‘ç¹åˆ·æ–°
			FlushMessages:    200,                   // æ›´å¤§æ‰¹æ¬¡
			Timeout:          30 * time.Second,
			FlushBytes:       2 * 1024 * 1024,      // 2MB
			RetryMax:         5,
			BatchSize:        32 * 1024,            // 32KB
			BufferSize:       64 * 1024 * 1024,     // 64MB
			Idempotent:       true,
			MaxMessageBytes:  2 * 1024 * 1024,      // 2MB
			PartitionerType:  "hash",
			LingerMs:         2 * time.Millisecond,  // æ›´çŸ­ç­‰å¾…
			CompressionLevel: 6,
			MaxInFlight:      1,                     // å¹‚ç­‰æ€§ç”Ÿäº§è€…è¦æ±‚MaxInFlight=1
		},
		Consumer: ConsumerConfig{
			GroupID:            "stress-test-group",
			AutoOffsetReset:    "earliest",
			SessionTimeout:     30 * time.Second,
			HeartbeatInterval:  3 * time.Second,
			MaxProcessingTime:  60 * time.Second,
			FetchMinBytes:      1024,
			FetchMaxBytes:      100 * 1024 * 1024,  // 100MB
			FetchMaxWait:       100 * time.Millisecond, // æ›´çŸ­ç­‰å¾…
			MaxPollRecords:     1000,               // æ›´å¤§æ‰¹æ¬¡
			EnableAutoCommit:   false,
			AutoCommitInterval: 1 * time.Second,    // æ›´é¢‘ç¹æäº¤
			IsolationLevel:     "read_committed",
			RebalanceStrategy:  "range",
		},
		HealthCheckInterval:  30 * time.Second,
		ClientID:            "stress-test-client",
		MetadataRefreshFreq: 5 * time.Minute,
		MetadataRetryMax:    5,
		MetadataRetryBackoff: 100 * time.Millisecond,
		Net: NetConfig{
			DialTimeout:   10 * time.Second,
			ReadTimeout:   30 * time.Second,
			WriteTimeout:  30 * time.Second,
			KeepAlive:     30 * time.Second,
			MaxIdleConns:  20,
			MaxOpenConns:  200,
		},
		Security: SecurityConfig{Enabled: false},
	}
}

// setupStressTestConsumers è®¾ç½®å‹åŠ›æµ‹è¯•æ¶ˆè´¹è€…
func setupStressTestConsumers(ctx context.Context, eventBus EventBus, config StressTestConfig, metrics *StressTestMetrics) error {
	topics := make([]string, config.TopicCount)
	for i := 0; i < config.TopicCount; i++ {
		topics[i] = fmt.Sprintf("stress-topic-%d", i)
	}

	var wg sync.WaitGroup
	for _, topic := range topics {
		wg.Add(1)
		go func(topicName string) {
			defer wg.Done()
			
			handler := createStressTestHandler(topicName, metrics)
			err := eventBus.Subscribe(ctx, topicName, handler)
			if err != nil {
				panic(fmt.Sprintf("Failed to subscribe to %s: %v", topicName, err))
			}
		}(topic)
	}
	
	wg.Wait()
	return nil
}

// createStressTestHandler åˆ›å»ºå‹åŠ›æµ‹è¯•å¤„ç†å™¨
func createStressTestHandler(topic string, metrics *StressTestMetrics) MessageHandler {
	return func(ctx context.Context, message []byte) error {
		startTime := time.Now()
		
		// è§£ææ¶ˆæ¯
		var envelope TestEnvelope
		if err := json.Unmarshal(message, &envelope); err != nil {
			atomic.AddInt64(&metrics.ProcessErrors, 1)
			return err
		}
		
		// æ£€æŸ¥æ¶ˆæ¯é¡ºåº
		checkStressTestOrder(envelope, metrics)
		
		// æ¨¡æ‹Ÿå¤„ç†æ—¶é—´ï¼ˆ1-5mséšæœºï¼‰
		processingTime := time.Duration(1+rand.Intn(4)) * time.Millisecond
		time.Sleep(processingTime)
		
		// æ›´æ–°æŒ‡æ ‡
		atomic.AddInt64(&metrics.MessagesReceived, 1)
		
		// è®°å½•å¤„ç†å»¶è¿Ÿ
		latency := time.Since(startTime).Microseconds()
		atomic.AddInt64(&metrics.TotalProcessLatency, latency)
		atomic.AddInt64(&metrics.LatencyCount, 1)
		
		// æ›´æ–°æœ€å¤§å»¶è¿Ÿ
		for {
			current := atomic.LoadInt64(&metrics.MaxProcessLatency)
			if latency <= current || atomic.CompareAndSwapInt64(&metrics.MaxProcessLatency, current, latency) {
				break
			}
		}
		
		return nil
	}
}

// checkStressTestOrder æ£€æŸ¥å‹åŠ›æµ‹è¯•ä¸­çš„æ¶ˆæ¯é¡ºåº
func checkStressTestOrder(envelope TestEnvelope, metrics *StressTestMetrics) {
	aggregateID := envelope.AggregateID
	expectedSeq := envelope.Sequence
	
	// è·å–æˆ–åˆå§‹åŒ–èšåˆåºåˆ—å·
	actualInterface, _ := metrics.AggregateSequences.LoadOrStore(aggregateID, int64(-1))
	lastSeq := actualInterface.(int64)
	
	// æ£€æŸ¥é¡ºåº
	if lastSeq >= 0 && expectedSeq != lastSeq+1 {
		atomic.AddInt64(&metrics.OrderViolations, 1)
	}
	
	// æ›´æ–°åºåˆ—å·
	metrics.AggregateSequences.Store(aggregateID, expectedSeq)
}

// runStressTestProducers è¿è¡Œå‹åŠ›æµ‹è¯•ç”Ÿäº§è€…
func runStressTestProducers(ctx context.Context, eventBus EventBus, config StressTestConfig, metrics *StressTestMetrics) error {
	topics := make([]string, config.TopicCount)
	for i := 0; i < config.TopicCount; i++ {
		topics[i] = fmt.Sprintf("stress-topic-%d", i)
	}

	// è®¡ç®—æ¯ä¸ªç”Ÿäº§è€…çš„å‘é€é€Ÿç‡
	messagesPerProducer := config.MessagesPerSecond / config.ProducerCount
	sendInterval := time.Second / time.Duration(messagesPerProducer)

	var wg sync.WaitGroup
	for i := 0; i < config.ProducerCount; i++ {
		wg.Add(1)
		go func(producerID int) {
			defer wg.Done()
			runSingleProducer(ctx, eventBus, topics, config, metrics, producerID, sendInterval)
		}(i)
	}

	go func() {
		wg.Wait()
	}()

	return nil
}

// runSingleProducer è¿è¡Œå•ä¸ªç”Ÿäº§è€…
func runSingleProducer(ctx context.Context, eventBus EventBus, topics []string, config StressTestConfig, metrics *StressTestMetrics, producerID int, sendInterval time.Duration) {
	ticker := time.NewTicker(sendInterval)
	defer ticker.Stop()
	
	messageCounter := int64(0)
	
	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			// é€‰æ‹©topicå’ŒèšåˆID
			topic := topics[rand.Intn(len(topics))]
			aggregateID := fmt.Sprintf("agg-%d", rand.Intn(config.AggregateCount))
			
			// åˆ›å»ºæ¶ˆæ¯
			envelope := TestEnvelope{
				AggregateID: aggregateID,
				EventType:   fmt.Sprintf("StressEvent-%d", messageCounter%10),
				Sequence:    messageCounter,
				Timestamp:   time.Now(),
				Payload: map[string]interface{}{
					"producerID": producerID,
					"counter":    messageCounter,
					"data":       generateRandomData(config.MessageSize),
				},
				Metadata: map[string]string{
					"producer": fmt.Sprintf("producer-%d", producerID),
					"topic":    topic,
				},
			}
			
			messageBytes, err := json.Marshal(envelope)
			if err != nil {
				atomic.AddInt64(&metrics.SendErrors, 1)
				continue
			}
			
			// å‘é€æ¶ˆæ¯
			sendStart := time.Now()
			err = eventBus.Publish(ctx, topic, messageBytes)
			sendLatency := time.Since(sendStart).Microseconds()
			
			if err != nil {
				atomic.AddInt64(&metrics.SendErrors, 1)
			} else {
				atomic.AddInt64(&metrics.MessagesSent, 1)
				atomic.AddInt64(&metrics.TotalSendLatency, sendLatency)
				
				// æ›´æ–°æœ€å¤§å‘é€å»¶è¿Ÿ
				for {
					current := atomic.LoadInt64(&metrics.MaxSendLatency)
					if sendLatency <= current || atomic.CompareAndSwapInt64(&metrics.MaxSendLatency, current, sendLatency) {
						break
					}
				}
			}
			
			messageCounter++
		}
	}
}

// generateRandomData ç”ŸæˆæŒ‡å®šå¤§å°çš„éšæœºæ•°æ®
func generateRandomData(size int) string {
	const charset = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"
	b := make([]byte, size)
	for i := range b {
		b[i] = charset[rand.Intn(len(charset))]
	}
	return string(b)
}

// startSystemMonitoring å¯åŠ¨ç³»ç»Ÿç›‘æ§
func startSystemMonitoring(metrics *StressTestMetrics) func() {
	ctx, cancel := context.WithCancel(context.Background())

	go func() {
		ticker := time.NewTicker(1 * time.Second)
		defer ticker.Stop()

		for {
			select {
			case <-ctx.Done():
				return
			case <-ticker.C:
				// ç›‘æ§å†…å­˜ä½¿ç”¨
				var m runtime.MemStats
				runtime.ReadMemStats(&m)
				if m.Alloc > metrics.MaxMemoryUsage {
					metrics.MaxMemoryUsage = m.Alloc
				}

				// ç›‘æ§åç¨‹æ•°é‡
				goroutines := runtime.NumGoroutine()
				if goroutines > metrics.MaxGoroutines {
					metrics.MaxGoroutines = goroutines
				}
			}
		}
	}()

	return cancel
}

// waitForRemainingMessages ç­‰å¾…å‰©ä½™æ¶ˆæ¯å¤„ç†å®Œæˆ
func waitForRemainingMessages(metrics *StressTestMetrics, timeout time.Duration, t *testing.T) {
	deadline := time.Now().Add(timeout)
	ticker := time.NewTicker(2 * time.Second)
	defer ticker.Stop()

	lastReceived := atomic.LoadInt64(&metrics.MessagesReceived)
	stableCount := 0

	for time.Now().Before(deadline) {
		select {
		case <-ticker.C:
			currentReceived := atomic.LoadInt64(&metrics.MessagesReceived)
			sent := atomic.LoadInt64(&metrics.MessagesSent)

			t.Logf("ğŸ“Š ç­‰å¾…å¤„ç†å®Œæˆ: %d/%d (%.1f%%)",
				currentReceived, sent, float64(currentReceived)/float64(sent)*100)

			// å¦‚æœæ¥æ”¶æ•°é‡æ²¡æœ‰å˜åŒ–ï¼Œè®¤ä¸ºå¤„ç†å®Œæˆ
			if currentReceived == lastReceived {
				stableCount++
				if stableCount >= 3 { // è¿ç»­3æ¬¡æ£€æŸ¥éƒ½æ²¡æœ‰å˜åŒ–
					t.Log("âœ… æ¶ˆæ¯å¤„ç†å®Œæˆ")
					return
				}
			} else {
				stableCount = 0
				lastReceived = currentReceived
			}
		}
	}

	t.Log("âš ï¸  ç­‰å¾…è¶…æ—¶ï¼Œå¯èƒ½è¿˜æœ‰æ¶ˆæ¯åœ¨å¤„ç†ä¸­")
}

// analyzeStressTestResults åˆ†æå‹åŠ›æµ‹è¯•ç»“æœ
func analyzeStressTestResults(config StressTestConfig, metrics *StressTestMetrics, t *testing.T) {
	metrics.EndTime = time.Now()
	duration := metrics.EndTime.Sub(metrics.StartTime)

	// åŸºæœ¬ç»Ÿè®¡
	sent := atomic.LoadInt64(&metrics.MessagesSent)
	received := atomic.LoadInt64(&metrics.MessagesReceived)
	sendErrors := atomic.LoadInt64(&metrics.SendErrors)
	processErrors := atomic.LoadInt64(&metrics.ProcessErrors)
	orderViolations := atomic.LoadInt64(&metrics.OrderViolations)

	// æ€§èƒ½è®¡ç®—
	sendThroughput := float64(sent) / duration.Seconds()
	receiveThroughput := float64(received) / duration.Seconds()

	// å»¶è¿Ÿè®¡ç®—
	latencyCount := atomic.LoadInt64(&metrics.LatencyCount)
	var avgSendLatency, avgProcessLatency float64
	if latencyCount > 0 {
		avgSendLatency = float64(atomic.LoadInt64(&metrics.TotalSendLatency)) / float64(latencyCount)
		avgProcessLatency = float64(atomic.LoadInt64(&metrics.TotalProcessLatency)) / float64(latencyCount)
	}

	maxSendLatency := atomic.LoadInt64(&metrics.MaxSendLatency)
	maxProcessLatency := atomic.LoadInt64(&metrics.MaxProcessLatency)

	// è¾“å‡ºè¯¦ç»†ç»“æœ
	t.Log("\n" + strings.Repeat("=", 100))
	t.Log("ğŸ”¥ Kafka UnifiedConsumer å‹åŠ›æµ‹è¯•ç»“æœ")
	t.Log(strings.Repeat("=", 100))

	t.Logf("ğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
	t.Logf("   æµ‹è¯•æŒç»­æ—¶é—´: %.2f ç§’", duration.Seconds())
	t.Logf("   å‘é€æ¶ˆæ¯æ•°: %d", sent)
	t.Logf("   æ¥æ”¶æ¶ˆæ¯æ•°: %d", received)
	t.Logf("   æ¶ˆæ¯æˆåŠŸç‡: %.2f%%", float64(received)/float64(sent)*100)
	t.Logf("   å‘é€é”™è¯¯: %d", sendErrors)
	t.Logf("   å¤„ç†é”™è¯¯: %d", processErrors)

	t.Logf("\nğŸš€ æ€§èƒ½æŒ‡æ ‡:")
	t.Logf("   å‘é€ååé‡: %.2f msg/s", sendThroughput)
	t.Logf("   æ¥æ”¶ååé‡: %.2f msg/s", receiveThroughput)
	t.Logf("   ç›®æ ‡ååé‡: %d msg/s", config.MessagesPerSecond)
	t.Logf("   ååé‡è¾¾æˆç‡: %.1f%%", sendThroughput/float64(config.MessagesPerSecond)*100)

	t.Logf("\nâ±ï¸  å»¶è¿Ÿç»Ÿè®¡:")
	t.Logf("   å¹³å‡å‘é€å»¶è¿Ÿ: %.2f Î¼s", avgSendLatency)
	t.Logf("   æœ€å¤§å‘é€å»¶è¿Ÿ: %d Î¼s", maxSendLatency)
	t.Logf("   å¹³å‡å¤„ç†å»¶è¿Ÿ: %.2f Î¼s", avgProcessLatency)
	t.Logf("   æœ€å¤§å¤„ç†å»¶è¿Ÿ: %d Î¼s", maxProcessLatency)

	t.Logf("\nğŸ” é¡ºåºæ€§æ£€æŸ¥:")
	t.Logf("   é¡ºåºè¿åæ¬¡æ•°: %d", orderViolations)
	t.Logf("   é¡ºåºæ­£ç¡®ç‡: %.4f%%", (1.0-float64(orderViolations)/float64(received))*100)

	t.Logf("\nğŸ’¾ ç³»ç»Ÿèµ„æº:")
	t.Logf("   æœ€å¤§å†…å­˜ä½¿ç”¨: %.2f MB", float64(metrics.MaxMemoryUsage)/1024/1024)
	t.Logf("   æœ€å¤§åç¨‹æ•°: %d", metrics.MaxGoroutines)

	// èšåˆå¤„ç†ç»Ÿè®¡
	analyzeAggregateDistribution(metrics, t)

	// æ€§èƒ½æ–­è¨€
	assert.True(t, sendThroughput > float64(config.MessagesPerSecond)*0.8,
		"å‘é€ååé‡åº”è¯¥è¾¾åˆ°ç›®æ ‡çš„80%ä»¥ä¸Š")
	assert.True(t, receiveThroughput > float64(config.MessagesPerSecond)*0.8,
		"æ¥æ”¶ååé‡åº”è¯¥è¾¾åˆ°ç›®æ ‡çš„80%ä»¥ä¸Š")
	assert.True(t, float64(received)/float64(sent) > 0.95,
		"æ¶ˆæ¯æˆåŠŸç‡åº”è¯¥ > 95%")
	assert.Equal(t, int64(0), orderViolations,
		"ä¸åº”è¯¥æœ‰ä»»ä½•é¡ºåºè¿å")
	assert.True(t, avgSendLatency < 10000,
		"å¹³å‡å‘é€å»¶è¿Ÿåº”è¯¥ < 10ms")
	assert.True(t, avgProcessLatency < 20000,
		"å¹³å‡å¤„ç†å»¶è¿Ÿåº”è¯¥ < 20ms")

	t.Log(strings.Repeat("=", 100))
}

// analyzeAggregateDistribution åˆ†æèšåˆåˆ†å¸ƒæƒ…å†µ
func analyzeAggregateDistribution(metrics *StressTestMetrics, t *testing.T) {
	t.Logf("\nğŸ“ˆ èšåˆIDåˆ†å¸ƒåˆ†æ:")

	var aggregateCount int64
	var minMessages, maxMessages int64 = 999999, 0
	var totalMessages int64

	metrics.AggregateSequences.Range(func(key, value interface{}) bool {
		sequence := value.(int64)
		messageCount := sequence + 1 // åºåˆ—å·ä»0å¼€å§‹

		aggregateCount++
		totalMessages += messageCount

		if messageCount < minMessages {
			minMessages = messageCount
		}
		if messageCount > maxMessages {
			maxMessages = messageCount
		}

		return true
	})

	if aggregateCount > 0 {
		avgMessages := float64(totalMessages) / float64(aggregateCount)
		t.Logf("   èšåˆIDæ€»æ•°: %d", aggregateCount)
		t.Logf("   æ¶ˆæ¯åˆ†å¸ƒ: %d - %d (æœ€å°‘-æœ€å¤š)", minMessages, maxMessages)
		t.Logf("   å¹³å‡æ¯èšåˆ: %.2f æ¶ˆæ¯", avgMessages)

		// è®¡ç®—åˆ†å¸ƒå‡åŒ€æ€§
		variance := float64(maxMessages - minMessages)
		uniformity := (1.0 - variance/avgMessages) * 100
		if uniformity < 0 {
			uniformity = 0
		}
		t.Logf("   åˆ†å¸ƒå‡åŒ€æ€§: %.1f%%", uniformity)
	}
}
