package eventbus

import (
	"context"
	"encoding/json"
	"fmt"
	"math/rand"
	"strings"
	"sync"
	"sync/atomic"
	"testing"
	"time"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
)

// ğŸ¯ Kafka UnifiedConsumeræ€§èƒ½åŸºå‡†æµ‹è¯•
// æµ‹è¯•åœºæ™¯ï¼š
// - 1ä¸ªæ¶ˆè´¹è€…ç»„ï¼Œ1ä¸ªKafkaè¿æ¥
// - è®¢é˜…10ä¸ªtopicä¸»é¢˜
// - å‘é€ç«¯å‘é€envelopeæ ¼å¼æ¶ˆæ¯
// - è®¢é˜…ç«¯æ ¹æ®èšåˆID hashåˆ°keyed-workeræ± 
// - ç¡®ä¿åŒä¸€èšåˆIDçš„äº‹ä»¶ä¸¥æ ¼é¡ºåºå¤„ç†
// - Kafkaæ¶ˆæ¯æŒä¹…åŒ–

// BenchmarkConfig åŸºå‡†æµ‹è¯•é…ç½®
type BenchmarkConfig struct {
	TopicCount       int           // topicæ•°é‡
	AggregateCount   int           // èšåˆIDæ•°é‡
	MessagesPerTopic int           // æ¯ä¸ªtopicçš„æ¶ˆæ¯æ•°é‡
	WorkerPoolSize   int           // workeræ± å¤§å°
	TestDuration     time.Duration // æµ‹è¯•æŒç»­æ—¶é—´
	BatchSize        int           // æ‰¹é‡å‘é€å¤§å°
}

// DefaultBenchmarkConfig é»˜è®¤åŸºå‡†æµ‹è¯•é…ç½®
func DefaultBenchmarkConfig() BenchmarkConfig {
	return BenchmarkConfig{
		TopicCount:       10,
		AggregateCount:   100,
		MessagesPerTopic: 1000,
		WorkerPoolSize:   20,
		TestDuration:     30 * time.Second,
		BatchSize:        50,
	}
}

// BenchmarkMetrics åŸºå‡†æµ‹è¯•æŒ‡æ ‡
type BenchmarkMetrics struct {
	// å‘é€æŒ‡æ ‡
	MessagesSent     int64         // å‘é€æ¶ˆæ¯æ€»æ•°
	SendErrors       int64         // å‘é€é”™è¯¯æ•°
	SendLatencySum   int64         // å‘é€å»¶è¿Ÿæ€»å’Œ(å¾®ç§’)
	SendLatencyCount int64         // å‘é€å»¶è¿Ÿè®¡æ•°
	
	// æ¥æ”¶æŒ‡æ ‡
	MessagesReceived int64         // æ¥æ”¶æ¶ˆæ¯æ€»æ•°
	ProcessErrors    int64         // å¤„ç†é”™è¯¯æ•°
	ProcessLatencySum int64        // å¤„ç†å»¶è¿Ÿæ€»å’Œ(å¾®ç§’)
	ProcessLatencyCount int64      // å¤„ç†å»¶è¿Ÿè®¡æ•°
	
	// é¡ºåºæ€§æŒ‡æ ‡
	OrderViolations  int64         // é¡ºåºè¿åæ¬¡æ•°
	DuplicateMessages int64        // é‡å¤æ¶ˆæ¯æ•°
	
	// æ€§èƒ½æŒ‡æ ‡
	StartTime        time.Time     // å¼€å§‹æ—¶é—´
	EndTime          time.Time     // ç»“æŸæ—¶é—´
	PeakThroughput   int64         // å³°å€¼ååé‡(msg/s)
	
	// èšåˆIDå¤„ç†ç»Ÿè®¡
	AggregateStats   sync.Map      // map[string]*AggregateProcessingStats
}

// AggregateProcessingStats èšåˆIDå¤„ç†ç»Ÿè®¡
type AggregateProcessingStats struct {
	MessageCount    int64     // æ¶ˆæ¯æ•°é‡
	LastSequence    int64     // æœ€åå¤„ç†çš„åºåˆ—å·
	OrderViolations int64     // é¡ºåºè¿åæ¬¡æ•°
	FirstMessageTime time.Time // ç¬¬ä¸€æ¡æ¶ˆæ¯æ—¶é—´
	LastMessageTime  time.Time // æœ€åä¸€æ¡æ¶ˆæ¯æ—¶é—´
}

// TestEnvelope æµ‹è¯•ç”¨çš„Envelopeæ¶ˆæ¯æ ¼å¼
type TestEnvelope struct {
	AggregateID   string                 `json:"aggregateId"`
	EventType     string                 `json:"eventType"`
	Sequence      int64                  `json:"sequence"`
	Timestamp     time.Time              `json:"timestamp"`
	Payload       map[string]interface{} `json:"payload"`
	Metadata      map[string]string      `json:"metadata"`
}

// TestBenchmarkKafkaUnifiedConsumerPerformance ä¸»è¦çš„æ€§èƒ½åŸºå‡†æµ‹è¯•
func TestBenchmarkKafkaUnifiedConsumerPerformance(t *testing.T) {
	if testing.Short() {
		t.Skip("Skipping benchmark test in short mode")
	}

	config := DefaultBenchmarkConfig()
	metrics := &BenchmarkMetrics{
		StartTime: time.Now(),
	}

	// åˆ›å»ºKafka EventBus
	kafkaConfig := &KafkaConfig{
		Brokers: []string{"localhost:29092"},
		Producer: ProducerConfig{
			RequiredAcks:   -1, // WaitForAllï¼Œå¹‚ç­‰æ€§ç”Ÿäº§è€…è¦æ±‚
			Compression:    "snappy",
			FlushFrequency: 10 * time.Millisecond,
			FlushMessages:  100,
			Timeout:        30 * time.Second,
			// ç¨‹åºå‘˜æ§åˆ¶çš„æ€§èƒ½ä¼˜åŒ–å­—æ®µ
			FlushBytes:      1024 * 1024,     // 1MB
			RetryMax:        3,
			BatchSize:       16 * 1024,       // 16KB
			BufferSize:      32 * 1024 * 1024, // 32MB
			Idempotent:      true,
			MaxMessageBytes: 1024 * 1024,
			PartitionerType: "hash",
			LingerMs:        5 * time.Millisecond,
			CompressionLevel: 6,
			MaxInFlight:     1, // å¹‚ç­‰æ€§ç”Ÿäº§è€…è¦æ±‚MaxInFlight=1
		},
		Consumer: ConsumerConfig{
			GroupID:           "benchmark-test-group",
			AutoOffsetReset:   "earliest",
			SessionTimeout:    30 * time.Second,
			HeartbeatInterval: 3 * time.Second,
			// ç¨‹åºå‘˜æ§åˆ¶çš„æ€§èƒ½ä¼˜åŒ–å­—æ®µ
			MaxProcessingTime:  30 * time.Second,
			FetchMinBytes:      1024,
			FetchMaxBytes:      50 * 1024 * 1024, // 50MB
			FetchMaxWait:       500 * time.Millisecond,
			MaxPollRecords:     500,
			EnableAutoCommit:   false,
			AutoCommitInterval: 5 * time.Second,
			IsolationLevel:     "read_committed",
			RebalanceStrategy:  "range",
		},
		HealthCheckInterval: 30 * time.Second,
		ClientID:           "benchmark-client",
		MetadataRefreshFreq: 10 * time.Minute,
		MetadataRetryMax:   3,
		MetadataRetryBackoff: 250 * time.Millisecond,
		Net: NetConfig{
			DialTimeout:   30 * time.Second,
			ReadTimeout:   30 * time.Second,
			WriteTimeout:  30 * time.Second,
			KeepAlive:     30 * time.Second,
			MaxIdleConns:  10,
			MaxOpenConns:  100,
		},
		Security: SecurityConfig{Enabled: false},
	}

	eventBus, err := NewKafkaEventBus(kafkaConfig)
	require.NoError(t, err)
	defer eventBus.Close()

	ctx, cancel := context.WithTimeout(context.Background(), config.TestDuration+30*time.Second)
	defer cancel()

	// ç”Ÿæˆæµ‹è¯•topicåˆ—è¡¨
	topics := make([]string, config.TopicCount)
	for i := 0; i < config.TopicCount; i++ {
		topics[i] = fmt.Sprintf("benchmark-topic-%d", i)
	}

	t.Logf("ğŸš€ å¼€å§‹Kafka UnifiedConsumeræ€§èƒ½åŸºå‡†æµ‹è¯•")
	t.Logf("ğŸ“Š æµ‹è¯•é…ç½®: %d topics, %d aggregates, %d messages/topic, %d workers",
		config.TopicCount, config.AggregateCount, config.MessagesPerTopic, config.WorkerPoolSize)

	// ç¬¬ä¸€é˜¶æ®µï¼šè®¾ç½®è®¢é˜…è€…
	t.Log("ğŸ“¡ é˜¶æ®µ1: è®¾ç½®UnifiedConsumerè®¢é˜…è€…...")
	err = setupUnifiedConsumerSubscriptions(ctx, eventBus, topics, config, metrics)
	require.NoError(t, err)

	// ç­‰å¾…è®¢é˜…è€…å‡†å¤‡å°±ç»ª
	time.Sleep(2 * time.Second)

	// ç¬¬äºŒé˜¶æ®µï¼šå‘é€æµ‹è¯•æ¶ˆæ¯
	t.Log("ğŸ“¤ é˜¶æ®µ2: å‘é€æµ‹è¯•æ¶ˆæ¯...")
	err = sendBenchmarkMessages(ctx, eventBus, topics, config, metrics)
	require.NoError(t, err)

	// ç¬¬ä¸‰é˜¶æ®µï¼šç­‰å¾…æ¶ˆæ¯å¤„ç†å®Œæˆ
	t.Log("â³ é˜¶æ®µ3: ç­‰å¾…æ¶ˆæ¯å¤„ç†å®Œæˆ...")
	waitForMessageProcessing(ctx, config, metrics, t)

	// ç¬¬å››é˜¶æ®µï¼šåˆ†æç»“æœ
	t.Log("ğŸ“ˆ é˜¶æ®µ4: åˆ†ææ€§èƒ½ç»“æœ...")
	analyzePerformanceResults(config, metrics, t)
}

// setupUnifiedConsumerSubscriptions è®¾ç½®UnifiedConsumerè®¢é˜…
func setupUnifiedConsumerSubscriptions(ctx context.Context, eventBus EventBus, topics []string, config BenchmarkConfig, metrics *BenchmarkMetrics) error {
	var wg sync.WaitGroup
	
	// ä¸ºæ¯ä¸ªtopicè®¾ç½®è®¢é˜…å¤„ç†å™¨
	for _, topic := range topics {
		wg.Add(1)
		go func(topicName string) {
			defer wg.Done()
			
			// åˆ›å»ºæ¶ˆæ¯å¤„ç†å™¨
			handler := createBenchmarkMessageHandler(topicName, config, metrics)
			
			// ä½¿ç”¨åŸºæœ¬çš„Subscribeæ–¹æ³•ï¼ˆUnifiedConsumerå†…éƒ¨ä¼šå¤„ç†Keyed-Workeræ± ï¼‰
			err := eventBus.Subscribe(ctx, topicName, handler)
			if err != nil {
				panic(fmt.Sprintf("Failed to subscribe to topic %s: %v", topicName, err))
			}
		}(topic)
	}
	
	wg.Wait()
	return nil
}

// createBenchmarkMessageHandler åˆ›å»ºåŸºå‡†æµ‹è¯•æ¶ˆæ¯å¤„ç†å™¨
func createBenchmarkMessageHandler(topic string, config BenchmarkConfig, metrics *BenchmarkMetrics) MessageHandler {
	return func(ctx context.Context, message []byte) error {
		startTime := time.Now()
		
		// è§£æEnvelopeæ¶ˆæ¯
		var envelope TestEnvelope
		if err := json.Unmarshal(message, &envelope); err != nil {
			atomic.AddInt64(&metrics.ProcessErrors, 1)
			return fmt.Errorf("failed to unmarshal envelope: %w", err)
		}
		
		// æ›´æ–°æ¥æ”¶æŒ‡æ ‡
		atomic.AddInt64(&metrics.MessagesReceived, 1)
		
		// æ£€æŸ¥æ¶ˆæ¯é¡ºåºæ€§
		checkMessageOrder(envelope, metrics)
		
		// æ¨¡æ‹Ÿä¸šåŠ¡å¤„ç†ï¼ˆå¯é…ç½®çš„å¤„ç†æ—¶é—´ï¼‰
		processingTime := time.Duration(rand.Intn(5)) * time.Millisecond
		time.Sleep(processingTime)
		
		// æ›´æ–°å¤„ç†å»¶è¿ŸæŒ‡æ ‡
		latency := time.Since(startTime).Microseconds()
		atomic.AddInt64(&metrics.ProcessLatencySum, latency)
		atomic.AddInt64(&metrics.ProcessLatencyCount, 1)
		
		return nil
	}
}

// checkMessageOrder æ£€æŸ¥æ¶ˆæ¯é¡ºåºæ€§
func checkMessageOrder(envelope TestEnvelope, metrics *BenchmarkMetrics) {
	statsInterface, _ := metrics.AggregateStats.LoadOrStore(envelope.AggregateID, &AggregateProcessingStats{
		FirstMessageTime: envelope.Timestamp,
	})
	
	stats := statsInterface.(*AggregateProcessingStats)
	
	// åŸå­æ“ä½œæ›´æ–°ç»Ÿè®¡
	currentCount := atomic.AddInt64(&stats.MessageCount, 1)
	
	// æ£€æŸ¥åºåˆ—å·é¡ºåº
	if envelope.Sequence <= stats.LastSequence && currentCount > 1 {
		atomic.AddInt64(&stats.OrderViolations, 1)
		atomic.AddInt64(&metrics.OrderViolations, 1)
	}
	
	stats.LastSequence = envelope.Sequence
	stats.LastMessageTime = envelope.Timestamp
}

// sendBenchmarkMessages å‘é€åŸºå‡†æµ‹è¯•æ¶ˆæ¯
func sendBenchmarkMessages(ctx context.Context, eventBus EventBus, topics []string, config BenchmarkConfig, metrics *BenchmarkMetrics) error {
	var wg sync.WaitGroup

	// ä¸ºæ¯ä¸ªtopicå¯åŠ¨å‘é€åç¨‹
	for topicIndex, topic := range topics {
		wg.Add(1)
		go func(topicName string, tIndex int) {
			defer wg.Done()

			// ä¸ºè¯¥topicç”Ÿæˆæ¶ˆæ¯
			for msgIndex := 0; msgIndex < config.MessagesPerTopic; msgIndex++ {
				// é€‰æ‹©èšåˆIDï¼ˆç¡®ä¿åˆ†å¸ƒå‡åŒ€ï¼‰
				aggregateID := fmt.Sprintf("aggregate-%d", (tIndex*config.MessagesPerTopic+msgIndex)%config.AggregateCount)

				// åˆ›å»ºEnvelopeæ¶ˆæ¯
				envelope := TestEnvelope{
					AggregateID: aggregateID,
					EventType:   fmt.Sprintf("TestEvent-%d", msgIndex%5),
					Sequence:    int64(msgIndex),
					Timestamp:   time.Now(),
					Payload: map[string]interface{}{
						"topicIndex":   tIndex,
						"messageIndex": msgIndex,
						"data":         fmt.Sprintf("test-data-%d-%d", tIndex, msgIndex),
						"value":        rand.Intn(1000),
					},
					Metadata: map[string]string{
						"source":    "benchmark-test",
						"topic":     topicName,
						"aggregate": aggregateID,
					},
				}

				// åºåˆ—åŒ–æ¶ˆæ¯
				messageBytes, err := json.Marshal(envelope)
				if err != nil {
					atomic.AddInt64(&metrics.SendErrors, 1)
					continue
				}

				// å‘é€æ¶ˆæ¯å¹¶æµ‹é‡å»¶è¿Ÿ
				sendStart := time.Now()
				err = eventBus.Publish(ctx, topicName, messageBytes)
				sendLatency := time.Since(sendStart).Microseconds()

				if err != nil {
					atomic.AddInt64(&metrics.SendErrors, 1)
				} else {
					atomic.AddInt64(&metrics.MessagesSent, 1)
					atomic.AddInt64(&metrics.SendLatencySum, sendLatency)
					atomic.AddInt64(&metrics.SendLatencyCount, 1)
				}

				// æ‰¹é‡å‘é€æ§åˆ¶
				if msgIndex%config.BatchSize == 0 {
					time.Sleep(1 * time.Millisecond) // å°å»¶è¿Ÿé¿å…è¿‡è½½
				}
			}
		}(topic, topicIndex)
	}

	wg.Wait()
	return nil
}

// waitForMessageProcessing ç­‰å¾…æ¶ˆæ¯å¤„ç†å®Œæˆ
func waitForMessageProcessing(ctx context.Context, config BenchmarkConfig, metrics *BenchmarkMetrics, t *testing.T) {
	expectedMessages := int64(config.TopicCount * config.MessagesPerTopic)
	timeout := time.After(60 * time.Second) // æœ€å¤§ç­‰å¾…æ—¶é—´
	ticker := time.NewTicker(2 * time.Second)
	defer ticker.Stop()

	for {
		select {
		case <-timeout:
			t.Logf("âš ï¸  ç­‰å¾…è¶…æ—¶ï¼Œå·²å¤„ç† %d/%d æ¶ˆæ¯",
				atomic.LoadInt64(&metrics.MessagesReceived), expectedMessages)
			return
		case <-ticker.C:
			received := atomic.LoadInt64(&metrics.MessagesReceived)
			t.Logf("ğŸ“Š å¤„ç†è¿›åº¦: %d/%d (%.1f%%)",
				received, expectedMessages, float64(received)/float64(expectedMessages)*100)

			if received >= expectedMessages {
				t.Log("âœ… æ‰€æœ‰æ¶ˆæ¯å¤„ç†å®Œæˆ")
				return
			}
		case <-ctx.Done():
			return
		}
	}
}

// analyzePerformanceResults åˆ†ææ€§èƒ½ç»“æœ
func analyzePerformanceResults(config BenchmarkConfig, metrics *BenchmarkMetrics, t *testing.T) {
	metrics.EndTime = time.Now()
	duration := metrics.EndTime.Sub(metrics.StartTime)

	// åŸºæœ¬ç»Ÿè®¡
	messagesSent := atomic.LoadInt64(&metrics.MessagesSent)
	messagesReceived := atomic.LoadInt64(&metrics.MessagesReceived)
	sendErrors := atomic.LoadInt64(&metrics.SendErrors)
	processErrors := atomic.LoadInt64(&metrics.ProcessErrors)
	orderViolations := atomic.LoadInt64(&metrics.OrderViolations)

	// è®¡ç®—ååé‡
	sendThroughput := float64(messagesSent) / duration.Seconds()
	receiveThroughput := float64(messagesReceived) / duration.Seconds()

	// è®¡ç®—å¹³å‡å»¶è¿Ÿ
	var avgSendLatency, avgProcessLatency float64
	if sendLatencyCount := atomic.LoadInt64(&metrics.SendLatencyCount); sendLatencyCount > 0 {
		avgSendLatency = float64(atomic.LoadInt64(&metrics.SendLatencySum)) / float64(sendLatencyCount)
	}
	if processLatencyCount := atomic.LoadInt64(&metrics.ProcessLatencyCount); processLatencyCount > 0 {
		avgProcessLatency = float64(atomic.LoadInt64(&metrics.ProcessLatencySum)) / float64(processLatencyCount)
	}

	// è¾“å‡ºè¯¦ç»†ç»“æœ
	t.Log("\n" + strings.Repeat("=", 80))
	t.Log("ğŸ¯ Kafka UnifiedConsumer æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ")
	t.Log(strings.Repeat("=", 80))

	t.Logf("ğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
	t.Logf("   æµ‹è¯•æŒç»­æ—¶é—´: %.2f ç§’", duration.Seconds())
	t.Logf("   å‘é€æ¶ˆæ¯æ•°: %d", messagesSent)
	t.Logf("   æ¥æ”¶æ¶ˆæ¯æ•°: %d", messagesReceived)
	t.Logf("   æ¶ˆæ¯æˆåŠŸç‡: %.2f%%", float64(messagesReceived)/float64(messagesSent)*100)

	t.Logf("\nğŸš€ æ€§èƒ½æŒ‡æ ‡:")
	t.Logf("   å‘é€ååé‡: %.2f msg/s", sendThroughput)
	t.Logf("   æ¥æ”¶ååé‡: %.2f msg/s", receiveThroughput)
	t.Logf("   å¹³å‡å‘é€å»¶è¿Ÿ: %.2f Î¼s", avgSendLatency)
	t.Logf("   å¹³å‡å¤„ç†å»¶è¿Ÿ: %.2f Î¼s", avgProcessLatency)

	t.Logf("\nâŒ é”™è¯¯ç»Ÿè®¡:")
	t.Logf("   å‘é€é”™è¯¯: %d", sendErrors)
	t.Logf("   å¤„ç†é”™è¯¯: %d", processErrors)
	t.Logf("   é¡ºåºè¿å: %d", orderViolations)

	// èšåˆIDå¤„ç†ç»Ÿè®¡
	analyzeAggregateProcessing(metrics, t)

	// æ€§èƒ½æ–­è¨€
	assert.True(t, sendThroughput > 1000, "å‘é€ååé‡åº”è¯¥ > 1000 msg/s")
	assert.True(t, receiveThroughput > 1000, "æ¥æ”¶ååé‡åº”è¯¥ > 1000 msg/s")
	assert.Equal(t, int64(0), orderViolations, "ä¸åº”è¯¥æœ‰é¡ºåºè¿å")
	assert.True(t, float64(messagesReceived)/float64(messagesSent) > 0.99, "æ¶ˆæ¯æˆåŠŸç‡åº”è¯¥ > 99%")

	t.Log(strings.Repeat("=", 80))
}

// analyzeAggregateProcessing åˆ†æèšåˆIDå¤„ç†æƒ…å†µ
func analyzeAggregateProcessing(metrics *BenchmarkMetrics, t *testing.T) {
	t.Logf("\nğŸ” èšåˆIDå¤„ç†åˆ†æ:")

	var totalAggregates int64
	var totalOrderViolations int64
	var minMessages, maxMessages int64 = 999999, 0

	metrics.AggregateStats.Range(func(key, value interface{}) bool {
		aggregateID := key.(string)
		stats := value.(*AggregateProcessingStats)

		messageCount := atomic.LoadInt64(&stats.MessageCount)
		violations := atomic.LoadInt64(&stats.OrderViolations)

		totalAggregates++
		totalOrderViolations += violations

		if messageCount < minMessages {
			minMessages = messageCount
		}
		if messageCount > maxMessages {
			maxMessages = messageCount
		}

		// è¾“å‡ºå‰5ä¸ªèšåˆçš„è¯¦ç»†ä¿¡æ¯
		if totalAggregates <= 5 {
			duration := stats.LastMessageTime.Sub(stats.FirstMessageTime)
			t.Logf("   èšåˆ %s: %d æ¶ˆæ¯, %d è¿å, å¤„ç†æ—¶é•¿ %.2f ms",
				aggregateID, messageCount, violations, float64(duration.Nanoseconds())/1e6)
		}

		return true
	})

	t.Logf("   æ€»èšåˆæ•°: %d", totalAggregates)
	t.Logf("   æ¶ˆæ¯åˆ†å¸ƒ: %d - %d (æœ€å°‘-æœ€å¤š)", minMessages, maxMessages)
	t.Logf("   æ€»é¡ºåºè¿å: %d", totalOrderViolations)
}
