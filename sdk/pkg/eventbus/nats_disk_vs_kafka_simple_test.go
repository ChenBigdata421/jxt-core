package eventbus

import (
	"context"
	"encoding/json"
	"fmt"
	"sync"
	"sync/atomic"
	"testing"
	"time"

	"github.com/stretchr/testify/require"
)

// SimpleDiskTestMessage ç®€å•ç£ç›˜æµ‹è¯•æ¶ˆæ¯
type SimpleDiskTestMessage struct {
	ID        string    `json:"id"`
	Topic     string    `json:"topic"`
	Sequence  int64     `json:"sequence"`
	Timestamp time.Time `json:"timestamp"`
	Data      string    `json:"data"`
}

// SimpleDiskMetrics ç®€å•ç£ç›˜æµ‹è¯•æŒ‡æ ‡
type SimpleDiskMetrics struct {
	MessagesSent     int64
	MessagesReceived int64
	SendErrors       int64
	ProcessErrors    int64
	StartTime        time.Time
	EndTime          time.Time
	OrderViolations  int64
	SendLatencySum   int64
	SendLatencyCount int64
}

// TestNATSDiskLowPressureSimple NATSç£ç›˜å­˜å‚¨ä½å‹åŠ›ç®€å•æµ‹è¯•
func TestNATSDiskLowPressureSimple(t *testing.T) {
	if testing.Short() {
		t.Skip("Skipping NATS disk low pressure simple test in short mode")
	}

	metrics := &SimpleDiskMetrics{StartTime: time.Now()}

	// ğŸ”µ NATSç£ç›˜å­˜å‚¨é…ç½®
	natsConfig := &NATSConfig{
		URLs:                []string{"nats://localhost:4223"},
		ClientID:            "nats-disk-simple-client",
		MaxReconnects:       10,
		ReconnectWait:       1 * time.Second,
		ConnectionTimeout:   30 * time.Second,
		HealthCheckInterval: 60 * time.Second,
		JetStream: JetStreamConfig{
			Enabled: true, // å¯ç”¨JetStream
			Stream: StreamConfig{
				Name:      "SIMPLE_DISK_STREAM",
				Subjects:  []string{"simple.disk.>"},
				Storage:   "file", // ğŸ”‘ å…³é”®ï¼šä½¿ç”¨ç£ç›˜å­˜å‚¨
				Retention: "limits",
				MaxAge:    24 * time.Hour,
				MaxBytes:  50 * 1024 * 1024, // 50MB
				MaxMsgs:   50000,
				Replicas:  1,
				Discard:   "old",
			},
			Consumer: NATSConsumerConfig{
				DurableName:   "simple-disk-consumer",
				DeliverPolicy: "all",
				AckPolicy:     "explicit",
				ReplayPolicy:  "instant",
				MaxAckPending: 500,
				MaxWaiting:    256,
				MaxDeliver:    3,
				BackOff:       []time.Duration{1 * time.Second, 2 * time.Second},
			},
			PublishTimeout: 10 * time.Second,
		},
	}

	eventBus, err := NewNATSEventBus(natsConfig)
	require.NoError(t, err, "Failed to create NATS disk EventBus")
	defer eventBus.Close()

	// ç­‰å¾…JetStreamåˆå§‹åŒ–
	time.Sleep(5 * time.Second)

	t.Logf("ğŸ”µ Starting NATS Disk Low Pressure Simple Test")

	// ä½å‹åŠ›ï¼š2ä¸ªtopicï¼Œæ¯ä¸ª300æ¡æ¶ˆæ¯ï¼Œæ€»è®¡600æ¡
	runSimpleDiskTest(t, eventBus, "NATS-Disk", 2, 300, metrics)
	analyzeSimpleDiskResults(t, "NATS-Disk", metrics)
}

// TestKafkaDiskLowPressureSimple Kafkaç£ç›˜å­˜å‚¨ä½å‹åŠ›ç®€å•æµ‹è¯•
func TestKafkaDiskLowPressureSimple(t *testing.T) {
	if testing.Short() {
		t.Skip("Skipping Kafka disk low pressure simple test in short mode")
	}

	metrics := &SimpleDiskMetrics{StartTime: time.Now()}

	// ğŸŸ  Kafkaç£ç›˜å­˜å‚¨é…ç½®
	kafkaConfig := &KafkaConfig{
		Brokers: []string{"localhost:29093"},
		Producer: ProducerConfig{
			RequiredAcks:     1,
			Compression:      "none",
			FlushFrequency:   10 * time.Millisecond,
			FlushMessages:    10,
			Timeout:          30 * time.Second,
			FlushBytes:       64 * 1024,
			RetryMax:         3,
			BatchSize:        4 * 1024,
			BufferSize:       8 * 1024 * 1024,
			Idempotent:       false,
			MaxMessageBytes:  256 * 1024,
			PartitionerType:  "round_robin",
			LingerMs:         5 * time.Millisecond,
			CompressionLevel: 1,
			MaxInFlight:      3,
		},
		Consumer: ConsumerConfig{
			GroupID:            "kafka-disk-simple-group",
			AutoOffsetReset:    "earliest",
			SessionTimeout:     60 * time.Second,
			HeartbeatInterval:  20 * time.Second,
			MaxProcessingTime:  90 * time.Second,
			FetchMinBytes:      1,
			FetchMaxBytes:      5 * 1024 * 1024,
			FetchMaxWait:       1 * time.Second,
			MaxPollRecords:     50,
			EnableAutoCommit:   true,
			AutoCommitInterval: 10 * time.Second,
			IsolationLevel:     "read_uncommitted",
			RebalanceStrategy:  "sticky",
		},
		HealthCheckInterval:  60 * time.Second,
		ClientID:             "kafka-disk-simple-client",
		MetadataRefreshFreq:  10 * time.Minute,
		MetadataRetryMax:     3,
		MetadataRetryBackoff: 250 * time.Millisecond,
		Net: NetConfig{
			DialTimeout:  30 * time.Second,
			ReadTimeout:  30 * time.Second,
			WriteTimeout: 30 * time.Second,
			KeepAlive:    30 * time.Second,
			MaxIdleConns: 10,
			MaxOpenConns: 100,
		},
	}

	eventBus, err := NewKafkaEventBus(kafkaConfig)
	require.NoError(t, err, "Failed to create Kafka disk EventBus")
	defer eventBus.Close()

	// ç­‰å¾…è¿æ¥å»ºç«‹
	time.Sleep(5 * time.Second)

	t.Logf("ğŸŸ  Starting Kafka Disk Low Pressure Simple Test")

	// ä½å‹åŠ›ï¼š2ä¸ªtopicï¼Œæ¯ä¸ª300æ¡æ¶ˆæ¯ï¼Œæ€»è®¡600æ¡
	runSimpleDiskTest(t, eventBus, "Kafka-Disk", 2, 300, metrics)
	analyzeSimpleDiskResults(t, "Kafka-Disk", metrics)
}

// TestNATSDiskMediumPressureSimple NATSç£ç›˜å­˜å‚¨ä¸­å‹åŠ›ç®€å•æµ‹è¯•
func TestNATSDiskMediumPressureSimple(t *testing.T) {
	if testing.Short() {
		t.Skip("Skipping NATS disk medium pressure simple test in short mode")
	}

	metrics := &SimpleDiskMetrics{StartTime: time.Now()}

	natsConfig := &NATSConfig{
		URLs:                []string{"nats://localhost:4223"},
		ClientID:            "nats-disk-medium-simple-client",
		MaxReconnects:       10,
		ReconnectWait:       1 * time.Second,
		ConnectionTimeout:   30 * time.Second,
		HealthCheckInterval: 60 * time.Second,
		JetStream: JetStreamConfig{
			Enabled: true,
			Stream: StreamConfig{
				Name:      "MEDIUM_DISK_STREAM",
				Subjects:  []string{"medium.disk.>"},
				Storage:   "file", // ç£ç›˜å­˜å‚¨
				Retention: "limits",
				MaxAge:    24 * time.Hour,
				MaxBytes:  100 * 1024 * 1024, // 100MB
				MaxMsgs:   100000,
				Replicas:  1,
				Discard:   "old",
			},
			Consumer: NATSConsumerConfig{
				DurableName:   "medium-disk-consumer",
				DeliverPolicy: "all",
				AckPolicy:     "explicit",
				ReplayPolicy:  "instant",
				MaxAckPending: 1000,
				MaxWaiting:    512,
				MaxDeliver:    3,
				BackOff:       []time.Duration{1 * time.Second, 2 * time.Second},
			},
			PublishTimeout: 15 * time.Second,
		},
	}

	eventBus, err := NewNATSEventBus(natsConfig)
	require.NoError(t, err, "Failed to create NATS disk EventBus")
	defer eventBus.Close()

	time.Sleep(5 * time.Second)

	t.Logf("ğŸ”µ Starting NATS Disk Medium Pressure Simple Test")

	// ä¸­å‹åŠ›ï¼š3ä¸ªtopicï¼Œæ¯ä¸ª1000æ¡æ¶ˆæ¯ï¼Œæ€»è®¡3,000æ¡
	runSimpleDiskTest(t, eventBus, "NATS-Disk", 3, 1000, metrics)
	analyzeSimpleDiskResults(t, "NATS-Disk", metrics)
}

// TestKafkaDiskMediumPressureSimple Kafkaç£ç›˜å­˜å‚¨ä¸­å‹åŠ›ç®€å•æµ‹è¯•
func TestKafkaDiskMediumPressureSimple(t *testing.T) {
	if testing.Short() {
		t.Skip("Skipping Kafka disk medium pressure simple test in short mode")
	}

	metrics := &SimpleDiskMetrics{StartTime: time.Now()}

	kafkaConfig := &KafkaConfig{
		Brokers: []string{"localhost:29093"},
		Producer: ProducerConfig{
			RequiredAcks:     1,
			Compression:      "none",
			FlushFrequency:   10 * time.Millisecond,
			FlushMessages:    20,
			Timeout:          60 * time.Second,
			FlushBytes:       128 * 1024,
			RetryMax:         3,
			BatchSize:        8 * 1024,
			BufferSize:       16 * 1024 * 1024,
			Idempotent:       false,
			MaxMessageBytes:  512 * 1024,
			PartitionerType:  "round_robin",
			LingerMs:         5 * time.Millisecond,
			CompressionLevel: 1,
			MaxInFlight:      3,
		},
		Consumer: ConsumerConfig{
			GroupID:            "kafka-disk-medium-simple-group",
			AutoOffsetReset:    "earliest",
			SessionTimeout:     90 * time.Second,
			HeartbeatInterval:  30 * time.Second,
			MaxProcessingTime:  120 * time.Second,
			FetchMinBytes:      1,
			FetchMaxBytes:      10 * 1024 * 1024,
			FetchMaxWait:       2 * time.Second,
			MaxPollRecords:     100,
			EnableAutoCommit:   true,
			AutoCommitInterval: 15 * time.Second,
			IsolationLevel:     "read_uncommitted",
			RebalanceStrategy:  "sticky",
		},
		HealthCheckInterval:  90 * time.Second,
		ClientID:             "kafka-disk-medium-simple-client",
		MetadataRefreshFreq:  15 * time.Minute,
		MetadataRetryMax:     3,
		MetadataRetryBackoff: 500 * time.Millisecond,
		Net: NetConfig{
			DialTimeout:  60 * time.Second,
			ReadTimeout:  60 * time.Second,
			WriteTimeout: 60 * time.Second,
			KeepAlive:    60 * time.Second,
			MaxIdleConns: 15,
			MaxOpenConns: 150,
		},
	}

	eventBus, err := NewKafkaEventBus(kafkaConfig)
	require.NoError(t, err, "Failed to create Kafka disk EventBus")
	defer eventBus.Close()

	time.Sleep(8 * time.Second)

	t.Logf("ğŸŸ  Starting Kafka Disk Medium Pressure Simple Test")

	// ä¸­å‹åŠ›ï¼š3ä¸ªtopicï¼Œæ¯ä¸ª1000æ¡æ¶ˆæ¯ï¼Œæ€»è®¡3,000æ¡
	runSimpleDiskTest(t, eventBus, "Kafka-Disk", 3, 1000, metrics)
	analyzeSimpleDiskResults(t, "Kafka-Disk", metrics)
}

// runSimpleDiskTest è¿è¡Œç®€å•ç£ç›˜æµ‹è¯•
func runSimpleDiskTest(t *testing.T, eventBus EventBus, system string, topicCount, messagesPerTopic int, metrics *SimpleDiskMetrics) {
	ctx, cancel := context.WithTimeout(context.Background(), 180*time.Second) // 3åˆ†é’Ÿè¶…æ—¶
	defer cancel()

	// ç”Ÿæˆtopicåˆ—è¡¨
	topics := make([]string, topicCount)
	for i := 0; i < topicCount; i++ {
		topics[i] = fmt.Sprintf("simple.disk.topic.%d", i+1)
	}

	totalMessages := topicCount * messagesPerTopic
	t.Logf("ğŸ“Š %s Simple Disk Test Config: %d topics, %d msgs/topic, total: %d messages",
		system, topicCount, messagesPerTopic, totalMessages)

	// è®¾ç½®æ¶ˆæ¯å¤„ç†å™¨
	var wg sync.WaitGroup
	setupSimpleDiskHandlers(t, eventBus, topics, metrics, &wg)

	// ç­‰å¾…è®¢é˜…å»ºç«‹
	t.Logf("â³ Waiting for %s subscriptions to stabilize...", system)
	time.Sleep(3 * time.Second)

	// å¼€å§‹å‘é€æ¶ˆæ¯
	metrics.StartTime = time.Now()
	sendSimpleDiskMessages(t, eventBus, topics, messagesPerTopic, metrics)

	// ç­‰å¾…æ¶ˆæ¯å¤„ç†å®Œæˆ
	done := make(chan struct{})
	go func() {
		wg.Wait()
		close(done)
	}()

	select {
	case <-done:
		t.Logf("âœ… All %s simple disk messages processed successfully", system)
	case <-time.After(120 * time.Second): // 2åˆ†é’Ÿç­‰å¾…
		t.Logf("â° %s simple disk test timeout reached", system)
	case <-ctx.Done():
		t.Logf("ğŸ›‘ %s simple disk context cancelled", system)
	}

	metrics.EndTime = time.Now()
}

// setupSimpleDiskHandlers è®¾ç½®ç®€å•ç£ç›˜å¤„ç†å™¨
func setupSimpleDiskHandlers(t *testing.T, eventBus EventBus, topics []string, metrics *SimpleDiskMetrics, wg *sync.WaitGroup) {
	for _, topic := range topics {
		wg.Add(1)
		go func(topicName string) {
			defer wg.Done()

			var lastSequence int64 = -1

			handler := func(ctx context.Context, message []byte) error {
				// è§£ææ¶ˆæ¯
				var testMsg SimpleDiskTestMessage
				if err := json.Unmarshal(message, &testMsg); err != nil {
					atomic.AddInt64(&metrics.ProcessErrors, 1)
					return err
				}

				// æ£€æµ‹é¡ºåºè¿å
				if lastSequence >= 0 && testMsg.Sequence <= lastSequence {
					atomic.AddInt64(&metrics.OrderViolations, 1)
				}
				lastSequence = testMsg.Sequence

				// æ›´æ–°æ¥æ”¶è®¡æ•°
				atomic.AddInt64(&metrics.MessagesReceived, 1)

				return nil
			}

			err := eventBus.Subscribe(context.Background(), topicName, handler)
			if err != nil {
				t.Errorf("Failed to subscribe to topic %s: %v", topicName, err)
			}
		}(topic)
	}
}

// sendSimpleDiskMessages å‘é€ç®€å•ç£ç›˜æ¶ˆæ¯
func sendSimpleDiskMessages(t *testing.T, eventBus EventBus, topics []string, messagesPerTopic int, metrics *SimpleDiskMetrics) {
	var sendWg sync.WaitGroup

	for _, topic := range topics {
		sendWg.Add(1)
		go func(topicName string) {
			defer sendWg.Done()

			for i := 0; i < messagesPerTopic; i++ {
				testMsg := SimpleDiskTestMessage{
					ID:        fmt.Sprintf("%s-%d", topicName, i),
					Topic:     topicName,
					Sequence:  int64(i),
					Timestamp: time.Now(),
					Data:      fmt.Sprintf("simple-disk-data-%d", i),
				}

				messageBytes, err := json.Marshal(testMsg)
				if err != nil {
					atomic.AddInt64(&metrics.SendErrors, 1)
					continue
				}

				startTime := time.Now()
				err = eventBus.Publish(context.Background(), topicName, messageBytes)
				sendTime := time.Since(startTime).Microseconds()

				if err != nil {
					atomic.AddInt64(&metrics.SendErrors, 1)
				} else {
					atomic.AddInt64(&metrics.MessagesSent, 1)
					atomic.AddInt64(&metrics.SendLatencySum, sendTime)
					atomic.AddInt64(&metrics.SendLatencyCount, 1)
				}

				// æ§åˆ¶å‘é€é€Ÿç‡ï¼šæ¯ç§’çº¦200æ¡æ¶ˆæ¯
				time.Sleep(5 * time.Millisecond)
			}
		}(topic)
	}

	sendWg.Wait()
	t.Logf("ğŸ“¤ Finished sending simple disk messages")
}

// analyzeSimpleDiskResults åˆ†æç®€å•ç£ç›˜ç»“æœ
func analyzeSimpleDiskResults(t *testing.T, system string, metrics *SimpleDiskMetrics) {
	duration := metrics.EndTime.Sub(metrics.StartTime)
	successRate := float64(metrics.MessagesReceived) / float64(metrics.MessagesSent) * 100
	throughput := float64(metrics.MessagesReceived) / duration.Seconds()

	avgSendLatency := float64(0)
	if metrics.SendLatencyCount > 0 {
		avgSendLatency = float64(metrics.SendLatencySum) / float64(metrics.SendLatencyCount) / 1000.0 // ms
	}

	t.Logf("\nğŸ¯ ===== %s Simple Disk Test Results =====", system)
	t.Logf("â±ï¸  Test Duration: %v", duration)
	t.Logf("ğŸ“¤ Messages Sent: %d", metrics.MessagesSent)
	t.Logf("ğŸ“¥ Messages Received: %d", metrics.MessagesReceived)
	t.Logf("âŒ Send Errors: %d", metrics.SendErrors)
	t.Logf("âŒ Process Errors: %d", metrics.ProcessErrors)
	t.Logf("âœ… Success Rate: %.2f%%", successRate)
	t.Logf("ğŸš€ Throughput: %.2f msg/s", throughput)
	t.Logf("âš¡ Avg Send Latency: %.3f ms", avgSendLatency)
	t.Logf("âš ï¸ Order Violations: %d", metrics.OrderViolations)

	// ğŸ† æ€§èƒ½è¯„ä¼°
	t.Logf("\nğŸ† %s Simple Disk Performance Evaluation:", system)
	if successRate >= 95.0 {
		t.Logf("ğŸ‰ ä¼˜ç§€! %sç£ç›˜å­˜å‚¨è¡¨ç°å“è¶Š!", system)
		t.Logf("   âœ… æˆåŠŸç‡: %.2f%%", successRate)
		t.Logf("   âœ… ååé‡: %.0f msg/s", throughput)
		t.Logf("   âœ… å»¶è¿Ÿ: %.3f ms", avgSendLatency)
	} else if successRate >= 80.0 {
		t.Logf("âš ï¸ è‰¯å¥½! %sç£ç›˜å­˜å‚¨è¡¨ç°è‰¯å¥½", system)
		t.Logf("   âœ… æˆåŠŸç‡: %.2f%%", successRate)
	} else {
		t.Logf("âŒ %sç£ç›˜å­˜å‚¨éœ€è¦ä¼˜åŒ–ï¼ŒæˆåŠŸç‡ä»…ä¸º %.2f%%", system, successRate)
	}

	t.Logf("âœ… %s Simple Disk Test Completed!", system)
}
