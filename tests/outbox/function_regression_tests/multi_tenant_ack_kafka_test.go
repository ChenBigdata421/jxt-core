package function_regression_tests

import (
	"context"
	"fmt"
	"sync"
	"testing"
	"time"

	"github.com/ChenBigdata421/jxt-core/sdk/pkg/eventbus"
	"github.com/ChenBigdata421/jxt-core/sdk/pkg/logger"
	"github.com/ChenBigdata421/jxt-core/sdk/pkg/outbox"
	outboxadapters "github.com/ChenBigdata421/jxt-core/sdk/pkg/outbox/adapters"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
)

// TestMultiTenantACKChannel_Kafka æµ‹è¯•å¤šç§Ÿæˆ· ACK Channelï¼ˆKafkaï¼‰
// å®Œæ•´ç‰ˆæœ¬ï¼šæµ‹è¯•äº‹ä»¶å‘å¸ƒå’Œ ACK æ¥æ”¶
func TestMultiTenantACKChannel_Kafka(t *testing.T) {
	// åˆå§‹åŒ– logger
	logger.Setup()

	// åˆ›å»º Kafka EventBus
	clientID := fmt.Sprintf("kafka-multi-tenant-%d", time.Now().UnixNano())

	cfg := &eventbus.KafkaConfig{
		Brokers: []string{"localhost:29094"},
		Producer: eventbus.ProducerConfig{
			RequiredAcks:    1,
			FlushFrequency:  10 * time.Millisecond,
			FlushMessages:   100,
			Timeout:         5 * time.Second,
			FlushBytes:      1048576,
			RetryMax:        3,
			BatchSize:       16384,
			BufferSize:      8388608,
			MaxMessageBytes: 1048576,
		},
		Consumer: eventbus.ConsumerConfig{
			GroupID:            fmt.Sprintf("test-group-%s", clientID),
			SessionTimeout:     10 * time.Second,
			HeartbeatInterval:  3 * time.Second,
			MaxProcessingTime:  5 * time.Second,
			AutoOffsetReset:    "earliest",
			EnableAutoCommit:   true,
			AutoCommitInterval: 1 * time.Second,
			FetchMaxWait:       100 * time.Millisecond,
			MaxPollRecords:     500,
		},
		Net: eventbus.NetConfig{
			DialTimeout:  10 * time.Second,
			ReadTimeout:  10 * time.Second,
			WriteTimeout: 10 * time.Second,
		},
		ClientID: clientID,
	}

	kafkaEventBus, err := eventbus.NewKafkaEventBus(cfg)
	require.NoError(t, err, "Failed to create Kafka EventBus")
	defer kafkaEventBus.Close()

	// åˆ›å»º EventBusAdapter
	adapter := outboxadapters.NewEventBusAdapter(kafkaEventBus)
	defer adapter.Close()

	// å®šä¹‰ç§Ÿæˆ·åˆ—è¡¨ï¼š10ä¸ªç§Ÿæˆ·
	tenants := []string{
		"tenant-kafka-01",
		"tenant-kafka-02",
		"tenant-kafka-03",
		"tenant-kafka-04",
		"tenant-kafka-05",
		"tenant-kafka-06",
		"tenant-kafka-07",
		"tenant-kafka-08",
		"tenant-kafka-09",
		"tenant-kafka-10",
	}

	// ä¸ºæ¯ä¸ªç§Ÿæˆ·æ³¨å†Œ ACK Channel
	for _, tenantID := range tenants {
		err := adapter.RegisterTenant(tenantID, 1000)
		require.NoError(t, err, "Failed to register tenant %s", tenantID)
		t.Logf("âœ… Registered tenant: %s", tenantID)
	}

	// éªŒè¯ç§Ÿæˆ·å·²æ³¨å†Œ
	registeredTenants := adapter.GetRegisteredTenants()
	assert.Equal(t, len(tenants), len(registeredTenants), "Should have registered all tenants")
	t.Logf("âœ… Registered tenants: %v", registeredTenants)

	// ä¸ºæ¯ä¸ªç§Ÿæˆ·åˆ›å»º Repository å’Œ Publisher
	type TenantContext struct {
		tenantID   string
		repo       *MockRepository
		publisher  *outbox.OutboxPublisher
		ackChan    <-chan *outbox.PublishResult
		receivedMu sync.Mutex
		received   []*outbox.PublishResult
	}

	tenantContexts := make(map[string]*TenantContext)

	// ä¸ºæ¯ä¸ªç§Ÿæˆ·é…ç½® Topic
	topicNames := make(map[string]string)
	for _, tenantID := range tenants {
		topicName := fmt.Sprintf("tenant-%s-events-%d", tenantID, time.Now().UnixNano())
		topicNames[tenantID] = topicName
		t.Logf("âœ… Will use topic: %s for tenant: %s", topicName, tenantID)
	}

	for _, tenantID := range tenants {
		// åˆ›å»º Repository
		repo := NewMockRepository()

		// åˆ›å»º TopicMapperï¼ˆä½¿ç”¨é™æ€ Topicï¼‰
		topicMapper := outbox.NewStaticTopicMapper(topicNames[tenantID])

		// åˆ›å»º Publisher
		publisherCfg := &outbox.PublisherConfig{
			MaxRetries: 3,
			RetryDelay: 100 * time.Millisecond,
		}
		publisher := outbox.NewOutboxPublisher(repo, adapter, topicMapper, publisherCfg)

		// è·å–ç§Ÿæˆ·ä¸“å±çš„ ACK Channel
		ackChan := adapter.GetTenantPublishResultChannel(tenantID)
		require.NotNil(t, ackChan, "ACK channel should not be nil for tenant %s", tenantID)

		tenantContexts[tenantID] = &TenantContext{
			tenantID:  tenantID,
			repo:      repo,
			publisher: publisher,
			ackChan:   ackChan,
			received:  make([]*outbox.PublishResult, 0),
		}

		t.Logf("âœ… Created context for tenant: %s", tenantID)
	}

	// ä¸ºæ¯ä¸ªç§Ÿæˆ·åˆ›å»ºäº‹ä»¶ï¼šæ¯ä¸ªç§Ÿæˆ·500ä¸ªäº‹ä»¶
	eventsPerTenant := 500
	for _, tenantID := range tenants {
		tenantCtx := tenantContexts[tenantID]

		for i := 0; i < eventsPerTenant; i++ {
			event := &outbox.OutboxEvent{
				ID:            fmt.Sprintf("event-%s-%d", tenantID, i),
				AggregateID:   fmt.Sprintf("agg-%s-%d", tenantID, i),
				AggregateType: "TestAggregate",
				EventType:     "TestEvent",
				Payload:       []byte(fmt.Sprintf(`{"tenant":"%s","index":%d}`, tenantID, i)),
				Status:        outbox.EventStatusPending,
				TenantID:      tenantID,
			}
			err := tenantCtx.repo.Save(context.Background(), event)
			require.NoError(t, err, "Failed to save event for tenant %s", tenantID)
		}
		t.Logf("âœ… Created %d events for tenant: %s", eventsPerTenant, tenantID)
	}

	// å¯åŠ¨æ‰€æœ‰ç§Ÿæˆ·çš„ ACK ç›‘å¬å™¨
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	for _, tenantID := range tenants {
		tenantCtx := tenantContexts[tenantID]

		// ä½¿ç”¨ç§Ÿæˆ·ä¸“å±çš„ ACK Channel å¯åŠ¨ ACK ç›‘å¬å™¨
		// Publisher çš„ ACK ç›‘å¬å™¨ä¼šè‡ªåŠ¨è°ƒç”¨ repo.MarkAsPublished()
		tenantCtx.publisher.StartACKListenerWithChannel(ctx, tenantCtx.ackChan)

		t.Logf("âœ… Started ACK listener for tenant: %s", tenantID)
	}

	// ä¸ºæ¯ä¸ªç§Ÿæˆ·å‘å¸ƒäº‹ä»¶
	for _, tenantID := range tenants {
		tenantCtx := tenantContexts[tenantID]

		// è·å–å¾…å‘å¸ƒçš„äº‹ä»¶
		events, err := tenantCtx.repo.FindPendingEvents(ctx, eventsPerTenant, tenantID)
		require.NoError(t, err, "Failed to find pending events for tenant %s", tenantID)
		require.Equal(t, eventsPerTenant, len(events), "Should have %d pending events for tenant %s", eventsPerTenant, tenantID)

		// å‘å¸ƒäº‹ä»¶
		for _, event := range events {
			err := tenantCtx.publisher.PublishEvent(ctx, event)
			require.NoError(t, err, "Failed to publish event for tenant %s", tenantID)
		}

		t.Logf("âœ… Published %d events for tenant: %s", len(events), tenantID)
	}

	// ç­‰å¾…æ‰€æœ‰ ACK è¢«æ¥æ”¶ï¼ˆKafka å¼‚æ­¥å‘å¸ƒéœ€è¦æ—¶é—´ï¼‰
	// Publisher çš„ ACK ç›‘å¬å™¨ä¼šè°ƒç”¨ repo.MarkAsPublished()
	// æˆ‘ä»¬é€šè¿‡æ£€æŸ¥ Repository ä¸­çš„äº‹ä»¶çŠ¶æ€æ¥éªŒè¯
	t.Log("â³ Waiting for all events to be marked as Published...")
	t.Logf("ğŸ“Š Total events to publish: %d (10 tenants Ã— 500 events)", len(tenants)*eventsPerTenant)
	maxWaitTime := 120 * time.Second // å¢åŠ åˆ°120ç§’ï¼Œå› ä¸ºæœ‰5000ä¸ªäº‹ä»¶
	checkInterval := 1 * time.Second
	startTime := time.Now()

	allPublished := false
	lastLogTime := startTime
	for time.Since(startTime) < maxWaitTime {
		allPublished = true
		totalPublished := 0

		for _, tenantID := range tenants {
			tenantCtx := tenantContexts[tenantID]

			// ç»Ÿè®¡å·²å‘å¸ƒçš„äº‹ä»¶æ•°é‡
			publishedCount := 0
			for i := 0; i < eventsPerTenant; i++ {
				eventID := fmt.Sprintf("event-%s-%d", tenantID, i)
				event, err := tenantCtx.repo.FindByID(ctx, eventID)
				if err == nil && event != nil && event.IsPublished() {
					publishedCount++
				}
			}

			totalPublished += publishedCount

			if publishedCount < eventsPerTenant {
				allPublished = false
			}
		}

		// æ¯10ç§’æ‰“å°ä¸€æ¬¡è¿›åº¦ï¼ˆå› ä¸ºäº‹ä»¶æ•°é‡å¤šï¼Œå‡å°‘æ—¥å¿—é¢‘ç‡ï¼‰
		if time.Since(lastLogTime) >= 10*time.Second {
			percentage := float64(totalPublished) / float64(len(tenants)*eventsPerTenant) * 100
			rate := float64(totalPublished) / time.Since(startTime).Seconds()
			t.Logf("â³ Progress: %d/%d events published (%.1f%%), rate: %.0f events/s, elapsed: %v",
				totalPublished, len(tenants)*eventsPerTenant,
				percentage, rate,
				time.Since(startTime))
			lastLogTime = time.Now()
		}

		if allPublished {
			t.Logf("âœ… All events published in %v", time.Since(startTime))
			break
		}

		time.Sleep(checkInterval)
	}

	if !allPublished {
		t.Logf("âš ï¸  Timeout after %v, checking partial results...", maxWaitTime)
	}

	// éªŒè¯æ¯ä¸ªç§Ÿæˆ·çš„äº‹ä»¶å‘å¸ƒçŠ¶æ€
	for _, tenantID := range tenants {
		tenantCtx := tenantContexts[tenantID]

		// ç»Ÿè®¡å·²å‘å¸ƒçš„äº‹ä»¶
		publishedCount := 0
		publishedEvents := make(map[string]bool)

		for i := 0; i < eventsPerTenant; i++ {
			eventID := fmt.Sprintf("event-%s-%d", tenantID, i)
			event, err := tenantCtx.repo.FindByID(ctx, eventID)
			if err == nil && event != nil && event.IsPublished() {
				publishedCount++
				publishedEvents[eventID] = true
			}
		}

		// éªŒè¯ï¼šæ¯ä¸ªç§Ÿæˆ·åº”è¯¥æœ‰æ‰€æœ‰50ä¸ªäº‹ä»¶è¢«æ ‡è®°ä¸ºPublished
		// ç”±äºKafkaå¼‚æ­¥å‘å¸ƒçš„ç‰¹æ€§ï¼Œå…è®¸å°‘é‡ä¸¢å¤±ï¼Œä½†è‡³å°‘è¦æœ‰80%
		minExpectedPublished := int(float64(eventsPerTenant) * 0.8)
		assert.GreaterOrEqual(t, publishedCount, minExpectedPublished,
			"Tenant %s should have at least %d events published (80%%), but got %d",
			tenantID, minExpectedPublished, publishedCount)

		if publishedCount == eventsPerTenant {
			t.Logf("âœ… Tenant %s: all %d/%d events published (100%%)", tenantID, publishedCount, eventsPerTenant)
		} else {
			t.Logf("âš ï¸  Tenant %s: %d/%d events published (%.1f%%)",
				tenantID, publishedCount, eventsPerTenant,
				float64(publishedCount)/float64(eventsPerTenant)*100)
		}
	}

	// æ¸…ç†ï¼šæ³¨é”€æ‰€æœ‰ç§Ÿæˆ·
	for _, tenantID := range tenants {
		err := adapter.UnregisterTenant(tenantID)
		require.NoError(t, err, "Failed to unregister tenant %s", tenantID)
		t.Logf("âœ… Unregistered tenant: %s", tenantID)
	}

	t.Log("âœ… Multi-tenant ACK Channel test with Kafka passed!")
}
