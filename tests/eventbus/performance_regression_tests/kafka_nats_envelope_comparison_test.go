package performance_tests

import (
	"context"
	"fmt"
	"hash/fnv"
	"math"
	"runtime"
	"strings"
	"sync"
	"sync/atomic"
	"testing"
	"time"

	"github.com/ChenBigdata421/jxt-core/sdk/pkg/eventbus"
	jxtjson "github.com/ChenBigdata421/jxt-core/sdk/pkg/json"
	"github.com/ChenBigdata421/jxt-core/sdk/pkg/logger"
	"github.com/IBM/sarama"
	"github.com/nats-io/nats.go"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
	"go.uber.org/zap"
)

// ğŸ¯ Kafka vs NATS JetStream å…¨é¢æ€§èƒ½å¯¹æ¯”æµ‹è¯•
//
// æµ‹è¯•è¦æ±‚ï¼š
// 1. åˆ›å»º EventBus å®ä¾‹ï¼ˆè¦†ç›– Kafka å’Œ NATS JetStream ä¸¤ç§å®ç°ï¼‰
// 2. NATS JetStream å¿…é¡»æŒä¹…åŒ–åˆ°ç£ç›˜
// 3. å‘å¸ƒç«¯å¿…é¡»é‡‡ç”¨ PublishEnvelope æ–¹æ³•
// 4. è®¢é˜…ç«¯å¿…é¡»é‡‡ç”¨ SubscribeEnvelope æ–¹æ³•
// 5. æµ‹è¯•è¦†ç›–ä½å‹500ã€ä¸­å‹2000ã€é«˜å‹5000ã€æé™10000
// 6. Topic æ•°é‡ä¸º 5
// 7. è¾“å‡ºæŠ¥å‘ŠåŒ…æ‹¬æ€§èƒ½æŒ‡æ ‡ã€å…³é”®èµ„æºå ç”¨æƒ…å†µã€è¿æ¥æ•°ã€æ¶ˆè´¹è€…ç»„ä¸ªæ•°å¯¹æ¯”
// 8. Kafka çš„ ClientID å’Œ topic åç§°åªä½¿ç”¨ ASCII å­—ç¬¦
// 9. å¤„ç†å»¶è¿Ÿæµ‹é‡ï¼šç«¯åˆ°ç«¯å»¶è¿Ÿï¼ˆä» Envelope.Timestamp å‘é€æ—¶é—´åˆ°æ¥æ”¶æ—¶é—´ï¼‰

// OrderChecker é«˜æ€§èƒ½é¡ºåºæ£€æŸ¥å™¨ï¼ˆä½¿ç”¨åˆ†ç‰‡é”å‡å°‘ç«äº‰ï¼‰
type OrderChecker struct {
	shards     [256]*orderShard // 256 ä¸ªåˆ†ç‰‡ï¼Œå‡å°‘é”ç«äº‰
	violations int64            // é¡ºåºè¿åè®¡æ•°ï¼ˆåŸå­æ“ä½œï¼‰
}

// orderShard å•ä¸ªåˆ†ç‰‡
type orderShard struct {
	mu        sync.Mutex
	sequences map[string]int64
}

// NewOrderChecker åˆ›å»ºé¡ºåºæ£€æŸ¥å™¨
func NewOrderChecker() *OrderChecker {
	oc := &OrderChecker{}
	for i := 0; i < 256; i++ {
		oc.shards[i] = &orderShard{
			sequences: make(map[string]int64),
		}
	}
	return oc
}

// Check æ£€æŸ¥é¡ºåºï¼ˆçº¿ç¨‹å®‰å…¨ï¼Œä½¿ç”¨åˆ†ç‰‡é”ï¼‰
func (oc *OrderChecker) Check(aggregateID string, version int64) bool {
	// ä½¿ç”¨ FNV-1a hash é€‰æ‹©åˆ†ç‰‡ï¼ˆä¸ Keyed-Worker Pool ç›¸åŒçš„ç®—æ³•ï¼‰
	h := fnv.New32a()
	h.Write([]byte(aggregateID))
	shardIndex := h.Sum32() % 256

	shard := oc.shards[shardIndex]
	shard.mu.Lock()
	defer shard.mu.Unlock()

	lastSeq, exists := shard.sequences[aggregateID]
	if exists && version <= lastSeq {
		atomic.AddInt64(&oc.violations, 1)
		return false // é¡ºåºè¿å
	}

	shard.sequences[aggregateID] = version
	return true // é¡ºåºæ­£ç¡®
}

// GetViolations è·å–é¡ºåºè¿åæ¬¡æ•°
func (oc *OrderChecker) GetViolations() int64 {
	return atomic.LoadInt64(&oc.violations)
}

// PerfMetrics æ€§èƒ½æŒ‡æ ‡
type PerfMetrics struct {
	// åŸºæœ¬ä¿¡æ¯
	System       string        // ç³»ç»Ÿåç§° (Kafka/NATS)
	Pressure     string        // å‹åŠ›çº§åˆ«
	MessageCount int           // æ¶ˆæ¯æ€»æ•°
	StartTime    time.Time     // å¼€å§‹æ—¶é—´
	EndTime      time.Time     // ç»“æŸæ—¶é—´
	Duration     time.Duration // æŒç»­æ—¶é—´

	// æ¶ˆæ¯æŒ‡æ ‡
	MessagesSent     int64   // å‘é€æ¶ˆæ¯æ•°
	MessagesReceived int64   // æ¥æ”¶æ¶ˆæ¯æ•°
	SendErrors       int64   // å‘é€é”™è¯¯æ•°
	ProcessErrors    int64   // å¤„ç†é”™è¯¯æ•°
	SuccessRate      float64 // æˆåŠŸç‡

	// æ€§èƒ½æŒ‡æ ‡
	SendThroughput    float64 // å‘é€ååé‡ (msg/s)
	ReceiveThroughput float64 // æ¥æ”¶ååé‡ (msg/s)
	AvgSendLatency    float64 // å¹³å‡å‘é€å»¶è¿Ÿ (ms) - PublishEnvelope æ–¹æ³•æ‰§è¡Œæ—¶é—´
	AvgProcessLatency float64 // å¹³å‡å¤„ç†å»¶è¿Ÿ (ms) - ç«¯åˆ°ç«¯å»¶è¿Ÿï¼ˆEnvelope.Timestamp â†’ æ¥æ”¶æ—¶é—´ï¼‰

	// èµ„æºå ç”¨
	InitialGoroutines int     // åˆå§‹åç¨‹æ•°
	PeakGoroutines    int32   // å³°å€¼åç¨‹æ•° (ä½¿ç”¨ int32 ä»¥ä¾¿åŸå­æ“ä½œ)
	FinalGoroutines   int     // æœ€ç»ˆåç¨‹æ•°
	GoroutineLeak     int     // åç¨‹æ³„æ¼æ•°
	InitialMemoryMB   float64 // åˆå§‹å†…å­˜ (MB)
	PeakMemoryMB      uint64  // å³°å€¼å†…å­˜ (MB) - å­˜å‚¨ä¸º uint64 bits ä»¥ä¾¿åŸå­æ“ä½œ
	FinalMemoryMB     float64 // æœ€ç»ˆå†…å­˜ (MB)
	MemoryDeltaMB     float64 // å†…å­˜å¢é‡ (MB)

	// è¿æ¥å’Œæ¶ˆè´¹è€…ç»„ç»Ÿè®¡ï¼ˆæ–°å¢ï¼‰
	TopicCount         int              // Topic æ•°é‡
	ConnectionCount    int              // è¿æ¥æ•°
	ConsumerGroupCount int              // æ¶ˆè´¹è€…ç»„ä¸ªæ•°
	TopicList          []string         // Topic åˆ—è¡¨
	PartitionCount     map[string]int32 // Kafka Topic åˆ†åŒºæ•°

	// é¡ºåºæ€§æŒ‡æ ‡
	OrderViolations int64 // é¡ºåºè¿åæ¬¡æ•°

	// å†…éƒ¨ç»Ÿè®¡
	sendLatencySum   int64 // å‘é€å»¶è¿Ÿæ€»å’Œ (å¾®ç§’)
	sendLatencyCount int64 // å‘é€å»¶è¿Ÿè®¡æ•°
	procLatencySum   int64 // å¤„ç†å»¶è¿Ÿæ€»å’Œ (å¾®ç§’)
	procLatencyCount int64 // å¤„ç†å»¶è¿Ÿè®¡æ•°
}

// createKafkaTopicsWithBuilder ä½¿ç”¨ TopicBuilder åˆ›å»º Kafka Topics
// ğŸ”¥ é‡æ„åï¼šå‹ç¼©é…ç½®åœ¨ topic çº§åˆ«ï¼Œä¸å†ä½¿ç”¨ Producer çº§åˆ«çš„å‹ç¼©é…ç½®
func createKafkaTopicsWithBuilder(ctx context.Context, t *testing.T, bus eventbus.EventBus, topics []string, partitions int, replication int) map[string]int32 {
	t.Logf("ğŸ”§ ä½¿ç”¨ TopicBuilder åˆ›å»º Kafka Topics (åˆ†åŒºæ•°: %d, å‰¯æœ¬æ•°: %d)...", partitions, replication)
	t.Logf("   ğŸ“¦ å‹ç¼©æ¨¡å¼: Topic çº§åˆ«ï¼ˆæ¯ä¸ª topic ç‹¬ç«‹é…ç½®ï¼‰")

	// è®°å½•å®é™…åˆ›å»ºçš„åˆ†åŒºæ•°
	actualPartitions := make(map[string]int32)

	// ä½¿ç”¨ TopicBuilder ä¸ºæ¯ä¸ª topic åˆ›å»ºé…ç½®
	for _, topicName := range topics {
		// ğŸ”¥ é‡æ„åï¼šä½¿ç”¨ Builder æ¨¡å¼åˆ›å»º topicï¼Œå‹ç¼©é…ç½®åœ¨ topic çº§åˆ«
		// é…ç½®ï¼šæŒ‡å®šåˆ†åŒºæ•°ã€å‰¯æœ¬æ•°ã€Snappy å‹ç¼©ã€æŒä¹…åŒ–æ¨¡å¼
		err := eventbus.NewTopicBuilder(topicName).
			WithPartitions(partitions).    // åˆ†åŒºæ•°
			WithReplication(replication).  // å‰¯æœ¬æ•°
			SnappyCompression().           // âœ… Topic çº§åˆ«å‹ç¼©ï¼ˆSnappyï¼Œå¹³è¡¡æ€§èƒ½ï¼‰
			Persistent().                  // âœ… æŒä¹…åŒ–æ¨¡å¼
			WithRetention(7*24*time.Hour). // ä¿ç•™ 7 å¤©
			WithMaxSize(1*1024*1024*1024). // 1GB
			WithDescription(fmt.Sprintf("Performance test topic with %d partitions and snappy compression", partitions)).
			Build(ctx, bus)

		if err != nil {
			t.Logf("   âŒ åˆ›å»ºå¤±è´¥: %s - %v", topicName, err)
		} else {
			actualPartitions[topicName] = int32(partitions)
			t.Logf("   âœ… åˆ›å»ºæˆåŠŸ: %s (%d partitions, snappy compression, persistent)", topicName, partitions)
		}
	}

	// ç­‰å¾… topic åˆ›å»ºå®Œæˆ
	time.Sleep(500 * time.Millisecond)

	// éªŒè¯åˆ›å»ºçš„ topics
	t.Logf("ğŸ“Š éªŒè¯åˆ›å»ºçš„ Topics:")
	for _, topicName := range topics {
		config, err := bus.GetTopicConfig(topicName)
		if err != nil {
			t.Logf("   âš ï¸  æ— æ³•è·å– topic %s é…ç½®: %v", topicName, err)
		} else {
			actualPartitions[topicName] = int32(config.Partitions)
			t.Logf("   %s: partitions=%d, compression=%s, persistence=%s",
				topicName, config.Partitions, config.Compression, config.PersistenceMode)
		}
	}

	t.Logf("âœ… æˆåŠŸåˆ›å»º %d ä¸ª Kafka topics (Topic çº§åˆ«å‹ç¼©é…ç½®)", len(actualPartitions))
	return actualPartitions
}

// createNATSTopicsWithPersistence å·²åºŸå¼ƒ
// â­ Stream é¢„å»ºç«‹æ–¹å¼ï¼šåœ¨ NATSConfig ä¸­é…ç½®ç»Ÿä¸€çš„ Streamï¼ˆsubject pattern ä½¿ç”¨é€šé…ç¬¦ï¼‰
// è¿™æ ·å¯ä»¥é¿å…ä¸ºæ¯ä¸ª topic åˆ›å»ºå•ç‹¬çš„ Streamï¼Œé˜²æ­¢ subject é‡å é”™è¯¯
// å‚è€ƒï¼šjxt-core/sdk/pkg/eventbus/README.md - Stream é¢„å»ºç«‹ä¼˜åŒ–

// cleanupKafka æ¸…ç† Kafka æµ‹è¯•æ•°æ®
func cleanupKafka(t *testing.T, topicPrefix string) {
	t.Logf("ğŸ§¹ æ¸…ç† Kafka æµ‹è¯•æ•°æ® (topic prefix: %s)...", topicPrefix)

	// åˆ›å»º Kafka é…ç½®
	config := sarama.NewConfig()
	config.Version = sarama.V2_6_0_0

	// åˆ›å»º Kafka ç®¡ç†å®¢æˆ·ç«¯
	admin, err := sarama.NewClusterAdmin([]string{"localhost:29094"}, config)
	if err != nil {
		t.Logf("âš ï¸  æ— æ³•åˆ›å»º Kafka ç®¡ç†å®¢æˆ·ç«¯: %v", err)
		return
	}
	defer admin.Close()

	// åˆ—å‡ºæ‰€æœ‰ topics
	topics, err := admin.ListTopics()
	if err != nil {
		t.Logf("âš ï¸  æ— æ³•åˆ—å‡º Kafka topics: %v", err)
		return
	}

	// æ”¶é›†éœ€è¦åˆ é™¤çš„ topics
	var topicsToDelete []string
	for topic := range topics {
		// åªåˆ é™¤ä»¥æŒ‡å®šå‰ç¼€å¼€å¤´çš„ topic
		if len(topicPrefix) == 0 || strings.HasPrefix(topic, topicPrefix) {
			topicsToDelete = append(topicsToDelete, topic)
		}
	}

	// åˆ é™¤ topics
	if len(topicsToDelete) > 0 {
		err = admin.DeleteTopic(topicsToDelete[0]) // Sarama ä¸€æ¬¡åªèƒ½åˆ é™¤ä¸€ä¸ª
		for _, topic := range topicsToDelete {
			err = admin.DeleteTopic(topic)
			if err != nil {
				t.Logf("âš ï¸  åˆ é™¤ topic %s å¤±è´¥: %v", topic, err)
			} else {
				t.Logf("   âœ… å·²åˆ é™¤ topic: %s", topic)
			}
		}
		t.Logf("âœ… æˆåŠŸåˆ é™¤ %d ä¸ª Kafka topics", len(topicsToDelete))
	} else {
		t.Logf("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°éœ€è¦æ¸…ç†çš„ Kafka topics")
	}
}

// cleanupNATS æ¸…ç† NATS JetStream æµ‹è¯•æ•°æ®
func cleanupNATS(t *testing.T, streamPrefix string) {
	t.Logf("ğŸ§¹ æ¸…ç† NATS JetStream æµ‹è¯•æ•°æ® (stream prefix: %s)...", streamPrefix)

	// ç›´æ¥ä½¿ç”¨ NATS å®¢æˆ·ç«¯è¿æ¥
	nc, err := nats.Connect("nats://localhost:4223", nats.Name("nats-cleanup"))
	if err != nil {
		t.Logf("âš ï¸  æ— æ³•è¿æ¥åˆ° NATS: %v", err)
		return
	}
	defer nc.Close()

	// åˆ›å»º JetStream ä¸Šä¸‹æ–‡
	js, err := nc.JetStream()
	if err != nil {
		t.Logf("âš ï¸  æ— æ³•åˆ›å»º JetStream ä¸Šä¸‹æ–‡: %v", err)
		return
	}

	// åˆ—å‡ºæ‰€æœ‰ streams
	streams := js.StreamNames()
	deletedCount := 0

	for streamName := range streams {
		// åªåˆ é™¤ä»¥æŒ‡å®šå‰ç¼€å¼€å¤´çš„ stream
		if len(streamPrefix) == 0 || strings.HasPrefix(streamName, streamPrefix) {
			err := js.DeleteStream(streamName)
			if err != nil {
				t.Logf("âš ï¸  åˆ é™¤ stream %s å¤±è´¥: %v", streamName, err)
			} else {
				t.Logf("   âœ… å·²åˆ é™¤ stream: %s", streamName)
				deletedCount++
			}
		}
	}

	if deletedCount > 0 {
		t.Logf("âœ… æˆåŠŸåˆ é™¤ %d ä¸ª NATS streams", deletedCount)
	} else {
		t.Logf("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°éœ€è¦æ¸…ç†çš„ NATS streams")
	}
}

// cleanupBeforeTest åœ¨æ¯ä¸ªæµ‹è¯•åœºæ™¯å‰æ¸…ç†ç¯å¢ƒ
func cleanupBeforeTest(t *testing.T, scenarioName string) {
	t.Logf("\nğŸ”„ å‡†å¤‡æµ‹è¯•ç¯å¢ƒ: %s", scenarioName)

	// æ¸…ç† Kafka
	cleanupKafka(t, "kafka.perf")

	// æ¸…ç† NATS
	cleanupNATS(t, "PERF_")

	// å¼ºåˆ¶åƒåœ¾å›æ”¶
	runtime.GC()
	time.Sleep(2 * time.Second)

	t.Logf("âœ… æµ‹è¯•ç¯å¢ƒå‡†å¤‡å®Œæˆ\n")
}

// TestKafkaVsNATSPerformanceComparison ä¸»æµ‹è¯•å‡½æ•°
func TestKafkaVsNATSPerformanceComparison(t *testing.T) {
	if testing.Short() {
		t.Skip("Skipping comprehensive comparison test in short mode")
	}

	// åˆå§‹åŒ– loggerï¼ˆå¦‚æœè¿˜æ²¡æœ‰åˆå§‹åŒ–ï¼‰
	if logger.DefaultLogger == nil {
		zapLogger, err := zap.NewDevelopment()
		if err != nil {
			t.Fatalf("Failed to initialize logger: %v", err)
		}
		logger.Logger = zapLogger
		logger.DefaultLogger = zapLogger.Sugar()
	}

	t.Log("ğŸš€ ===== Kafka vs NATS JetStream å…¨é¢æ€§èƒ½å¯¹æ¯”æµ‹è¯• =====")
	t.Log("ğŸ“‹ æµ‹è¯•è¦æ±‚:")
	t.Log("   1. åˆ›å»º EventBus å®ä¾‹ï¼ˆKafka å’Œ NATS JetStreamï¼‰")
	t.Log("   2. NATS JetStream æŒä¹…åŒ–åˆ°ç£ç›˜")
	t.Log("   3. ä½¿ç”¨ PublishEnvelope æ–¹æ³•å‘å¸ƒ")
	t.Log("   4. ä½¿ç”¨ SubscribeEnvelope æ–¹æ³•è®¢é˜…")
	t.Log("   5. æµ‹è¯•å‹åŠ›ï¼šä½å‹500ã€ä¸­å‹2000ã€é«˜å‹5000ã€æé™10000")
	t.Log("   6. Topic æ•°é‡ï¼š5")
	t.Log("   7. ç»Ÿè®¡è¿æ¥æ•°å’Œæ¶ˆè´¹è€…ç»„ä¸ªæ•°")
	t.Log("   8. ä»…ä½¿ç”¨ ASCII å­—ç¬¦å‘½å")
	t.Log("")

	// æµ‹è¯•åœºæ™¯
	scenarios := []struct {
		name     string
		messages int
		timeout  time.Duration
	}{
		{"ä½å‹", 500, 60 * time.Second},
		{"ä¸­å‹", 2000, 120 * time.Second},
		{"é«˜å‹", 5000, 180 * time.Second},
		{"æé™", 10000, 300 * time.Second},
	}

	// å­˜å‚¨æ‰€æœ‰æµ‹è¯•ç»“æœ
	allResults := make(map[string][]*PerfMetrics)
	allResults["Kafka"] = make([]*PerfMetrics, 0)
	allResults["NATS"] = make([]*PerfMetrics, 0)

	// è¿è¡Œæ‰€æœ‰åœºæ™¯æµ‹è¯•
	for _, scenario := range scenarios {
		t.Log("\n" + strings.Repeat("=", 80))
		t.Logf("ğŸ¯ å¼€å§‹ %s æµ‹è¯• (%d æ¡æ¶ˆæ¯)", scenario.name, scenario.messages)
		t.Log(strings.Repeat("=", 80))

		// ğŸ”„ æ¸…ç†æµ‹è¯•ç¯å¢ƒ
		cleanupBeforeTest(t, scenario.name)

		// æµ‹è¯• Kafka
		t.Logf("\nğŸ”´ æµ‹è¯• Kafka...")
		kafkaMetrics := runKafkaTest(t, scenario.name, scenario.messages, scenario.timeout)
		allResults["Kafka"] = append(allResults["Kafka"], kafkaMetrics)

		// æ¸…ç†å’Œç­‰å¾…
		t.Logf("â³ ç­‰å¾…èµ„æºé‡Šæ”¾...")
		time.Sleep(5 * time.Second)
		runtime.GC()

		// æµ‹è¯• NATS JetStream
		t.Logf("\nğŸ”µ æµ‹è¯• NATS JetStream (ç£ç›˜æŒä¹…åŒ–)...")
		natsMetrics := runNATSTest(t, scenario.name, scenario.messages, scenario.timeout)
		allResults["NATS"] = append(allResults["NATS"], natsMetrics)

		// å¯¹æ¯”æœ¬è½®ç»“æœ
		compareRoundResults(t, scenario.name, kafkaMetrics, natsMetrics)

		// æ¸…ç†å’Œç­‰å¾…
		t.Logf("â³ ç­‰å¾…èµ„æºé‡Šæ”¾...")
		time.Sleep(5 * time.Second)
		runtime.GC()
	}

	// ç”Ÿæˆç»¼åˆå¯¹æ¯”æŠ¥å‘Š
	generateComparisonReport(t, allResults)
}

// runKafkaTest è¿è¡Œ Kafka æ€§èƒ½æµ‹è¯•
func runKafkaTest(t *testing.T, pressure string, messageCount int, timeout time.Duration) *PerfMetrics {
	metrics := &PerfMetrics{
		System:       "Kafka",
		Pressure:     pressure,
		MessageCount: messageCount,
	}

	// è®°å½•åˆå§‹èµ„æºçŠ¶æ€
	metrics.InitialGoroutines = runtime.NumGoroutine()
	var m runtime.MemStats
	runtime.ReadMemStats(&m)
	metrics.InitialMemoryMB = float64(m.Alloc) / 1024 / 1024

	// åˆ›å»º Kafka EventBus - å‚è€ƒæˆåŠŸçš„æµ‹è¯•é…ç½®
	// ğŸ”‘ å…³é”®ä¿®å¤ï¼šä¸èƒ½ä½¿ç”¨ä¸­æ–‡å­—ç¬¦ï¼Œä¼šå¯¼è‡´ Kafka æ— æ³•æ¥æ”¶æ¶ˆæ¯
	pressureEn := map[string]string{
		"ä½å‹": "low",
		"ä¸­å‹": "medium",
		"é«˜å‹": "high",
		"æé™": "extreme",
	}[pressure]
	if pressureEn == "" {
		pressureEn = pressure // å¦‚æœä¸æ˜¯ä¸­æ–‡ï¼Œç›´æ¥ä½¿ç”¨
	}

	kafkaConfig := &eventbus.KafkaConfig{
		Brokers:  []string{"localhost:29094"},
		ClientID: fmt.Sprintf("kafka-perf-%s-%d", pressureEn, time.Now().Unix()),
		Producer: eventbus.ProducerConfig{
			RequiredAcks: -1, // WaitForAllï¼Œå¹‚ç­‰æ€§ç”Ÿäº§è€…è¦æ±‚
			// ğŸ”¥ é‡æ„åï¼šå‹ç¼©é…ç½®å·²ä» Producer çº§åˆ«ç§»åˆ° Topic çº§åˆ«
			// ä¸å†åœ¨è¿™é‡Œé…ç½® Compression å’Œ CompressionLevel
			// å‹ç¼©é…ç½®ç°åœ¨é€šè¿‡ TopicBuilder.SnappyCompression() åœ¨ topic çº§åˆ«è®¾ç½®
			FlushFrequency:  10 * time.Millisecond,
			FlushMessages:   100,
			Timeout:         30 * time.Second,
			FlushBytes:      1024 * 1024, // 1MB
			RetryMax:        3,
			BatchSize:       16 * 1024,        // 16KB
			BufferSize:      32 * 1024 * 1024, // 32MB
			Idempotent:      true,
			MaxMessageBytes: 1024 * 1024,
			PartitionerType: "hash",
			LingerMs:        5 * time.Millisecond,
			MaxInFlight:     1, // å¹‚ç­‰æ€§ç”Ÿäº§è€…è¦æ±‚MaxInFlight=1
		},
		Consumer: eventbus.ConsumerConfig{
			GroupID:            fmt.Sprintf("kafka-perf-%s-group", pressureEn),
			AutoOffsetReset:    "earliest",
			SessionTimeout:     30 * time.Second,
			HeartbeatInterval:  3 * time.Second,
			MaxProcessingTime:  30 * time.Second,
			FetchMinBytes:      1024,
			FetchMaxBytes:      50 * 1024 * 1024,
			FetchMaxWait:       500 * time.Millisecond,
			MaxPollRecords:     500,
			EnableAutoCommit:   false,
			AutoCommitInterval: 5 * time.Second,
			IsolationLevel:     "read_committed",
			RebalanceStrategy:  "range",
		},
		HealthCheckInterval:  30 * time.Second,
		MetadataRefreshFreq:  10 * time.Minute,
		MetadataRetryMax:     3,
		MetadataRetryBackoff: 250 * time.Millisecond,
	}

	// ğŸ”‘ è¦æ±‚ï¼šåˆ›å»º 5 ä¸ª topicï¼Œæ¯ä¸ª topic 3 ä¸ªåˆ†åŒº
	topicCount := 5
	topics := make([]string, topicCount)
	for i := 0; i < topicCount; i++ {
		topics[i] = fmt.Sprintf("kafka.perf.%s.topic%d", pressureEn, i+1)
	}

	// åˆ›å»º Kafka EventBus
	eb, err := eventbus.NewKafkaEventBus(kafkaConfig)
	require.NoError(t, err, "Failed to create Kafka EventBus")

	// ğŸ”§ ä½¿ç”¨ TopicBuilder åˆ›å»º 3 åˆ†åŒºçš„ Kafka Topicsï¼ˆåŒ…å« Snappy å‹ç¼©é…ç½®ï¼‰
	ctx := context.Background()
	partitionMap := createKafkaTopicsWithBuilder(ctx, t, eb, topics, 3, 1)
	metrics.PartitionCount = partitionMap
	defer func() {
		// å…³é—­ EventBus
		eb.Close()

		// ç­‰å¾… goroutines å®Œå…¨é€€å‡º
		t.Logf("â³ ç­‰å¾… Kafka EventBus goroutines é€€å‡º...")
		time.Sleep(5 * time.Second)

		// å¼ºåˆ¶ GC
		runtime.GC()
		time.Sleep(100 * time.Millisecond)

		// è®°å½•æœ€ç»ˆèµ„æºçŠ¶æ€ï¼ˆåœ¨ Close() ä¹‹åï¼‰
		metrics.FinalGoroutines = runtime.NumGoroutine()
		var finalMem runtime.MemStats
		runtime.ReadMemStats(&finalMem)
		metrics.FinalMemoryMB = float64(finalMem.Alloc) / 1024 / 1024
		metrics.MemoryDeltaMB = metrics.FinalMemoryMB - metrics.InitialMemoryMB
		metrics.GoroutineLeak = metrics.FinalGoroutines - metrics.InitialGoroutines

		t.Logf("ğŸ“Š Kafka èµ„æºæ¸…ç†å®Œæˆ: åˆå§‹ %d -> æœ€ç»ˆ %d (æ³„æ¼ %d)",
			metrics.InitialGoroutines, metrics.FinalGoroutines, metrics.GoroutineLeak)
	}()

	// ğŸ”‘ ä¸šç•Œæœ€ä½³å®è·µï¼šåœ¨è®¢é˜…ä¹‹å‰è®¾ç½®é¢„è®¢é˜… topic åˆ—è¡¨
	// è¿™æ ·å¯ä»¥é¿å… Kafka Consumer Group çš„é¢‘ç¹é‡å¹³è¡¡ï¼Œæé«˜æ€§èƒ½å’Œç¨³å®šæ€§
	// å‚è€ƒï¼š
	//   - Confluent å®˜æ–¹æ–‡æ¡£ï¼šé¢„è®¢é˜…æ¨¡å¼é¿å…é‡å¹³è¡¡
	//   - LinkedIn å®è·µï¼šä¸€æ¬¡æ€§è®¢é˜…æ‰€æœ‰ topic
	//   - Uber å®è·µï¼šé¢„é…ç½® topic åˆ—è¡¨
	//   - sdk/pkg/eventbus/PRE_SUBSCRIPTION_FINAL_REPORT.md
	//   - sdk/pkg/eventbus/pre_subscription_test.go
	if kafkaBus, ok := eb.(interface {
		SetPreSubscriptionTopics([]string)
	}); ok {
		kafkaBus.SetPreSubscriptionTopics(topics)
		t.Logf("âœ… å·²è®¾ç½®é¢„è®¢é˜… topic åˆ—è¡¨: %v", topics)
	}

	// ç­‰å¾…è¿æ¥å»ºç«‹ï¼ˆå‚è€ƒæˆåŠŸæµ‹è¯•çš„ç­‰å¾…æ—¶é—´ï¼‰
	time.Sleep(3 * time.Second)

	// è®°å½• topic ä¿¡æ¯
	metrics.TopicCount = topicCount
	metrics.TopicList = topics
	metrics.ConnectionCount = 1    // Kafka ä½¿ç”¨å•ä¸ªè¿æ¥æ± 
	metrics.ConsumerGroupCount = 1 // ä¸€ä¸ªæ¶ˆè´¹è€…ç»„

	// è¿è¡Œæµ‹è¯• - åœ¨å¤šä¸ª topic ä¸Šåˆ†å‘æ¶ˆæ¯
	runPerformanceTestMultiTopic(t, eb, topics, messageCount, timeout, metrics)

	return metrics
}

// runNATSTest è¿è¡Œ NATS JetStream æ€§èƒ½æµ‹è¯•ï¼ˆç£ç›˜æŒä¹…åŒ–ï¼‰
func runNATSTest(t *testing.T, pressure string, messageCount int, timeout time.Duration) *PerfMetrics {
	metrics := &PerfMetrics{
		System:       "NATS",
		Pressure:     pressure,
		MessageCount: messageCount,
	}

	// è®°å½•åˆå§‹èµ„æºçŠ¶æ€
	metrics.InitialGoroutines = runtime.NumGoroutine()
	var m runtime.MemStats
	runtime.ReadMemStats(&m)
	metrics.InitialMemoryMB = float64(m.Alloc) / 1024 / 1024

	// åˆ›å»º NATS EventBusï¼ˆJetStream ç£ç›˜æŒä¹…åŒ–ï¼‰
	// ğŸ”‘ å…³é”®ä¿®å¤ï¼šä¸èƒ½ä½¿ç”¨ä¸­æ–‡å­—ç¬¦
	pressureEn := map[string]string{
		"ä½å‹": "low",
		"ä¸­å‹": "medium",
		"é«˜å‹": "high",
		"æé™": "extreme",
	}[pressure]
	if pressureEn == "" {
		pressureEn = pressure // å¦‚æœä¸æ˜¯ä¸­æ–‡ï¼Œç›´æ¥ä½¿ç”¨
	}

	timestamp := time.Now().Unix()
	streamName := fmt.Sprintf("PERF_%s_%d", pressureEn, timestamp)
	subjectPattern := fmt.Sprintf("nats.perf.%s.%d.>", pressureEn, timestamp) // ä½¿ç”¨è‹±æ–‡å’Œæ—¶é—´æˆ³ç¡®ä¿å”¯ä¸€æ€§

	natsConfig := &eventbus.NATSConfig{
		URLs:                []string{"nats://localhost:4223"},
		ClientID:            fmt.Sprintf("nats-perf-%s-%d", pressureEn, timestamp),
		MaxReconnects:       10,
		ReconnectWait:       1 * time.Second,
		ConnectionTimeout:   30 * time.Second,
		HealthCheckInterval: 60 * time.Second,
		JetStream: eventbus.JetStreamConfig{
			Enabled:        true,
			PublishTimeout: 10 * time.Second,
			AckWait:        15 * time.Second,
			MaxDeliver:     3,
			Stream: eventbus.StreamConfig{
				Name:      streamName,
				Subjects:  []string{subjectPattern},
				Storage:   "file", // ğŸ”‘ å…³é”®ï¼šç£ç›˜æŒä¹…åŒ–
				Retention: "limits",
				MaxAge:    1 * time.Hour,
				MaxBytes:  1024 * 1024 * 1024, // 1GB
				MaxMsgs:   100000,
				Replicas:  1,
				Discard:   "old",
			},
			Consumer: eventbus.NATSConsumerConfig{
				DurableName:   fmt.Sprintf("perf_%s_%d", pressureEn, timestamp),
				DeliverPolicy: "all",
				AckPolicy:     "explicit",
				ReplayPolicy:  "instant",
				MaxAckPending: 1000,
				MaxWaiting:    500,
				MaxDeliver:    3,
			},
		},
	}

	eb, err := eventbus.NewNATSEventBus(natsConfig)
	require.NoError(t, err, "Failed to create NATS EventBus")
	defer func() {
		// å…³é—­ EventBus
		eb.Close()

		// ç­‰å¾… goroutines å®Œå…¨é€€å‡º
		t.Logf("â³ ç­‰å¾… NATS EventBus goroutines é€€å‡º...")
		time.Sleep(5 * time.Second)

		// å¼ºåˆ¶ GC
		runtime.GC()
		time.Sleep(100 * time.Millisecond)

		// è®°å½•æœ€ç»ˆèµ„æºçŠ¶æ€ï¼ˆåœ¨ Close() ä¹‹åï¼‰
		metrics.FinalGoroutines = runtime.NumGoroutine()
		var finalMem runtime.MemStats
		runtime.ReadMemStats(&finalMem)
		metrics.FinalMemoryMB = float64(finalMem.Alloc) / 1024 / 1024
		metrics.MemoryDeltaMB = metrics.FinalMemoryMB - metrics.InitialMemoryMB
		metrics.GoroutineLeak = metrics.FinalGoroutines - metrics.InitialGoroutines

		t.Logf("ğŸ“Š NATS èµ„æºæ¸…ç†å®Œæˆ: åˆå§‹ %d -> æœ€ç»ˆ %d (æ³„æ¼ %d)",
			metrics.InitialGoroutines, metrics.FinalGoroutines, metrics.GoroutineLeak)
	}()

	// ç­‰å¾…è¿æ¥å»ºç«‹
	time.Sleep(2 * time.Second)

	// ğŸ”‘ è¦æ±‚ï¼šåˆ›å»º 5 ä¸ª topic (NATS ä¸­ç§°ä¸º subject)
	topicCount := 5
	topics := make([]string, topicCount)
	for i := 0; i < topicCount; i++ {
		topics[i] = fmt.Sprintf("nats.perf.%s.%d.topic%d", pressureEn, timestamp, i+1)
	}

	// â­ Stream å·²åœ¨é…ç½®ä¸­é¢„å»ºç«‹ï¼ˆsubject pattern: nats.perf.{pressure}.{timestamp}.>ï¼‰
	// æ— éœ€å†è°ƒç”¨ ConfigureTopicï¼Œé¿å… subject é‡å é”™è¯¯
	t.Logf("âœ… ä½¿ç”¨é¢„å»ºç«‹çš„ Stream: %s (subjects: %v)", streamName, subjectPattern)

	// è®°å½• topic ä¿¡æ¯
	metrics.TopicCount = topicCount
	metrics.TopicList = topics
	metrics.ConnectionCount = 1    // NATS ä½¿ç”¨å•ä¸ªè¿æ¥
	metrics.ConsumerGroupCount = 1 // NATS JetStream ä½¿ç”¨ä¸€ä¸ª durable consumer

	// è¿è¡Œæµ‹è¯• - åœ¨å¤šä¸ª topic ä¸Šåˆ†å‘æ¶ˆæ¯
	runPerformanceTestMultiTopic(t, eb, topics, messageCount, timeout, metrics)

	return metrics
}

// runPerformanceTestMultiTopic è¿è¡Œå¤š topic æ€§èƒ½æµ‹è¯•æ ¸å¿ƒé€»è¾‘
func runPerformanceTestMultiTopic(t *testing.T, eb eventbus.EventBus, topics []string, messageCount int, timeout time.Duration, metrics *PerfMetrics) {
	ctx, cancel := context.WithTimeout(context.Background(), timeout)
	defer cancel()

	// èšåˆIDé¡ºåºè·Ÿè¸ªï¼ˆä½¿ç”¨é«˜æ€§èƒ½åˆ†ç‰‡é”æ£€æŸ¥å™¨ï¼‰
	orderChecker := NewOrderChecker()

	// åˆ›å»ºç»Ÿä¸€çš„æ¶ˆæ¯å¤„ç†å™¨
	handler := func(ctx context.Context, envelope *eventbus.Envelope) error {
		receiveTime := time.Now()

		// æ›´æ–°æ¥æ”¶è®¡æ•°
		atomic.AddInt64(&metrics.MessagesReceived, 1)

		// æ£€æŸ¥é¡ºåºæ€§ï¼ˆçº¿ç¨‹å®‰å…¨ï¼Œä½¿ç”¨åˆ†ç‰‡é”ï¼‰
		orderChecker.Check(envelope.AggregateID, envelope.EventVersion)

		// è®°å½•ç«¯åˆ°ç«¯å¤„ç†å»¶è¿Ÿï¼ˆä»å‘é€æ—¶é—´åˆ°æ¥æ”¶æ—¶é—´ï¼‰
		if !envelope.Timestamp.IsZero() {
			latency := receiveTime.Sub(envelope.Timestamp).Microseconds()
			atomic.AddInt64(&metrics.procLatencySum, latency)
			atomic.AddInt64(&metrics.procLatencyCount, 1)
		}

		// æ›´æ–°å³°å€¼åç¨‹æ•°ï¼ˆåŸå­æ“ä½œï¼‰
		currentGoroutines := int32(runtime.NumGoroutine())
		for {
			oldPeak := atomic.LoadInt32(&metrics.PeakGoroutines)
			if currentGoroutines <= oldPeak {
				break
			}
			if atomic.CompareAndSwapInt32(&metrics.PeakGoroutines, oldPeak, currentGoroutines) {
				break
			}
		}

		// æ›´æ–°å³°å€¼å†…å­˜ï¼ˆåŸå­æ“ä½œï¼‰
		var m runtime.MemStats
		runtime.ReadMemStats(&m)
		currentMemoryMB := float64(m.Alloc) / 1024 / 1024
		currentMemoryBits := math.Float64bits(currentMemoryMB)
		for {
			oldPeakBits := atomic.LoadUint64(&metrics.PeakMemoryMB)
			oldPeakMB := math.Float64frombits(oldPeakBits)
			if currentMemoryMB <= oldPeakMB {
				break
			}
			if atomic.CompareAndSwapUint64(&metrics.PeakMemoryMB, oldPeakBits, currentMemoryBits) {
				break
			}
		}

		return nil
	}

	// ğŸ”‘ ä¸šç•Œæœ€ä½³å®è·µï¼šå¹¶å‘è®¢é˜…æ‰€æœ‰ topic
	// Kafka çš„é¢„è®¢é˜…æœºåˆ¶ç°åœ¨æ”¯æŒåŠ¨æ€è®¢é˜…ï¼Œä¼šè‡ªåŠ¨è§¦å‘é‡å¹³è¡¡
	var wg sync.WaitGroup
	for _, topic := range topics {
		wg.Add(1)
		topicName := topic // é¿å…é—­åŒ…é—®é¢˜

		go func() {
			defer wg.Done()

			// ä½¿ç”¨ SubscribeEnvelope è®¢é˜…
			if err := eb.SubscribeEnvelope(ctx, topicName, handler); err != nil {
				t.Logf("âš ï¸  è®¢é˜… topic %s å¤±è´¥: %v", topicName, err)
				atomic.AddInt64(&metrics.ProcessErrors, 1)
			}
		}()
	}

	// ç­‰å¾…æ‰€æœ‰è®¢é˜…å®Œæˆ
	wg.Wait()

	// ç­‰å¾…è®¢é˜…å»ºç«‹å’Œé‡å¹³è¡¡å®Œæˆï¼ˆå‚è€ƒæˆåŠŸæµ‹è¯•çš„ç­‰å¾…æ—¶é—´ï¼‰
	time.Sleep(5 * time.Second)

	// å¼€å§‹å‘é€æ¶ˆæ¯
	metrics.StartTime = time.Now()

	// æ ¹æ®æ¶ˆæ¯æ•°é‡åŠ¨æ€è®¡ç®—èšåˆIDæ•°é‡
	// ä½å‹500 -> 50ä¸ª, ä¸­å‹2000 -> 200ä¸ª, é«˜å‹5000 -> 500ä¸ª, æé™10000 -> 1000ä¸ª
	aggregateCount := messageCount / 10
	if aggregateCount < 50 {
		aggregateCount = 50
	}
	if aggregateCount > 1000 {
		aggregateCount = 1000
	}

	// ç”ŸæˆèšåˆIDåˆ—è¡¨
	aggregateIDs := make([]string, aggregateCount)
	for i := 0; i < aggregateCount; i++ {
		aggregateIDs[i] = fmt.Sprintf("aggregate-%d", i)
	}

	// è®¡ç®—æ¯ä¸ªèšåˆIDéœ€è¦å‘é€çš„æ¶ˆæ¯æ•°é‡
	messagesPerAggregate := messageCount / aggregateCount
	remainingMessages := messageCount % aggregateCount

	// ä¸ºæ¯ä¸ªèšåˆIDåˆ›å»ºä¸€ä¸ª goroutine ä¸²è¡Œå‘é€æ¶ˆæ¯
	// è¿™æ ·ä¿è¯åŒä¸€ä¸ªèšåˆIDçš„æ¶ˆæ¯ä¸¥æ ¼æŒ‰é¡ºåºå‘é€
	var sendWg sync.WaitGroup
	for aggIndex := 0; aggIndex < aggregateCount; aggIndex++ {
		sendWg.Add(1)
		go func(aggregateIndex int) {
			defer sendWg.Done()

			aggregateID := aggregateIDs[aggregateIndex]

			// è®¡ç®—è¯¥èšåˆIDéœ€è¦å‘é€çš„æ¶ˆæ¯æ•°é‡
			msgCount := messagesPerAggregate
			if aggregateIndex < remainingMessages {
				msgCount++ // å‰é¢çš„èšåˆIDå¤šå‘é€ä¸€æ¡æ¶ˆæ¯
			}

			// ä¸²è¡Œå‘é€è¯¥èšåˆIDçš„æ‰€æœ‰æ¶ˆæ¯
			for version := int64(1); version <= int64(msgCount); version++ {
				// è½®è¯¢é€‰æ‹© topic
				topic := topics[aggregateIndex%len(topics)]

				// ç”Ÿæˆ EventIDï¼ˆæ ¼å¼ï¼šAggregateID:EventType:EventVersion:Timestampï¼‰
				eventID := fmt.Sprintf("%s:TestEvent:%d:%d", aggregateID, version, time.Now().UnixNano())

				envelope := &eventbus.Envelope{
					EventID:      eventID,
					AggregateID:  aggregateID,
					EventType:    "TestEvent",
					EventVersion: version, // ä¸¥æ ¼é€’å¢çš„ç‰ˆæœ¬å·
					Timestamp:    time.Now(),
					Payload:      jxtjson.RawMessage(fmt.Sprintf(`{"aggregate":"%s","version":%d}`, aggregateID, version)),
				}

				sendStart := time.Now()
				if err := eb.PublishEnvelope(ctx, topic, envelope); err != nil {
					atomic.AddInt64(&metrics.SendErrors, 1)
				} else {
					atomic.AddInt64(&metrics.MessagesSent, 1)
					// è®°å½•å‘é€å»¶è¿Ÿ
					latency := time.Since(sendStart).Microseconds()
					atomic.AddInt64(&metrics.sendLatencySum, latency)
					atomic.AddInt64(&metrics.sendLatencyCount, 1)
				}
			}
		}(aggIndex)
	}

	// ç­‰å¾…æ‰€æœ‰å‘é€å®Œæˆ
	sendWg.Wait()
	t.Logf("âœ… å‘é€å®Œæˆ: %d/%d æ¡æ¶ˆæ¯", metrics.MessagesSent, messageCount)

	// è¾“å‡ºåˆ†åŒºä¿¡æ¯
	if len(metrics.PartitionCount) > 0 {
		t.Logf("ğŸ“Š Kafka Topic åˆ†åŒºé…ç½®:")
		for topic, partitions := range metrics.PartitionCount {
			t.Logf("   %s: %d partitions", topic, partitions)
		}
	}

	// ç­‰å¾…æ¥æ”¶å®Œæˆæˆ–è¶…æ—¶ï¼ˆå‚è€ƒæˆåŠŸæµ‹è¯•çš„ç­‰å¾…ç­–ç•¥ï¼‰
	t.Logf("â³ ç­‰å¾…æ¶ˆæ¯å¤„ç†å®Œæˆ...")
	waitTime := 15 * time.Second
	if messageCount > 2000 {
		waitTime = 30 * time.Second
	}
	time.Sleep(waitTime)

	metrics.EndTime = time.Now()
	metrics.Duration = metrics.EndTime.Sub(metrics.StartTime)

	// è®¡ç®—æ€§èƒ½æŒ‡æ ‡
	durationSeconds := metrics.Duration.Seconds()
	if durationSeconds > 0 {
		metrics.SendThroughput = float64(atomic.LoadInt64(&metrics.MessagesSent)) / durationSeconds
		metrics.ReceiveThroughput = float64(atomic.LoadInt64(&metrics.MessagesReceived)) / durationSeconds
	}

	sendLatencyCount := atomic.LoadInt64(&metrics.sendLatencyCount)
	if sendLatencyCount > 0 {
		sendLatencySum := atomic.LoadInt64(&metrics.sendLatencySum)
		metrics.AvgSendLatency = float64(sendLatencySum) / float64(sendLatencyCount) / 1000.0 // è½¬æ¢ä¸ºæ¯«ç§’
	}

	procLatencyCount := atomic.LoadInt64(&metrics.procLatencyCount)
	if procLatencyCount > 0 {
		procLatencySum := atomic.LoadInt64(&metrics.procLatencySum)
		metrics.AvgProcessLatency = float64(procLatencySum) / float64(procLatencyCount) / 1000.0 // è½¬æ¢ä¸ºæ¯«ç§’
	}

	metrics.SuccessRate = float64(metrics.MessagesReceived) / float64(messageCount) * 100

	// è·å–é¡ºåºè¿åæ¬¡æ•°
	metrics.OrderViolations = orderChecker.GetViolations()

	// å–æ¶ˆè®¢é˜…
	cancel()

	// æ³¨æ„ï¼šæœ€ç»ˆèµ„æºçŠ¶æ€å°†åœ¨ EventBus Close() ä¹‹åè®°å½•
	// è¿™æ ·å¯ä»¥å‡†ç¡®ç»Ÿè®¡åç¨‹æ³„æ¼
}

// runPerformanceTest è¿è¡Œå• topic æ€§èƒ½æµ‹è¯•æ ¸å¿ƒé€»è¾‘ï¼ˆä¿ç•™ç”¨äºå…¼å®¹ï¼‰
func runPerformanceTest(t *testing.T, eb eventbus.EventBus, topic string, messageCount int, timeout time.Duration, metrics *PerfMetrics) {
	ctx, cancel := context.WithTimeout(context.Background(), timeout)
	defer cancel()

	// èšåˆIDé¡ºåºè·Ÿè¸ªï¼ˆä½¿ç”¨é«˜æ€§èƒ½åˆ†ç‰‡é”æ£€æŸ¥å™¨ï¼‰
	orderChecker := NewOrderChecker()

	// ä½¿ç”¨ SubscribeEnvelope è®¢é˜…
	var wg sync.WaitGroup
	wg.Add(1)

	go func() {
		defer wg.Done()

		handler := func(ctx context.Context, envelope *eventbus.Envelope) error {
			receiveTime := time.Now()

			// æ›´æ–°æ¥æ”¶è®¡æ•°
			atomic.AddInt64(&metrics.MessagesReceived, 1)

			// æ£€æŸ¥é¡ºåºæ€§ï¼ˆçº¿ç¨‹å®‰å…¨ï¼Œä½¿ç”¨åˆ†ç‰‡é”ï¼‰
			orderChecker.Check(envelope.AggregateID, envelope.EventVersion)

			// è®°å½•ç«¯åˆ°ç«¯å¤„ç†å»¶è¿Ÿï¼ˆä»å‘é€æ—¶é—´åˆ°æ¥æ”¶æ—¶é—´ï¼‰
			if !envelope.Timestamp.IsZero() {
				latency := receiveTime.Sub(envelope.Timestamp).Microseconds()
				atomic.AddInt64(&metrics.procLatencySum, latency)
				atomic.AddInt64(&metrics.procLatencyCount, 1)
			}

			return nil
		}

		// ğŸ”‘ å…³é”®ï¼šä½¿ç”¨ SubscribeEnvelope æ–¹æ³•
		err := eb.(eventbus.EnvelopeSubscriber).SubscribeEnvelope(ctx, topic, handler)
		if err != nil {
			t.Logf("âŒ Subscribe error: %v", err)
			atomic.AddInt64(&metrics.ProcessErrors, 1)
		}
	}()

	// ç­‰å¾…è®¢é˜…å»ºç«‹ï¼ˆå‚è€ƒæˆåŠŸæµ‹è¯•çš„ç­‰å¾…æ—¶é—´ï¼‰
	time.Sleep(3 * time.Second)

	// å¼€å§‹å‘é€æ¶ˆæ¯
	metrics.StartTime = time.Now()

	// æ ¹æ®æ¶ˆæ¯æ•°é‡åŠ¨æ€è®¡ç®—èšåˆIDæ•°é‡
	// ä½å‹500 -> 50ä¸ª, ä¸­å‹2000 -> 200ä¸ª, é«˜å‹5000 -> 500ä¸ª, æé™10000 -> 1000ä¸ª
	aggregateCount := messageCount / 10
	if aggregateCount < 50 {
		aggregateCount = 50
	}
	if aggregateCount > 1000 {
		aggregateCount = 1000
	}

	// ç”ŸæˆèšåˆIDåˆ—è¡¨
	aggregateIDs := make([]string, aggregateCount)
	for i := 0; i < aggregateCount; i++ {
		aggregateIDs[i] = fmt.Sprintf("aggregate-%d", i)
	}

	// è®¡ç®—æ¯ä¸ªèšåˆIDéœ€è¦å‘é€çš„æ¶ˆæ¯æ•°é‡
	messagesPerAggregate := messageCount / aggregateCount
	remainingMessages := messageCount % aggregateCount

	// ä¸ºæ¯ä¸ªèšåˆIDåˆ›å»ºä¸€ä¸ª goroutine ä¸²è¡Œå‘é€æ¶ˆæ¯
	// è¿™æ ·ä¿è¯åŒä¸€ä¸ªèšåˆIDçš„æ¶ˆæ¯ä¸¥æ ¼æŒ‰é¡ºåºå‘é€
	var sendWg sync.WaitGroup
	for aggIndex := 0; aggIndex < aggregateCount; aggIndex++ {
		sendWg.Add(1)
		go func(aggregateIndex int) {
			defer sendWg.Done()

			aggregateID := aggregateIDs[aggregateIndex]

			// è®¡ç®—è¯¥èšåˆIDéœ€è¦å‘é€çš„æ¶ˆæ¯æ•°é‡
			msgCount := messagesPerAggregate
			if aggregateIndex < remainingMessages {
				msgCount++ // å‰é¢çš„èšåˆIDå¤šå‘é€ä¸€æ¡æ¶ˆæ¯
			}

			// ä¸²è¡Œå‘é€è¯¥èšåˆIDçš„æ‰€æœ‰æ¶ˆæ¯
			for version := int64(1); version <= int64(msgCount); version++ {
				// ç”Ÿæˆ EventIDï¼ˆæ ¼å¼ï¼šAggregateID:EventType:EventVersion:Timestampï¼‰
				eventID := fmt.Sprintf("%s:PerformanceTestEvent:%d:%d", aggregateID, version, time.Now().UnixNano())

				// åˆ›å»º Envelope
				envelope := &eventbus.Envelope{
					EventID:      eventID,
					AggregateID:  aggregateID,
					EventType:    "PerformanceTestEvent",
					EventVersion: version, // ä¸¥æ ¼é€’å¢çš„ç‰ˆæœ¬å·
					Timestamp:    time.Now(),
					Payload:      jxtjson.RawMessage(fmt.Sprintf(`{"aggregate":"%s","version":%d}`, aggregateID, version)),
				}

				// ğŸ”‘ å…³é”®ï¼šä½¿ç”¨ PublishEnvelope æ–¹æ³•
				sendStart := time.Now()
				err := eb.(eventbus.EnvelopePublisher).PublishEnvelope(ctx, topic, envelope)
				sendLatency := time.Since(sendStart).Microseconds()

				if err != nil {
					atomic.AddInt64(&metrics.SendErrors, 1)
				} else {
					atomic.AddInt64(&metrics.MessagesSent, 1)
					atomic.AddInt64(&metrics.sendLatencySum, sendLatency)
					atomic.AddInt64(&metrics.sendLatencyCount, 1)
				}

				// æ›´æ–°å³°å€¼èµ„æº
				updatePeakResources(metrics)
			}
		}(aggIndex)
	}

	// ç­‰å¾…å‘é€å®Œæˆ
	sendWg.Wait()
	t.Logf("âœ… å‘é€å®Œæˆ: %d/%d æ¡æ¶ˆæ¯", atomic.LoadInt64(&metrics.MessagesSent), messageCount)

	// ç­‰å¾…æ¥æ”¶å®Œæˆæˆ–è¶…æ—¶ï¼ˆå‚è€ƒæˆåŠŸæµ‹è¯•çš„ç­‰å¾…ç­–ç•¥ï¼‰
	t.Logf("â³ ç­‰å¾…æ¶ˆæ¯å¤„ç†å®Œæˆ...")
	waitTime := 15 * time.Second
	if messageCount > 2000 {
		waitTime = 30 * time.Second
	}
	time.Sleep(waitTime)

	metrics.EndTime = time.Now()
	metrics.Duration = metrics.EndTime.Sub(metrics.StartTime)

	// è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
	calculateFinalMetrics(metrics)

	// è®°å½•æœ€ç»ˆèµ„æºçŠ¶æ€
	metrics.FinalGoroutines = runtime.NumGoroutine()
	var m runtime.MemStats
	runtime.ReadMemStats(&m)
	metrics.FinalMemoryMB = float64(m.Alloc) / 1024 / 1024
	metrics.GoroutineLeak = metrics.FinalGoroutines - metrics.InitialGoroutines

	// è·å–é¡ºåºè¿åæ¬¡æ•°
	metrics.OrderViolations = orderChecker.GetViolations()

	// å–æ¶ˆä¸Šä¸‹æ–‡ï¼Œåœæ­¢è®¢é˜…
	cancel()
	time.Sleep(1 * time.Second)
}

// updatePeakResources æ›´æ–°å³°å€¼èµ„æºå ç”¨ï¼ˆåŸå­æ“ä½œï¼‰
func updatePeakResources(metrics *PerfMetrics) {
	// æ›´æ–°å³°å€¼åç¨‹æ•°
	currentGoroutines := int32(runtime.NumGoroutine())
	for {
		oldPeak := atomic.LoadInt32(&metrics.PeakGoroutines)
		if currentGoroutines <= oldPeak {
			break
		}
		if atomic.CompareAndSwapInt32(&metrics.PeakGoroutines, oldPeak, currentGoroutines) {
			break
		}
	}

	// æ›´æ–°å³°å€¼å†…å­˜
	var m runtime.MemStats
	runtime.ReadMemStats(&m)
	memoryMB := float64(m.Alloc) / 1024 / 1024
	memoryBits := math.Float64bits(memoryMB)
	for {
		oldPeakBits := atomic.LoadUint64(&metrics.PeakMemoryMB)
		oldPeakMB := math.Float64frombits(oldPeakBits)
		if memoryMB <= oldPeakMB {
			break
		}
		if atomic.CompareAndSwapUint64(&metrics.PeakMemoryMB, oldPeakBits, memoryBits) {
			break
		}
	}
}

// calculateFinalMetrics è®¡ç®—æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡
func calculateFinalMetrics(metrics *PerfMetrics) {
	// æˆåŠŸç‡
	messagesSent := atomic.LoadInt64(&metrics.MessagesSent)
	messagesReceived := atomic.LoadInt64(&metrics.MessagesReceived)
	if messagesSent > 0 {
		metrics.SuccessRate = float64(messagesReceived) / float64(messagesSent) * 100
	}

	// ååé‡
	if metrics.Duration.Seconds() > 0 {
		metrics.SendThroughput = float64(messagesSent) / metrics.Duration.Seconds()
		metrics.ReceiveThroughput = float64(messagesReceived) / metrics.Duration.Seconds()
	}

	// å¹³å‡å»¶è¿Ÿ
	sendLatencyCount := atomic.LoadInt64(&metrics.sendLatencyCount)
	if sendLatencyCount > 0 {
		sendLatencySum := atomic.LoadInt64(&metrics.sendLatencySum)
		metrics.AvgSendLatency = float64(sendLatencySum) / float64(sendLatencyCount) / 1000.0 // ms
	}
	procLatencyCount := atomic.LoadInt64(&metrics.procLatencyCount)
	if procLatencyCount > 0 {
		procLatencySum := atomic.LoadInt64(&metrics.procLatencySum)
		metrics.AvgProcessLatency = float64(procLatencySum) / float64(procLatencyCount) / 1000.0 // ms
	}

	// èµ„æºå¢é‡
	peakMemoryMB := math.Float64frombits(atomic.LoadUint64(&metrics.PeakMemoryMB))
	metrics.MemoryDeltaMB = peakMemoryMB - metrics.InitialMemoryMB
}

// compareRoundResults å¯¹æ¯”å•è½®æµ‹è¯•ç»“æœ
func compareRoundResults(t *testing.T, pressure string, kafka, nats *PerfMetrics) {
	t.Log("\n" + strings.Repeat("=", 80))
	t.Logf("ğŸ“Š %s æµ‹è¯•ç»“æœå¯¹æ¯”", pressure)
	t.Log(strings.Repeat("=", 80))

	// æ¶ˆæ¯ç»Ÿè®¡å¯¹æ¯”
	t.Logf("\nğŸ“¨ æ¶ˆæ¯ç»Ÿè®¡:")
	t.Logf("   %-20s | Kafka: %8d | NATS: %8d", "å‘é€æ¶ˆæ¯æ•°", kafka.MessagesSent, nats.MessagesSent)
	t.Logf("   %-20s | Kafka: %8d | NATS: %8d", "æ¥æ”¶æ¶ˆæ¯æ•°", kafka.MessagesReceived, nats.MessagesReceived)
	t.Logf("   %-20s | Kafka: %7.2f%% | NATS: %7.2f%%", "æˆåŠŸç‡", kafka.SuccessRate, nats.SuccessRate)
	t.Logf("   %-20s | Kafka: %8d | NATS: %8d", "å‘é€é”™è¯¯", kafka.SendErrors, nats.SendErrors)
	t.Logf("   %-20s | Kafka: %8d | NATS: %8d", "å¤„ç†é”™è¯¯", kafka.ProcessErrors, nats.ProcessErrors)
	t.Logf("   %-20s | Kafka: %8d | NATS: %8d", "é¡ºåºè¿å", kafka.OrderViolations, nats.OrderViolations)

	// æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
	t.Logf("\nğŸš€ æ€§èƒ½æŒ‡æ ‡:")
	t.Logf("   %-20s | Kafka: %10.2f msg/s | NATS: %10.2f msg/s", "å‘é€ååé‡", kafka.SendThroughput, nats.SendThroughput)
	t.Logf("   %-20s | Kafka: %10.2f msg/s | NATS: %10.2f msg/s", "æ¥æ”¶ååé‡", kafka.ReceiveThroughput, nats.ReceiveThroughput)
	t.Logf("   %-20s | Kafka: %10.3f ms | NATS: %10.3f ms", "å¹³å‡å‘é€å»¶è¿Ÿ", kafka.AvgSendLatency, nats.AvgSendLatency)
	t.Logf("   %-20s | Kafka: %10.3f ms | NATS: %10.3f ms", "å¹³å‡å¤„ç†å»¶è¿Ÿ", kafka.AvgProcessLatency, nats.AvgProcessLatency)
	t.Logf("   %-20s | Kafka: %10.2f s | NATS: %10.2f s", "æ€»è€—æ—¶", kafka.Duration.Seconds(), nats.Duration.Seconds())

	// èµ„æºå ç”¨å¯¹æ¯”
	t.Logf("\nğŸ’¾ èµ„æºå ç”¨:")
	t.Logf("   %-20s | Kafka: %8d | NATS: %8d", "åˆå§‹åç¨‹æ•°", kafka.InitialGoroutines, nats.InitialGoroutines)
	t.Logf("   %-20s | Kafka: %8d | NATS: %8d", "å³°å€¼åç¨‹æ•°", atomic.LoadInt32(&kafka.PeakGoroutines), atomic.LoadInt32(&nats.PeakGoroutines))
	t.Logf("   %-20s | Kafka: %8d | NATS: %8d", "æœ€ç»ˆåç¨‹æ•°", kafka.FinalGoroutines, nats.FinalGoroutines)
	t.Logf("   %-20s | Kafka: %8d | NATS: %8d", "åç¨‹æ³„æ¼", kafka.GoroutineLeak, nats.GoroutineLeak)
	t.Logf("   %-20s | Kafka: %10.2f MB | NATS: %10.2f MB", "åˆå§‹å†…å­˜", kafka.InitialMemoryMB, nats.InitialMemoryMB)
	t.Logf("   %-20s | Kafka: %10.2f MB | NATS: %10.2f MB", "å³°å€¼å†…å­˜", math.Float64frombits(atomic.LoadUint64(&kafka.PeakMemoryMB)), math.Float64frombits(atomic.LoadUint64(&nats.PeakMemoryMB)))
	t.Logf("   %-20s | Kafka: %10.2f MB | NATS: %10.2f MB", "æœ€ç»ˆå†…å­˜", kafka.FinalMemoryMB, nats.FinalMemoryMB)
	t.Logf("   %-20s | Kafka: %10.2f MB | NATS: %10.2f MB", "å†…å­˜å¢é‡", kafka.MemoryDeltaMB, nats.MemoryDeltaMB)

	// è¿æ¥å’Œæ¶ˆè´¹è€…ç»„ç»Ÿè®¡ï¼ˆæ–°å¢ï¼‰
	t.Logf("\nğŸ”— è¿æ¥å’Œæ¶ˆè´¹è€…ç»„:")
	t.Logf("   %-20s | Kafka: %8d | NATS: %8d", "Topic æ•°é‡", kafka.TopicCount, nats.TopicCount)
	t.Logf("   %-20s | Kafka: %8d | NATS: %8d", "è¿æ¥æ•°", kafka.ConnectionCount, nats.ConnectionCount)
	t.Logf("   %-20s | Kafka: %8d | NATS: %8d", "æ¶ˆè´¹è€…ç»„ä¸ªæ•°", kafka.ConsumerGroupCount, nats.ConsumerGroupCount)
	if len(kafka.TopicList) > 0 {
		t.Logf("   Kafka Topics: %v", kafka.TopicList)
	}

	// è¾“å‡º Kafka åˆ†åŒºä¿¡æ¯
	if len(kafka.PartitionCount) > 0 {
		t.Logf("   Kafka Partitions:")
		for topic, partitions := range kafka.PartitionCount {
			t.Logf("      %s: %d partitions", topic, partitions)
		}
	}

	if len(nats.TopicList) > 0 {
		t.Logf("   NATS Topics: %v", nats.TopicList)
	}

	// æ€§èƒ½ä¼˜åŠ¿åˆ†æ
	t.Logf("\nğŸ† æ€§èƒ½ä¼˜åŠ¿:")
	if kafka.ReceiveThroughput > nats.ReceiveThroughput {
		improvement := (kafka.ReceiveThroughput - nats.ReceiveThroughput) / nats.ReceiveThroughput * 100
		t.Logf("   Kafka ååé‡ä¼˜åŠ¿: +%.2f%%", improvement)
	} else {
		improvement := (nats.ReceiveThroughput - kafka.ReceiveThroughput) / kafka.ReceiveThroughput * 100
		t.Logf("   NATS ååé‡ä¼˜åŠ¿: +%.2f%%", improvement)
	}

	if kafka.AvgProcessLatency < nats.AvgProcessLatency {
		improvement := (nats.AvgProcessLatency - kafka.AvgProcessLatency) / nats.AvgProcessLatency * 100
		t.Logf("   Kafka å»¶è¿Ÿä¼˜åŠ¿: -%.2f%%", improvement)
	} else {
		improvement := (kafka.AvgProcessLatency - nats.AvgProcessLatency) / kafka.AvgProcessLatency * 100
		t.Logf("   NATS å»¶è¿Ÿä¼˜åŠ¿: -%.2f%%", improvement)
	}

	if kafka.MemoryDeltaMB < nats.MemoryDeltaMB {
		improvement := (nats.MemoryDeltaMB - kafka.MemoryDeltaMB) / nats.MemoryDeltaMB * 100
		t.Logf("   Kafka å†…å­˜ä¼˜åŠ¿: -%.2f%%", improvement)
	} else {
		improvement := (kafka.MemoryDeltaMB - nats.MemoryDeltaMB) / kafka.MemoryDeltaMB * 100
		t.Logf("   NATS å†…å­˜ä¼˜åŠ¿: -%.2f%%", improvement)
	}

	t.Log(strings.Repeat("=", 80))

	// åŸºæœ¬æ–­è¨€
	assert.Greater(t, kafka.SuccessRate, 90.0, "Kafka æˆåŠŸç‡åº”è¯¥ > 90%")
	assert.Greater(t, nats.SuccessRate, 90.0, "NATS æˆåŠŸç‡åº”è¯¥ > 90%")
	assert.Equal(t, int64(0), kafka.OrderViolations, "Kafka ä¸åº”è¯¥æœ‰é¡ºåºè¿å")
	assert.Equal(t, int64(0), nats.OrderViolations, "NATS ä¸åº”è¯¥æœ‰é¡ºåºè¿å")
}

// generateComparisonReport ç”Ÿæˆç»¼åˆå¯¹æ¯”æŠ¥å‘Š
func generateComparisonReport(t *testing.T, allResults map[string][]*PerfMetrics) {
	t.Log("\n" + strings.Repeat("=", 100))
	t.Log("ğŸ† Kafka vs NATS JetStream ç»¼åˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
	t.Log(strings.Repeat("=", 100))

	kafkaResults := allResults["Kafka"]
	natsResults := allResults["NATS"]

	if len(kafkaResults) == 0 || len(natsResults) == 0 {
		t.Logf("âŒ æµ‹è¯•æ•°æ®ä¸è¶³ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
		return
	}

	// æ±‡æ€»è¡¨æ ¼
	t.Logf("\nğŸ“Š æ€§èƒ½æŒ‡æ ‡æ±‡æ€»è¡¨:")
	t.Logf("%-10s | %-15s | %-15s | %-15s | %-15s | %-15s | %-15s",
		"å‹åŠ›çº§åˆ«", "ç³»ç»Ÿ", "ååé‡(msg/s)", "å»¶è¿Ÿ(ms)", "æˆåŠŸç‡(%)", "å†…å­˜å¢é‡(MB)", "åç¨‹æ³„æ¼")
	t.Log(strings.Repeat("-", 100))

	for i := 0; i < len(kafkaResults); i++ {
		kafka := kafkaResults[i]
		nats := natsResults[i]

		t.Logf("%-10s | %-15s | %15.2f | %15.3f | %15.2f | %15.2f | %15d",
			kafka.Pressure, "Kafka", kafka.ReceiveThroughput, kafka.AvgProcessLatency,
			kafka.SuccessRate, kafka.MemoryDeltaMB, kafka.GoroutineLeak)
		t.Logf("%-10s | %-15s | %15.2f | %15.3f | %15.2f | %15.2f | %15d",
			nats.Pressure, "NATS", nats.ReceiveThroughput, nats.AvgProcessLatency,
			nats.SuccessRate, nats.MemoryDeltaMB, nats.GoroutineLeak)
		t.Log(strings.Repeat("-", 100))
	}

	// è®¡ç®—å¹³å‡å€¼
	var kafkaAvgThroughput, kafkaAvgLatency, kafkaAvgMemory float64
	var natsAvgThroughput, natsAvgLatency, natsAvgMemory float64

	for i := 0; i < len(kafkaResults); i++ {
		kafkaAvgThroughput += kafkaResults[i].ReceiveThroughput
		kafkaAvgLatency += kafkaResults[i].AvgProcessLatency
		kafkaAvgMemory += kafkaResults[i].MemoryDeltaMB

		natsAvgThroughput += natsResults[i].ReceiveThroughput
		natsAvgLatency += natsResults[i].AvgProcessLatency
		natsAvgMemory += natsResults[i].MemoryDeltaMB
	}

	count := float64(len(kafkaResults))
	kafkaAvgThroughput /= count
	kafkaAvgLatency /= count
	kafkaAvgMemory /= count
	natsAvgThroughput /= count
	natsAvgLatency /= count
	natsAvgMemory /= count

	// ç»¼åˆè¯„åˆ†
	t.Logf("\nğŸ¯ ç»¼åˆè¯„åˆ†:")
	t.Logf("%-20s | Kafka: %10.2f msg/s | NATS: %10.2f msg/s", "å¹³å‡ååé‡", kafkaAvgThroughput, natsAvgThroughput)
	t.Logf("%-20s | Kafka: %10.3f ms | NATS: %10.3f ms", "å¹³å‡å»¶è¿Ÿ", kafkaAvgLatency, natsAvgLatency)
	t.Logf("%-20s | Kafka: %10.2f MB | NATS: %10.2f MB", "å¹³å‡å†…å­˜å¢é‡", kafkaAvgMemory, natsAvgMemory)

	// è¿æ¥å’Œæ¶ˆè´¹è€…ç»„ç»Ÿè®¡ï¼ˆæ–°å¢ï¼‰
	if len(kafkaResults) > 0 && len(natsResults) > 0 {
		t.Logf("\nğŸ”— è¿æ¥å’Œæ¶ˆè´¹è€…ç»„ç»Ÿè®¡:")
		t.Logf("%-20s | Kafka: %8d | NATS: %8d", "Topic æ•°é‡", kafkaResults[0].TopicCount, natsResults[0].TopicCount)
		t.Logf("%-20s | Kafka: %8d | NATS: %8d", "è¿æ¥æ•°", kafkaResults[0].ConnectionCount, natsResults[0].ConnectionCount)
		t.Logf("%-20s | Kafka: %8d | NATS: %8d", "æ¶ˆè´¹è€…ç»„ä¸ªæ•°", kafkaResults[0].ConsumerGroupCount, natsResults[0].ConsumerGroupCount)
	}

	// è·èƒœè€…åˆ¤å®š
	t.Logf("\nğŸ† æœ€ç»ˆç»“è®º:")

	kafkaWins := 0
	natsWins := 0

	if kafkaAvgThroughput > natsAvgThroughput {
		kafkaWins++
		t.Logf("   âœ… ååé‡: Kafka èƒœå‡º (+%.2f%%)",
			(kafkaAvgThroughput-natsAvgThroughput)/natsAvgThroughput*100)
	} else {
		natsWins++
		t.Logf("   âœ… ååé‡: NATS èƒœå‡º (+%.2f%%)",
			(natsAvgThroughput-kafkaAvgThroughput)/kafkaAvgThroughput*100)
	}

	if kafkaAvgLatency < natsAvgLatency {
		kafkaWins++
		t.Logf("   âœ… å»¶è¿Ÿ: Kafka èƒœå‡º (-%.2f%%)",
			(natsAvgLatency-kafkaAvgLatency)/natsAvgLatency*100)
	} else {
		natsWins++
		t.Logf("   âœ… å»¶è¿Ÿ: NATS èƒœå‡º (-%.2f%%)",
			(kafkaAvgLatency-natsAvgLatency)/kafkaAvgLatency*100)
	}

	if kafkaAvgMemory < natsAvgMemory {
		kafkaWins++
		t.Logf("   âœ… å†…å­˜æ•ˆç‡: Kafka èƒœå‡º (-%.2f%%)",
			(natsAvgMemory-kafkaAvgMemory)/natsAvgMemory*100)
	} else {
		natsWins++
		t.Logf("   âœ… å†…å­˜æ•ˆç‡: NATS èƒœå‡º (-%.2f%%)",
			(kafkaAvgMemory-natsAvgMemory)/kafkaAvgMemory*100)
	}

	t.Logf("\nğŸ‰ æ€»ä½“è·èƒœè€…:")
	if kafkaWins > natsWins {
		t.Logf("   ğŸ¥‡ Kafka ä»¥ %d:%d è·èƒœï¼", kafkaWins, natsWins)
	} else if natsWins > kafkaWins {
		t.Logf("   ğŸ¥‡ NATS ä»¥ %d:%d è·èƒœï¼", natsWins, kafkaWins)
	} else {
		t.Logf("   ğŸ¤ å¹³å±€ %d:%d", kafkaWins, natsWins)
	}

	// ä½¿ç”¨å»ºè®®
	t.Logf("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
	t.Logf("ğŸ”´ é€‰æ‹© Kafka å½“:")
	t.Logf("   â€¢ éœ€è¦ä¼ä¸šçº§å¯é æ€§å’ŒæŒä¹…åŒ–ä¿è¯")
	t.Logf("   â€¢ éœ€è¦å¤æ‚çš„æ•°æ®å¤„ç†ç®¡é“")
	t.Logf("   â€¢ éœ€è¦æˆç†Ÿçš„ç”Ÿæ€ç³»ç»Ÿå’Œå·¥å…·é“¾")
	t.Logf("   â€¢ æ•°æ®é‡å¤§ä¸”éœ€è¦é•¿æœŸä¿å­˜")

	t.Logf("\nğŸ”µ é€‰æ‹© NATS JetStream å½“:")
	t.Logf("   â€¢ éœ€è¦æè‡´çš„æ€§èƒ½å’Œä½å»¶è¿Ÿ")
	t.Logf("   â€¢ éœ€è¦ç®€å•çš„éƒ¨ç½²å’Œè¿ç»´")
	t.Logf("   â€¢ éœ€è¦è½»é‡çº§çš„æ¶ˆæ¯ä¼ é€’")
	t.Logf("   â€¢ æ„å»ºå®æ—¶åº”ç”¨å’Œå¾®æœåŠ¡")

	t.Log("\n" + strings.Repeat("=", 100))
	t.Logf("âœ… æµ‹è¯•å®Œæˆï¼å…±æ‰§è¡Œ %d ä¸ªåœºæ™¯ï¼Œ%d æ¬¡æµ‹è¯•", len(kafkaResults), len(kafkaResults)*2)
	t.Log(strings.Repeat("=", 100))
}
