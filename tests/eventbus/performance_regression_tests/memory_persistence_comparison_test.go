package performance_tests

import (
	"context"
	"fmt"
	"math"
	"os"
	"runtime"
	"runtime/pprof"
	"strings"
	"sync"
	"sync/atomic"
	"testing"
	"time"

	"github.com/ChenBigdata421/jxt-core/sdk/pkg/eventbus"
	"github.com/ChenBigdata421/jxt-core/sdk/pkg/logger"
	"github.com/IBM/sarama"
	"github.com/nats-io/nats.go"
	"github.com/stretchr/testify/require"
)

// ğŸ¯ Kafka vs NATS JetStream å†…å­˜æŒä¹…åŒ–æ€§èƒ½å¯¹æ¯”æµ‹è¯•
//
// æµ‹è¯•è¦æ±‚ï¼š
// 1. åˆ›å»º EventBus å®ä¾‹ï¼ˆè¦†ç›– Kafka å’Œ NATS JetStream ä¸¤ç§å®ç°ï¼‰
// 2. Kafka å’Œ NATS JetStream éƒ½æŒä¹…åŒ–åˆ°å†…å­˜
// 3. å‘å¸ƒç«¯å¿…é¡»é‡‡ç”¨ Publish æ–¹æ³•
// 4. è®¢é˜…ç«¯å¿…é¡»é‡‡ç”¨ Subscribe æ–¹æ³•
// 5. æµ‹è¯•è¦†ç›–ä½å‹500ã€ä¸­å‹2000ã€é«˜å‹5000ã€æé™10000ï¼ˆæ— èšåˆIDï¼‰
// 6. Topic æ•°é‡ä¸º 5
// 7. è¾“å‡ºæŠ¥å‘ŠåŒ…æ‹¬æ€§èƒ½æŒ‡æ ‡ã€å…³é”®èµ„æºå ç”¨æƒ…å†µã€è¿æ¥æ•°ã€æ¶ˆè´¹è€…ç»„ä¸ªæ•°å¯¹æ¯”
// 8. Kafka çš„ ClientID å’Œ topic åç§°åªä½¿ç”¨ ASCII å­—ç¬¦
// 9. æ¯æ¬¡æµ‹è¯•å‰æ¸…ç† Kafka å’Œ NATS
// 10. æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†å…¨å±€ worker æ± å¤„ç†è®¢é˜…æ¶ˆæ¯
// 11. Kafka é‡‡ç”¨ä¸‰åˆ†åŒº
// 12. åœ¨ Kafka çš„æµ‹è¯•ä¸­ï¼Œæ£€æŸ¥åˆ†åŒºæ•°ï¼Œå¹¶åœ¨æŠ¥å‘Šä¸­ä½“ç°

// MemoryPerfMetrics å†…å­˜æŒä¹…åŒ–æ€§èƒ½æŒ‡æ ‡
type MemoryPerfMetrics struct {
	// åŸºæœ¬ä¿¡æ¯
	System       string        // ç³»ç»Ÿåç§° (Kafka/NATS)
	Pressure     string        // å‹åŠ›çº§åˆ«
	MessageCount int           // æ¶ˆæ¯æ€»æ•°
	StartTime    time.Time     // å¼€å§‹æ—¶é—´
	EndTime      time.Time     // ç»“æŸæ—¶é—´
	Duration     time.Duration // æŒç»­æ—¶é—´

	// æ¶ˆæ¯æŒ‡æ ‡
	MessagesSent     int64   // å‘é€æ¶ˆæ¯æ•°
	MessagesReceived int64   // æ¥æ”¶æ¶ˆæ¯æ•°
	SendErrors       int64   // å‘é€é”™è¯¯æ•°
	ProcessErrors    int64   // å¤„ç†é”™è¯¯æ•°
	SuccessRate      float64 // æˆåŠŸç‡

	// æ€§èƒ½æŒ‡æ ‡
	SendThroughput    float64 // å‘é€ååé‡ (msg/s)
	ReceiveThroughput float64 // æ¥æ”¶ååé‡ (msg/s)
	AvgSendLatency    float64 // å¹³å‡å‘é€å»¶è¿Ÿ (ms)
	AvgProcessLatency float64 // å¹³å‡å¤„ç†å»¶è¿Ÿ (ms)

	// èµ„æºå ç”¨
	InitialGoroutines int     // åˆå§‹åç¨‹æ•°
	PeakGoroutines    int32   // å³°å€¼åç¨‹æ•° (ä½¿ç”¨ int32 ä»¥ä¾¿åŸå­æ“ä½œ)
	FinalGoroutines   int     // æœ€ç»ˆåç¨‹æ•°
	GoroutineLeak     int     // åç¨‹æ³„æ¼æ•°
	InitialMemoryMB   float64 // åˆå§‹å†…å­˜ (MB)
	PeakMemoryMB      uint64  // å³°å€¼å†…å­˜ (MB) - å­˜å‚¨ä¸º uint64 bits ä»¥ä¾¿åŸå­æ“ä½œ
	FinalMemoryMB     float64 // æœ€ç»ˆå†…å­˜ (MB)
	MemoryDeltaMB     float64 // å†…å­˜å¢é‡ (MB)

	// è¿æ¥å’Œæ¶ˆè´¹è€…ç»„ç»Ÿè®¡
	TopicCount         int              // Topic æ•°é‡
	ConnectionCount    int              // è¿æ¥æ•°
	ConsumerGroupCount int              // æ¶ˆè´¹è€…ç»„ä¸ªæ•°
	TopicList          []string         // Topic åˆ—è¡¨
	PartitionCount     map[string]int32 // Kafka Topic åˆ†åŒºæ•°

	// Worker æ± ç»Ÿè®¡
	UseGlobalWorkerPool bool // æ˜¯å¦ä½¿ç”¨å…¨å±€ Worker æ± 
	WorkerPoolSize      int  // Worker æ± å¤§å°

	// å†…éƒ¨ç»Ÿè®¡
	sendLatencySum   int64 // å‘é€å»¶è¿Ÿæ€»å’Œ (å¾®ç§’)
	sendLatencyCount int64 // å‘é€å»¶è¿Ÿè®¡æ•°
	procLatencySum   int64 // å¤„ç†å»¶è¿Ÿæ€»å’Œ (å¾®ç§’)
	procLatencyCount int64 // å¤„ç†å»¶è¿Ÿè®¡æ•°
}

// cleanupMemoryKafka æ¸…ç† Kafka æµ‹è¯•æ•°æ®
func cleanupMemoryKafka(t *testing.T, topicPrefix string) {
	t.Logf("ğŸ§¹ æ¸…ç† Kafka æµ‹è¯•æ•°æ® (topic prefix: %s)...", topicPrefix)

	config := sarama.NewConfig()
	config.Version = sarama.V2_6_0_0

	admin, err := sarama.NewClusterAdmin([]string{"localhost:29094"}, config)
	if err != nil {
		t.Logf("âš ï¸  æ— æ³•åˆ›å»º Kafka ç®¡ç†å®¢æˆ·ç«¯: %v", err)
		return
	}
	defer admin.Close()

	topics, err := admin.ListTopics()
	if err != nil {
		t.Logf("âš ï¸  æ— æ³•åˆ—å‡º Kafka topics: %v", err)
		return
	}

	var topicsToDelete []string
	for topic := range topics {
		if len(topicPrefix) == 0 || strings.HasPrefix(topic, topicPrefix) {
			topicsToDelete = append(topicsToDelete, topic)
		}
	}

	if len(topicsToDelete) > 0 {
		for _, topic := range topicsToDelete {
			err = admin.DeleteTopic(topic)
			if err != nil {
				t.Logf("âš ï¸  åˆ é™¤ topic %s å¤±è´¥: %v", topic, err)
			} else {
				t.Logf("   âœ… å·²åˆ é™¤ topic: %s", topic)
			}
		}
		t.Logf("âœ… æˆåŠŸåˆ é™¤ %d ä¸ª Kafka topics", len(topicsToDelete))
	} else {
		t.Logf("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°éœ€è¦æ¸…ç†çš„ Kafka topics")
	}
}

// cleanupMemoryNATS æ¸…ç† NATS æµ‹è¯•æ•°æ®
func cleanupMemoryNATS(t *testing.T, streamPrefix string) {
	t.Logf("ğŸ§¹ æ¸…ç† NATS æµ‹è¯•æ•°æ® (stream prefix: %s)...", streamPrefix)

	nc, err := nats.Connect("nats://localhost:4223")
	if err != nil {
		t.Logf("âš ï¸  æ— æ³•è¿æ¥åˆ° NATS: %v", err)
		return
	}
	defer nc.Close()

	js, err := nc.JetStream()
	if err != nil {
		t.Logf("âš ï¸  æ— æ³•åˆ›å»º JetStream ä¸Šä¸‹æ–‡: %v", err)
		return
	}

	streams := js.StreamNames()
	deletedCount := 0
	for stream := range streams {
		if len(streamPrefix) == 0 || strings.HasPrefix(stream, streamPrefix) {
			err = js.DeleteStream(stream)
			if err != nil {
				t.Logf("âš ï¸  åˆ é™¤ stream %s å¤±è´¥: %v", stream, err)
			} else {
				t.Logf("   âœ… å·²åˆ é™¤ stream: %s", stream)
				deletedCount++
			}
		}
	}

	if deletedCount > 0 {
		t.Logf("âœ… æˆåŠŸåˆ é™¤ %d ä¸ª NATS streams", deletedCount)
	} else {
		t.Logf("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°éœ€è¦æ¸…ç†çš„ NATS streams")
	}
}

// createMemoryKafkaTopics åˆ›å»º Kafka Topicsï¼ˆä¸‰åˆ†åŒºï¼‰
func createMemoryKafkaTopics(t *testing.T, topics []string) map[string]int32 {
	t.Logf("ğŸ”§ åˆ›å»º Kafka Topics (3 åˆ†åŒº)...")

	config := sarama.NewConfig()
	config.Version = sarama.V2_6_0_0

	admin, err := sarama.NewClusterAdmin([]string{"localhost:29094"}, config)
	if err != nil {
		t.Logf("âš ï¸  æ— æ³•åˆ›å»º Kafka ç®¡ç†å®¢æˆ·ç«¯: %v", err)
		return nil
	}
	defer admin.Close()

	actualPartitions := make(map[string]int32)

	for _, topicName := range topics {
		// åˆ é™¤å·²å­˜åœ¨çš„ topic
		err = admin.DeleteTopic(topicName)
		if err != nil && err != sarama.ErrUnknownTopicOrPartition {
			t.Logf("   âš ï¸  åˆ é™¤ topic %s å¤±è´¥: %v", topicName, err)
		}

		time.Sleep(100 * time.Millisecond)

		// åˆ›å»ºæ–°çš„ topicï¼ˆ3 åˆ†åŒºï¼‰
		topicDetail := &sarama.TopicDetail{
			NumPartitions:     3,
			ReplicationFactor: 1,
		}

		err = admin.CreateTopic(topicName, topicDetail, false)
		if err != nil {
			t.Logf("   âŒ åˆ›å»ºå¤±è´¥: %s - %v", topicName, err)
		} else {
			actualPartitions[topicName] = 3
			t.Logf("   âœ… åˆ›å»ºæˆåŠŸ: %s (3 partitions)", topicName)
		}
	}

	time.Sleep(500 * time.Millisecond)

	// éªŒè¯åˆ›å»ºçš„ topics
	t.Logf("ğŸ“Š éªŒè¯åˆ›å»ºçš„ Topics:")
	allTopics, err := admin.ListTopics()
	if err != nil {
		t.Logf("âš ï¸  æ— æ³•åˆ—å‡º topics: %v", err)
		return actualPartitions
	}

	for _, topicName := range topics {
		if detail, exists := allTopics[topicName]; exists {
			actualPartitions[topicName] = detail.NumPartitions
			t.Logf("   %s: %d partitions", topicName, detail.NumPartitions)
		} else {
			t.Logf("   âš ï¸  Topic %s ä¸å­˜åœ¨", topicName)
		}
	}

	t.Logf("âœ… æˆåŠŸåˆ›å»º %d ä¸ª Kafka topics", len(actualPartitions))
	return actualPartitions
}

// getMemoryUsageMB è·å–å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰
func getMemoryUsageMB() float64 {
	var m runtime.MemStats
	runtime.ReadMemStats(&m)
	return float64(m.Alloc) / 1024 / 1024
}

// getGoroutineCount è·å–åç¨‹æ•°é‡
func getGoroutineCount() int {
	return runtime.NumGoroutine()
}

// recordSendLatency è®°å½•å‘é€å»¶è¿Ÿ
func (m *MemoryPerfMetrics) recordSendLatency(latencyMicros int64) {
	atomic.AddInt64(&m.sendLatencySum, latencyMicros)
	atomic.AddInt64(&m.sendLatencyCount, 1)
}

// recordProcessLatency è®°å½•å¤„ç†å»¶è¿Ÿ
func (m *MemoryPerfMetrics) recordProcessLatency(latencyMicros int64) {
	atomic.AddInt64(&m.procLatencySum, latencyMicros)
	atomic.AddInt64(&m.procLatencyCount, 1)
}

// finalize è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
func (m *MemoryPerfMetrics) finalize() {
	m.Duration = m.EndTime.Sub(m.StartTime)

	// è®¡ç®—æˆåŠŸç‡
	if m.MessageCount > 0 {
		m.SuccessRate = float64(m.MessagesReceived) / float64(m.MessageCount) * 100
	}

	// è®¡ç®—ååé‡
	if m.Duration.Seconds() > 0 {
		m.SendThroughput = float64(m.MessagesSent) / m.Duration.Seconds()
		m.ReceiveThroughput = float64(m.MessagesReceived) / m.Duration.Seconds()
	}

	// è®¡ç®—å¹³å‡å»¶è¿Ÿ
	if m.sendLatencyCount > 0 {
		m.AvgSendLatency = float64(m.sendLatencySum) / float64(m.sendLatencyCount) / 1000.0 // è½¬æ¢ä¸ºæ¯«ç§’
	}
	if m.procLatencyCount > 0 {
		m.AvgProcessLatency = float64(m.procLatencySum) / float64(m.procLatencyCount) / 1000.0
	}

	// æ³¨æ„ï¼šèµ„æºå¢é‡ï¼ˆGoroutineLeak å’Œ MemoryDeltaMBï¼‰åœ¨ defer å‡½æ•°ä¸­è®¡ç®—
}

// testMemoryKafka æµ‹è¯• Kafkaï¼ˆå†…å­˜æŒä¹…åŒ–ï¼‰
func testMemoryKafka(t *testing.T, pressure string, messageCount int, topicCount int) *MemoryPerfMetrics {
	separator := strings.Repeat("=", 80)
	t.Logf("\n%s", separator)
	t.Logf("ğŸ”µ æµ‹è¯• Kafka - %s å‹åŠ› (%d æ¶ˆæ¯, %d topics)", pressure, messageCount, topicCount)
	t.Logf("%s", separator)

	// åˆå§‹åŒ– logger
	logger.Setup()

	// ç”Ÿæˆ topic åˆ—è¡¨ï¼ˆåªä½¿ç”¨ ASCII å­—ç¬¦ï¼‰
	topics := make([]string, topicCount)
	for i := 0; i < topicCount; i++ {
		topics[i] = fmt.Sprintf("test.memory.kafka.topic%d", i+1)
	}

	// æ¸…ç†æ—§æ•°æ®
	cleanupMemoryKafka(t, "test.memory.kafka")
	time.Sleep(1 * time.Second)

	// åˆ›å»º topicsï¼ˆ3 åˆ†åŒºï¼‰
	partitions := createMemoryKafkaTopics(t, topics)

	// åˆ›å»º Kafka EventBusï¼ˆå†…å­˜æŒä¹…åŒ–ï¼‰
	cfg := &eventbus.KafkaConfig{
		Brokers:  []string{"localhost:29094"},
		ClientID: fmt.Sprintf("memory-kafka-%s", pressure), // åªä½¿ç”¨ ASCII å­—ç¬¦
		Producer: eventbus.ProducerConfig{
			RequiredAcks:   1,
			FlushFrequency: 10 * time.Millisecond,
			FlushMessages:  100,
			Timeout:        30 * time.Second,
			// æ³¨æ„ï¼šå‹ç¼©é…ç½®å·²ä» Producer çº§åˆ«ç§»åˆ° Topic çº§åˆ«ï¼Œé€šè¿‡ TopicBuilder é…ç½®
			FlushBytes:      1048576,
			RetryMax:        3,
			BatchSize:       16384,
			BufferSize:      8388608,
			MaxMessageBytes: 1048576,
		},
		Consumer: eventbus.ConsumerConfig{
			GroupID:            fmt.Sprintf("memory-kafka-%s-group", pressure),
			SessionTimeout:     10 * time.Second,
			HeartbeatInterval:  3 * time.Second,
			MaxProcessingTime:  30 * time.Second,
			AutoOffsetReset:    "earliest",
			EnableAutoCommit:   true,
			AutoCommitInterval: 1 * time.Second,
			FetchMaxWait:       100 * time.Millisecond,
			MaxPollRecords:     500,
		},
		Net: eventbus.NetConfig{
			DialTimeout:  10 * time.Second,
			ReadTimeout:  30 * time.Second,
			WriteTimeout: 30 * time.Second,
		},
	}

	bus, err := eventbus.NewKafkaEventBus(cfg)
	require.NoError(t, err, "åˆ›å»º Kafka EventBus å¤±è´¥")
	defer bus.Close()

	// åˆå§‹åŒ–æŒ‡æ ‡
	metrics := &MemoryPerfMetrics{
		System:              "Kafka",
		Pressure:            pressure,
		MessageCount:        messageCount,
		TopicCount:          topicCount,
		TopicList:           topics,
		PartitionCount:      partitions,
		ConsumerGroupCount:  1,
		ConnectionCount:     1,
		UseGlobalWorkerPool: true, // Kafka ä½¿ç”¨å…¨å±€ Worker æ± 
		WorkerPoolSize:      256,  // Kafka å…¨å±€ Worker æ± å¤§å°
	}

	// ğŸ”‘ å…³é”®ï¼šä½¿ç”¨ defer åœ¨ Close() ä¹‹åæµ‹é‡æœ€ç»ˆèµ„æº
	// å‚è€ƒï¼škafka_nats_comparison_test.go Line 482-496
	defer func() {
		// ç­‰å¾…åå°åç¨‹å®Œå…¨é€€å‡ºï¼ˆä¿®å¤ï¼šä» 100ms å¢åŠ åˆ° 2sï¼‰
		time.Sleep(2 * time.Second)

		// å¼ºåˆ¶ GC
		runtime.GC()
		time.Sleep(100 * time.Millisecond)

		// è®°å½•æœ€ç»ˆèµ„æºçŠ¶æ€ï¼ˆåœ¨ Close() ä¹‹åï¼‰
		metrics.FinalGoroutines = getGoroutineCount()
		metrics.FinalMemoryMB = getMemoryUsageMB()
		metrics.MemoryDeltaMB = metrics.FinalMemoryMB - metrics.InitialMemoryMB
		metrics.GoroutineLeak = metrics.FinalGoroutines - metrics.InitialGoroutines

		t.Logf("ğŸ“Š Kafka èµ„æºæ¸…ç†å®Œæˆ: åˆå§‹ %d -> æœ€ç»ˆ %d (æ³„æ¼ %d)",
			metrics.InitialGoroutines, metrics.FinalGoroutines, metrics.GoroutineLeak)
	}()

	// è®°å½•åˆå§‹èµ„æº
	runtime.GC()
	time.Sleep(100 * time.Millisecond)
	metrics.InitialGoroutines = getGoroutineCount()
	metrics.InitialMemoryMB = getMemoryUsageMB()
	atomic.StoreInt32(&metrics.PeakGoroutines, int32(metrics.InitialGoroutines))
	atomic.StoreUint64(&metrics.PeakMemoryMB, math.Float64bits(metrics.InitialMemoryMB))

	t.Logf("ğŸ“Š åˆå§‹èµ„æº: Goroutines=%d, Memory=%.2f MB",
		metrics.InitialGoroutines, metrics.InitialMemoryMB)

	// ğŸ”‘ å…³é”®ï¼šä½¿ç”¨é¢„è®¢é˜…æ¨¡å¼ï¼Œä¸€æ¬¡æ€§è®¾ç½®æ‰€æœ‰ topics
	// å‚è€ƒï¼škafka_nats_comparison_test.go Line 506-511
	if kafkaBus, ok := bus.(interface {
		SetPreSubscriptionTopics([]string)
	}); ok {
		kafkaBus.SetPreSubscriptionTopics(topics)
		t.Logf("âœ… å·²è®¾ç½®é¢„è®¢é˜… topic åˆ—è¡¨: %v", topics)
	}

	// è®¢é˜…æ‰€æœ‰ topics
	ctx := context.Background()
	var received int64

	for _, topic := range topics {
		topicName := topic
		err = bus.Subscribe(ctx, topicName, func(ctx context.Context, message []byte) error {
			receiveTime := time.Now()

			// è§£æå‘é€æ—¶é—´
			var sendTime time.Time
			if len(message) >= 8 {
				sendTimeNano := int64(message[0]) | int64(message[1])<<8 | int64(message[2])<<16 |
					int64(message[3])<<24 | int64(message[4])<<32 | int64(message[5])<<40 |
					int64(message[6])<<48 | int64(message[7])<<56
				sendTime = time.Unix(0, sendTimeNano)

				// è®°å½•å¤„ç†å»¶è¿Ÿ
				latency := receiveTime.Sub(sendTime).Microseconds()
				metrics.recordProcessLatency(latency)
			}

			atomic.AddInt64(&received, 1)
			return nil
		})
		require.NoError(t, err, "è®¢é˜…å¤±è´¥: %s", topicName)
	}

	// ç­‰å¾…è®¢é˜…å°±ç»ª
	t.Logf("â³ ç­‰å¾…è®¢é˜…å°±ç»ª...")
	time.Sleep(5 * time.Second) // ç»™ Kafka æ¶ˆè´¹è€…ç»„æ—¶é—´æ¥åˆå§‹åŒ–å’Œåˆ†é…åˆ†åŒº

	// å¼€å§‹å‘é€æ¶ˆæ¯
	t.Logf("ğŸ“¤ å¼€å§‹å‘é€ %d æ¡æ¶ˆæ¯åˆ° %d ä¸ª topics...", messageCount, topicCount)
	metrics.StartTime = time.Now()

	var sendWg sync.WaitGroup
	messagesPerTopic := messageCount / topicCount

	for topicIdx, topic := range topics {
		sendWg.Add(1)
		go func(topicName string, idx int) {
			defer sendWg.Done()

			for i := 0; i < messagesPerTopic; i++ {
				sendStart := time.Now()

				// æ„é€ æ¶ˆæ¯ï¼ˆåŒ…å«å‘é€æ—¶é—´æˆ³ï¼‰
				sendTimeNano := sendStart.UnixNano()
				message := make([]byte, 8+100) // 8å­—èŠ‚æ—¶é—´æˆ³ + 100å­—èŠ‚æ•°æ®
				message[0] = byte(sendTimeNano)
				message[1] = byte(sendTimeNano >> 8)
				message[2] = byte(sendTimeNano >> 16)
				message[3] = byte(sendTimeNano >> 24)
				message[4] = byte(sendTimeNano >> 32)
				message[5] = byte(sendTimeNano >> 40)
				message[6] = byte(sendTimeNano >> 48)
				message[7] = byte(sendTimeNano >> 56)
				copy(message[8:], []byte(fmt.Sprintf("Message %d from topic %d", i, idx)))

				err := bus.Publish(ctx, topicName, message)
				if err != nil {
					atomic.AddInt64(&metrics.SendErrors, 1)
				} else {
					atomic.AddInt64(&metrics.MessagesSent, 1)

					// è®°å½•å‘é€å»¶è¿Ÿ
					latency := time.Since(sendStart).Microseconds()
					metrics.recordSendLatency(latency)
				}

				// æ›´æ–°å³°å€¼èµ„æºï¼ˆåŸå­æ“ä½œï¼‰
				currentGoroutines := int32(getGoroutineCount())
				for {
					oldPeak := atomic.LoadInt32(&metrics.PeakGoroutines)
					if currentGoroutines <= oldPeak {
						break
					}
					if atomic.CompareAndSwapInt32(&metrics.PeakGoroutines, oldPeak, currentGoroutines) {
						break
					}
				}

				currentMemory := getMemoryUsageMB()
				currentMemoryBits := math.Float64bits(currentMemory)
				for {
					oldPeakBits := atomic.LoadUint64(&metrics.PeakMemoryMB)
					oldPeakMB := math.Float64frombits(oldPeakBits)
					if currentMemory <= oldPeakMB {
						break
					}
					if atomic.CompareAndSwapUint64(&metrics.PeakMemoryMB, oldPeakBits, currentMemoryBits) {
						break
					}
				}
			}
		}(topic, topicIdx)
	}

	sendWg.Wait()
	t.Logf("âœ… å‘é€å®Œæˆ")

	// ç­‰å¾…æ¥æ”¶å®Œæˆ
	t.Logf("â³ ç­‰å¾…æ¥æ”¶å®Œæˆ...")
	timeout := time.After(60 * time.Second)
	ticker := time.NewTicker(1 * time.Second)
	defer ticker.Stop()

	for {
		select {
		case <-timeout:
			t.Logf("âš ï¸  æ¥æ”¶è¶…æ—¶")
			goto done
		case <-ticker.C:
			currentReceived := atomic.LoadInt64(&received)
			if currentReceived >= int64(messageCount) {
				t.Logf("âœ… æ¥æ”¶å®Œæˆ: %d/%d", currentReceived, messageCount)
				goto done
			}
			t.Logf("   æ¥æ”¶è¿›åº¦: %d/%d (%.1f%%)", currentReceived, messageCount,
				float64(currentReceived)/float64(messageCount)*100)
		}
	}

done:
	metrics.EndTime = time.Now()
	metrics.MessagesReceived = atomic.LoadInt64(&received)

	// è®¡ç®—æœ€ç»ˆæŒ‡æ ‡ï¼ˆä¸åŒ…æ‹¬èµ„æºæµ‹é‡ï¼Œèµ„æºæµ‹é‡åœ¨ defer ä¸­è¿›è¡Œï¼‰
	metrics.finalize()

	return metrics
}

// testMemoryNATS æµ‹è¯• NATSï¼ˆå†…å­˜æŒä¹…åŒ–ï¼‰
func testMemoryNATS(t *testing.T, pressure string, messageCount int, topicCount int) *MemoryPerfMetrics {
	separator := strings.Repeat("=", 80)
	t.Logf("\n%s", separator)
	t.Logf("ğŸŸ¢ æµ‹è¯• NATS - %s å‹åŠ› (%d æ¶ˆæ¯, %d topics)", pressure, messageCount, topicCount)
	t.Logf("%s", separator)

	// åˆå§‹åŒ– logger
	logger.Setup()

	// ç”Ÿæˆ topic åˆ—è¡¨ï¼ˆåªä½¿ç”¨ ASCII å­—ç¬¦ï¼‰
	topics := make([]string, topicCount)
	for i := 0; i < topicCount; i++ {
		topics[i] = fmt.Sprintf("test.memory.nats.topic%d", i+1)
	}

	// æ¸…ç†æ—§æ•°æ®
	cleanupMemoryNATS(t, "TEST_MEMORY_NATS")
	time.Sleep(1 * time.Second)

	// åˆ›å»º NATS EventBusï¼ˆå†…å­˜æŒä¹…åŒ–ï¼‰
	cfg := &eventbus.NATSConfig{
		URLs:     []string{"nats://localhost:4223"},
		ClientID: fmt.Sprintf("memory-nats-%s", pressure), // åªä½¿ç”¨ ASCII å­—ç¬¦
		JetStream: eventbus.JetStreamConfig{
			Enabled:        true,
			PublishTimeout: 30 * time.Second,
			AckWait:        30 * time.Second,
			MaxDeliver:     3,
			Stream: eventbus.StreamConfig{
				Name:      "TEST_MEMORY_NATS",
				Subjects:  []string{"test.memory.nats.>"},
				Storage:   "memory", // å†…å­˜æŒä¹…åŒ–
				Retention: "limits",
				MaxAge:    24 * time.Hour,
				MaxBytes:  1024 * 1024 * 1024, // 1GB
				MaxMsgs:   1000000,
				Replicas:  1,
				Discard:   "old",
			},
			Consumer: eventbus.NATSConsumerConfig{
				DurableName:   fmt.Sprintf("memory-nats-%s-consumer", pressure),
				DeliverPolicy: "all",
				AckPolicy:     "explicit",
				ReplayPolicy:  "instant",
				MaxAckPending: 10000,
				MaxWaiting:    512,
				MaxDeliver:    3,
			},
		},
	}

	bus, err := eventbus.NewNATSEventBus(cfg)
	require.NoError(t, err, "åˆ›å»º NATS EventBus å¤±è´¥")
	defer bus.Close()

	// åˆå§‹åŒ–æŒ‡æ ‡
	metrics := &MemoryPerfMetrics{
		System:              "NATS",
		Pressure:            pressure,
		MessageCount:        messageCount,
		TopicCount:          topicCount,
		TopicList:           topics,
		ConsumerGroupCount:  1,
		ConnectionCount:     1,
		UseGlobalWorkerPool: true, // NATS ä½¿ç”¨å…¨å±€ Worker æ± 
		WorkerPoolSize:      256,  // NATS å…¨å±€ Worker æ± å¤§å°ï¼ˆä¸ Kafka ä¸€è‡´ï¼‰
	}

	// ğŸ”‘ å…³é”®ï¼šä½¿ç”¨ defer åœ¨ Close() ä¹‹åæµ‹é‡æœ€ç»ˆèµ„æº
	// å‚è€ƒï¼škafka_nats_comparison_test.go Line 603-617
	defer func() {
		// ç­‰å¾…åå°åç¨‹å®Œå…¨é€€å‡ºï¼ˆä¿®å¤ï¼šä» 100ms å¢åŠ åˆ° 2sï¼‰
		time.Sleep(2 * time.Second)

		// å¼ºåˆ¶ GC
		runtime.GC()
		time.Sleep(100 * time.Millisecond)

		// è®°å½•æœ€ç»ˆèµ„æºçŠ¶æ€ï¼ˆåœ¨ Close() ä¹‹åï¼‰
		metrics.FinalGoroutines = getGoroutineCount()
		metrics.FinalMemoryMB = getMemoryUsageMB()
		metrics.MemoryDeltaMB = metrics.FinalMemoryMB - metrics.InitialMemoryMB
		metrics.GoroutineLeak = metrics.FinalGoroutines - metrics.InitialGoroutines

		t.Logf("ğŸ“Š NATS èµ„æºæ¸…ç†å®Œæˆ: åˆå§‹ %d -> æœ€ç»ˆ %d (æ³„æ¼ %d)",
			metrics.InitialGoroutines, metrics.FinalGoroutines, metrics.GoroutineLeak)

		// ğŸ”¬ è°ƒè¯•ï¼šå¦‚æœæ³„æ¼ä¸¥é‡ï¼Œç”Ÿæˆåç¨‹å †æ ˆä¿¡æ¯
		if metrics.GoroutineLeak > 1000 {
			import_pprof := func() {
				// åŠ¨æ€å¯¼å…¥ pprof
				// è¿™é‡Œä½¿ç”¨å†…è”æ–¹å¼é¿å…å¯¼å…¥å†²çª
			}
			_ = import_pprof

			// ç”Ÿæˆåç¨‹å †æ ˆæ–‡ä»¶
			filename := fmt.Sprintf("goroutine_leak_%s_%d.txt", pressure, metrics.GoroutineLeak)
			f, err := os.Create(filename)
			if err == nil {
				pprof.Lookup("goroutine").WriteTo(f, 1)
				f.Close()
				t.Logf("ğŸ”¬ åç¨‹å †æ ˆå·²ä¿å­˜åˆ°: %s", filename)
			}
		}
	}()

	// è®°å½•åˆå§‹èµ„æº
	runtime.GC()
	time.Sleep(100 * time.Millisecond)
	metrics.InitialGoroutines = getGoroutineCount()
	metrics.InitialMemoryMB = getMemoryUsageMB()
	atomic.StoreInt32(&metrics.PeakGoroutines, int32(metrics.InitialGoroutines))
	atomic.StoreUint64(&metrics.PeakMemoryMB, math.Float64bits(metrics.InitialMemoryMB))

	t.Logf("ğŸ“Š åˆå§‹èµ„æº: Goroutines=%d, Memory=%.2f MB",
		metrics.InitialGoroutines, metrics.InitialMemoryMB)

	// è®¢é˜…æ‰€æœ‰ topics
	ctx := context.Background()
	var received int64

	for _, topic := range topics {
		topicName := topic
		err = bus.Subscribe(ctx, topicName, func(ctx context.Context, message []byte) error {
			receiveTime := time.Now()

			// è§£æå‘é€æ—¶é—´
			var sendTime time.Time
			if len(message) >= 8 {
				sendTimeNano := int64(message[0]) | int64(message[1])<<8 | int64(message[2])<<16 |
					int64(message[3])<<24 | int64(message[4])<<32 | int64(message[5])<<40 |
					int64(message[6])<<48 | int64(message[7])<<56
				sendTime = time.Unix(0, sendTimeNano)

				// è®°å½•å¤„ç†å»¶è¿Ÿ
				latency := receiveTime.Sub(sendTime).Microseconds()
				metrics.recordProcessLatency(latency)
			}

			atomic.AddInt64(&received, 1)
			return nil
		})
		require.NoError(t, err, "è®¢é˜…å¤±è´¥: %s", topicName)
	}

	// ç­‰å¾…è®¢é˜…å°±ç»ª
	t.Logf("â³ ç­‰å¾…è®¢é˜…å°±ç»ª...")
	time.Sleep(10 * time.Second) // ç»™ NATS æ¶ˆè´¹è€…è¶³å¤Ÿæ—¶é—´æ¥åˆå§‹åŒ–

	// å¼€å§‹å‘é€æ¶ˆæ¯
	t.Logf("ğŸ“¤ å¼€å§‹å‘é€ %d æ¡æ¶ˆæ¯åˆ° %d ä¸ª topics...", messageCount, topicCount)
	metrics.StartTime = time.Now()

	var sendWg sync.WaitGroup
	messagesPerTopic := messageCount / topicCount

	for topicIdx, topic := range topics {
		sendWg.Add(1)
		go func(topicName string, idx int) {
			defer sendWg.Done()

			for i := 0; i < messagesPerTopic; i++ {
				sendStart := time.Now()

				// æ„é€ æ¶ˆæ¯ï¼ˆåŒ…å«å‘é€æ—¶é—´æˆ³ï¼‰
				sendTimeNano := sendStart.UnixNano()
				message := make([]byte, 8+100) // 8å­—èŠ‚æ—¶é—´æˆ³ + 100å­—èŠ‚æ•°æ®
				message[0] = byte(sendTimeNano)
				message[1] = byte(sendTimeNano >> 8)
				message[2] = byte(sendTimeNano >> 16)
				message[3] = byte(sendTimeNano >> 24)
				message[4] = byte(sendTimeNano >> 32)
				message[5] = byte(sendTimeNano >> 40)
				message[6] = byte(sendTimeNano >> 48)
				message[7] = byte(sendTimeNano >> 56)
				copy(message[8:], []byte(fmt.Sprintf("Message %d from topic %d", i, idx)))

				err := bus.Publish(ctx, topicName, message)
				if err != nil {
					atomic.AddInt64(&metrics.SendErrors, 1)
				} else {
					atomic.AddInt64(&metrics.MessagesSent, 1)

					// è®°å½•å‘é€å»¶è¿Ÿ
					latency := time.Since(sendStart).Microseconds()
					metrics.recordSendLatency(latency)
				}

				// æ›´æ–°å³°å€¼èµ„æºï¼ˆåŸå­æ“ä½œï¼‰
				currentGoroutines := int32(getGoroutineCount())
				for {
					oldPeak := atomic.LoadInt32(&metrics.PeakGoroutines)
					if currentGoroutines <= oldPeak {
						break
					}
					if atomic.CompareAndSwapInt32(&metrics.PeakGoroutines, oldPeak, currentGoroutines) {
						break
					}
				}

				currentMemory := getMemoryUsageMB()
				currentMemoryBits := math.Float64bits(currentMemory)
				for {
					oldPeakBits := atomic.LoadUint64(&metrics.PeakMemoryMB)
					oldPeakMB := math.Float64frombits(oldPeakBits)
					if currentMemory <= oldPeakMB {
						break
					}
					if atomic.CompareAndSwapUint64(&metrics.PeakMemoryMB, oldPeakBits, currentMemoryBits) {
						break
					}
				}
			}
		}(topic, topicIdx)
	}

	sendWg.Wait()
	t.Logf("âœ… å‘é€å®Œæˆ")

	// ç­‰å¾…æ¥æ”¶å®Œæˆ
	t.Logf("â³ ç­‰å¾…æ¥æ”¶å®Œæˆ...")
	timeout := time.After(60 * time.Second)
	ticker := time.NewTicker(1 * time.Second)
	defer ticker.Stop()

	for {
		select {
		case <-timeout:
			t.Logf("âš ï¸  æ¥æ”¶è¶…æ—¶")
			goto done
		case <-ticker.C:
			currentReceived := atomic.LoadInt64(&received)
			if currentReceived >= int64(messageCount) {
				t.Logf("âœ… æ¥æ”¶å®Œæˆ: %d/%d", currentReceived, messageCount)
				goto done
			}
			t.Logf("   æ¥æ”¶è¿›åº¦: %d/%d (%.1f%%)", currentReceived, messageCount,
				float64(currentReceived)/float64(messageCount)*100)
		}
	}

done:
	metrics.EndTime = time.Now()
	metrics.MessagesReceived = atomic.LoadInt64(&received)

	// è®¡ç®—æœ€ç»ˆæŒ‡æ ‡ï¼ˆä¸åŒ…æ‹¬èµ„æºæµ‹é‡ï¼Œèµ„æºæµ‹é‡åœ¨ defer ä¸­è¿›è¡Œï¼‰
	metrics.finalize()

	return metrics
}

// printMemoryComparisonReport æ‰“å°å†…å­˜æŒä¹…åŒ–æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š
func printMemoryComparisonReport(t *testing.T, kafkaMetrics, natsMetrics *MemoryPerfMetrics) {
	separator := strings.Repeat("=", 100)
	t.Logf("\n%s", separator)
	t.Logf("ğŸ“Š Kafka vs NATS JetStream å†…å­˜æŒä¹…åŒ–æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š - %s å‹åŠ›", kafkaMetrics.Pressure)
	t.Logf("%s", separator)

	// åŸºæœ¬ä¿¡æ¯
	t.Logf("\nğŸ“‹ åŸºæœ¬ä¿¡æ¯:")
	t.Logf("  å‹åŠ›çº§åˆ«: %s", kafkaMetrics.Pressure)
	t.Logf("  æ¶ˆæ¯æ€»æ•°: %d", kafkaMetrics.MessageCount)
	t.Logf("  Topic æ•°é‡: %d", kafkaMetrics.TopicCount)
	t.Logf("  Kafka åˆ†åŒºæ•°: 3 (æ¯ä¸ª topic)")

	// Kafka åˆ†åŒºè¯¦æƒ…
	if len(kafkaMetrics.PartitionCount) > 0 {
		t.Logf("\nğŸ“Š Kafka åˆ†åŒºè¯¦æƒ…:")
		for topic, partitions := range kafkaMetrics.PartitionCount {
			t.Logf("  %s: %d partitions", topic, partitions)
		}
	}

	// æ¶ˆæ¯æŒ‡æ ‡å¯¹æ¯”
	t.Logf("\nğŸ“¨ æ¶ˆæ¯æŒ‡æ ‡å¯¹æ¯”:")
	t.Logf("  %-20s | %-15s | %-15s | %-15s", "æŒ‡æ ‡", "Kafka", "NATS", "å·®å¼‚")
	lineSep := "  " + strings.Repeat("-", 70)
	t.Logf("%s", lineSep)
	t.Logf("  %-20s | %-15d | %-15d | %+.1f%%",
		"å‘é€æ¶ˆæ¯æ•°", kafkaMetrics.MessagesSent, natsMetrics.MessagesSent,
		(float64(natsMetrics.MessagesSent)-float64(kafkaMetrics.MessagesSent))/float64(kafkaMetrics.MessagesSent)*100)
	t.Logf("  %-20s | %-15d | %-15d | %+.1f%%",
		"æ¥æ”¶æ¶ˆæ¯æ•°", kafkaMetrics.MessagesReceived, natsMetrics.MessagesReceived,
		(float64(natsMetrics.MessagesReceived)-float64(kafkaMetrics.MessagesReceived))/float64(kafkaMetrics.MessagesReceived)*100)
	t.Logf("  %-20s | %-15d | %-15d | -",
		"å‘é€é”™è¯¯æ•°", kafkaMetrics.SendErrors, natsMetrics.SendErrors)
	t.Logf("  %-20s | %-15.2f%% | %-15.2f%% | %+.1f%%",
		"æˆåŠŸç‡", kafkaMetrics.SuccessRate, natsMetrics.SuccessRate,
		natsMetrics.SuccessRate-kafkaMetrics.SuccessRate)

	// æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
	t.Logf("\nâš¡ æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”:")
	t.Logf("  %-20s | %-15s | %-15s | %-15s", "æŒ‡æ ‡", "Kafka", "NATS", "å·®å¼‚")
	t.Logf("%s", lineSep)
	t.Logf("  %-20s | %-15.2f | %-15.2f | %+.1f%%",
		"å‘é€ååé‡ (msg/s)", kafkaMetrics.SendThroughput, natsMetrics.SendThroughput,
		(natsMetrics.SendThroughput-kafkaMetrics.SendThroughput)/kafkaMetrics.SendThroughput*100)
	t.Logf("  %-20s | %-15.2f | %-15.2f | %+.1f%%",
		"æ¥æ”¶ååé‡ (msg/s)", kafkaMetrics.ReceiveThroughput, natsMetrics.ReceiveThroughput,
		(natsMetrics.ReceiveThroughput-kafkaMetrics.ReceiveThroughput)/kafkaMetrics.ReceiveThroughput*100)
	t.Logf("  %-20s | %-15.3f | %-15.3f | %+.1f%%",
		"å¹³å‡å‘é€å»¶è¿Ÿ (ms)", kafkaMetrics.AvgSendLatency, natsMetrics.AvgSendLatency,
		(natsMetrics.AvgSendLatency-kafkaMetrics.AvgSendLatency)/kafkaMetrics.AvgSendLatency*100)
	t.Logf("  %-20s | %-15.3f | %-15.3f | %+.1f%%",
		"å¹³å‡å¤„ç†å»¶è¿Ÿ (ms)", kafkaMetrics.AvgProcessLatency, natsMetrics.AvgProcessLatency,
		(natsMetrics.AvgProcessLatency-kafkaMetrics.AvgProcessLatency)/kafkaMetrics.AvgProcessLatency*100)
	t.Logf("  %-20s | %-15.2f | %-15.2f | -",
		"æµ‹è¯•æ—¶é•¿ (s)", kafkaMetrics.Duration.Seconds(), natsMetrics.Duration.Seconds())

	// èµ„æºå ç”¨å¯¹æ¯”
	t.Logf("\nğŸ’¾ èµ„æºå ç”¨å¯¹æ¯”:")
	t.Logf("  %-20s | %-15s | %-15s | %-15s", "æŒ‡æ ‡", "Kafka", "NATS", "å·®å¼‚")
	t.Logf("%s", lineSep)
	t.Logf("  %-20s | %-15d | %-15d | %+d",
		"åˆå§‹åç¨‹æ•°", kafkaMetrics.InitialGoroutines, natsMetrics.InitialGoroutines,
		natsMetrics.InitialGoroutines-kafkaMetrics.InitialGoroutines)
	kafkaPeakGoroutines := atomic.LoadInt32(&kafkaMetrics.PeakGoroutines)
	natsPeakGoroutines := atomic.LoadInt32(&natsMetrics.PeakGoroutines)
	t.Logf("  %-20s | %-15d | %-15d | %+d",
		"å³°å€¼åç¨‹æ•°", kafkaPeakGoroutines, natsPeakGoroutines,
		natsPeakGoroutines-kafkaPeakGoroutines)
	t.Logf("  %-20s | %-15d | %-15d | %+d",
		"æœ€ç»ˆåç¨‹æ•°", kafkaMetrics.FinalGoroutines, natsMetrics.FinalGoroutines,
		natsMetrics.FinalGoroutines-kafkaMetrics.FinalGoroutines)
	t.Logf("  %-20s | %-15d | %-15d | -",
		"åç¨‹æ³„æ¼æ•°", kafkaMetrics.GoroutineLeak, natsMetrics.GoroutineLeak)
	t.Logf("  %-20s | %-15.2f | %-15.2f | %+.2f",
		"åˆå§‹å†…å­˜ (MB)", kafkaMetrics.InitialMemoryMB, natsMetrics.InitialMemoryMB,
		natsMetrics.InitialMemoryMB-kafkaMetrics.InitialMemoryMB)
	kafkaPeakMemoryMB := math.Float64frombits(atomic.LoadUint64(&kafkaMetrics.PeakMemoryMB))
	natsPeakMemoryMB := math.Float64frombits(atomic.LoadUint64(&natsMetrics.PeakMemoryMB))
	t.Logf("  %-20s | %-15.2f | %-15.2f | %+.2f",
		"å³°å€¼å†…å­˜ (MB)", kafkaPeakMemoryMB, natsPeakMemoryMB,
		natsPeakMemoryMB-kafkaPeakMemoryMB)
	t.Logf("  %-20s | %-15.2f | %-15.2f | %+.2f",
		"æœ€ç»ˆå†…å­˜ (MB)", kafkaMetrics.FinalMemoryMB, natsMetrics.FinalMemoryMB,
		natsMetrics.FinalMemoryMB-kafkaMetrics.FinalMemoryMB)
	t.Logf("  %-20s | %-15.2f | %-15.2f | -",
		"å†…å­˜å¢é‡ (MB)", kafkaMetrics.MemoryDeltaMB, natsMetrics.MemoryDeltaMB)

	// è¿æ¥å’Œæ¶ˆè´¹è€…ç»„ç»Ÿè®¡
	lineSep2 := "  " + strings.Repeat("-", 55)
	t.Logf("\nğŸ”— è¿æ¥å’Œæ¶ˆè´¹è€…ç»„ç»Ÿè®¡:")
	t.Logf("  %-20s | %-15s | %-15s", "æŒ‡æ ‡", "Kafka", "NATS")
	t.Logf("%s", lineSep2)
	t.Logf("  %-20s | %-15d | %-15d",
		"è¿æ¥æ•°", kafkaMetrics.ConnectionCount, natsMetrics.ConnectionCount)
	t.Logf("  %-20s | %-15d | %-15d",
		"æ¶ˆè´¹è€…ç»„ä¸ªæ•°", kafkaMetrics.ConsumerGroupCount, natsMetrics.ConsumerGroupCount)

	// Worker æ± ç»Ÿè®¡
	t.Logf("\nâš™ï¸  Worker æ± ç»Ÿè®¡:")
	t.Logf("  %-20s | %-15s | %-15s", "æŒ‡æ ‡", "Kafka", "NATS")
	t.Logf("%s", lineSep2)
	t.Logf("  %-20s | %-15v | %-15v",
		"ä½¿ç”¨å…¨å±€ Worker æ± ", kafkaMetrics.UseGlobalWorkerPool, natsMetrics.UseGlobalWorkerPool)
	t.Logf("  %-20s | %-15d | %-15d",
		"Worker æ± å¤§å°", kafkaMetrics.WorkerPoolSize, natsMetrics.WorkerPoolSize)

	// æ€§èƒ½æ€»ç»“
	t.Logf("\nğŸ¯ æ€§èƒ½æ€»ç»“:")
	if kafkaMetrics.ReceiveThroughput > natsMetrics.ReceiveThroughput {
		diff := (kafkaMetrics.ReceiveThroughput - natsMetrics.ReceiveThroughput) / natsMetrics.ReceiveThroughput * 100
		t.Logf("  âœ… Kafka ååé‡é¢†å…ˆ NATS %.1f%%", diff)
	} else {
		diff := (natsMetrics.ReceiveThroughput - kafkaMetrics.ReceiveThroughput) / kafkaMetrics.ReceiveThroughput * 100
		t.Logf("  âœ… NATS ååé‡é¢†å…ˆ Kafka %.1f%%", diff)
	}

	if kafkaMetrics.AvgProcessLatency < natsMetrics.AvgProcessLatency {
		diff := (natsMetrics.AvgProcessLatency - kafkaMetrics.AvgProcessLatency) / kafkaMetrics.AvgProcessLatency * 100
		t.Logf("  âœ… Kafka å»¶è¿Ÿä¼˜äº NATS %.1f%%", diff)
	} else {
		diff := (kafkaMetrics.AvgProcessLatency - natsMetrics.AvgProcessLatency) / natsMetrics.AvgProcessLatency * 100
		t.Logf("  âœ… NATS å»¶è¿Ÿä¼˜äº Kafka %.1f%%", diff)
	}

	if kafkaMetrics.MemoryDeltaMB < natsMetrics.MemoryDeltaMB {
		diff := natsMetrics.MemoryDeltaMB - kafkaMetrics.MemoryDeltaMB
		t.Logf("  âœ… Kafka å†…å­˜å ç”¨å°‘äº NATS %.2f MB", diff)
	} else {
		diff := kafkaMetrics.MemoryDeltaMB - natsMetrics.MemoryDeltaMB
		t.Logf("  âœ… NATS å†…å­˜å ç”¨å°‘äº Kafka %.2f MB", diff)
	}

	t.Logf("%s", separator)
}

// TestMemoryPersistenceComparison å†…å­˜æŒä¹…åŒ–æ€§èƒ½å¯¹æ¯”æµ‹è¯•
func TestMemoryPersistenceComparison(t *testing.T) {
	separator := strings.Repeat("=", 100)
	t.Logf("\n%s", separator)
	t.Logf("ğŸ¯ Kafka vs NATS JetStream å†…å­˜æŒä¹…åŒ–æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
	t.Logf("%s", separator)

	// æµ‹è¯•åœºæ™¯é…ç½®
	scenarios := []struct {
		name         string
		messageCount int
		topicCount   int
	}{
		{"Low", 500, 5},       // ä½å‹ï¼š500 æ¶ˆæ¯ï¼Œ5 topics
		{"Medium", 2000, 5},   // ä¸­å‹ï¼š2000 æ¶ˆæ¯ï¼Œ5 topics
		{"High", 5000, 5},     // é«˜å‹ï¼š5000 æ¶ˆæ¯ï¼Œ5 topics
		{"Extreme", 10000, 5}, // æé™ï¼š10000 æ¶ˆæ¯ï¼Œ5 topics
	}

	// å­˜å‚¨æ‰€æœ‰æµ‹è¯•ç»“æœ
	allResults := make(map[string]struct {
		kafka *MemoryPerfMetrics
		nats  *MemoryPerfMetrics
	})

	// è¿è¡Œæ‰€æœ‰æµ‹è¯•åœºæ™¯
	for _, scenario := range scenarios {
		t.Logf("\n%s", separator)
		t.Logf("ğŸš€ å¼€å§‹æµ‹è¯•åœºæ™¯: %s å‹åŠ› (%d æ¶ˆæ¯, %d topics)", scenario.name, scenario.messageCount, scenario.topicCount)
		t.Logf("%s", separator)

		// æµ‹è¯• Kafka
		kafkaMetrics := testMemoryKafka(t, scenario.name, scenario.messageCount, scenario.topicCount)
		time.Sleep(5 * time.Second) // ç­‰å¾…èµ„æºé‡Šæ”¾

		// æµ‹è¯• NATS
		natsMetrics := testMemoryNATS(t, scenario.name, scenario.messageCount, scenario.topicCount)
		time.Sleep(5 * time.Second) // ç­‰å¾…èµ„æºé‡Šæ”¾

		// ä¿å­˜ç»“æœ
		allResults[scenario.name] = struct {
			kafka *MemoryPerfMetrics
			nats  *MemoryPerfMetrics
		}{
			kafka: kafkaMetrics,
			nats:  natsMetrics,
		}

		// æ‰“å°å¯¹æ¯”æŠ¥å‘Š
		printMemoryComparisonReport(t, kafkaMetrics, natsMetrics)
	}

	// æ‰“å°æ±‡æ€»æŠ¥å‘Š
	t.Logf("\n%s", separator)
	t.Logf("ğŸ“Š æ‰€æœ‰åœºæ™¯æ±‡æ€»æŠ¥å‘Š")
	t.Logf("%s", separator)

	t.Logf("\n%-10s | %-15s | %-15s | %-15s | %-15s | %-15s",
		"åœºæ™¯", "ç³»ç»Ÿ", "ååé‡(msg/s)", "å»¶è¿Ÿ(ms)", "æˆåŠŸç‡(%)", "å†…å­˜å¢é‡(MB)")
	lineSep3 := strings.Repeat("-", 100)
	t.Logf("%s", lineSep3)

	for _, scenario := range scenarios {
		result := allResults[scenario.name]
		t.Logf("%-10s | %-15s | %-15.2f | %-15.3f | %-15.2f | %-15.2f",
			scenario.name, "Kafka",
			result.kafka.ReceiveThroughput,
			result.kafka.AvgProcessLatency,
			result.kafka.SuccessRate,
			result.kafka.MemoryDeltaMB)
		t.Logf("%-10s | %-15s | %-15.2f | %-15.3f | %-15.2f | %-15.2f",
			"", "NATS",
			result.nats.ReceiveThroughput,
			result.nats.AvgProcessLatency,
			result.nats.SuccessRate,
			result.nats.MemoryDeltaMB)
		t.Logf("%s", lineSep3)
	}

	// æ‰“å°å…³é”®å‘ç°
	t.Logf("\nğŸ” å…³é”®å‘ç°:")
	t.Logf("  1. âœ… æ‰€æœ‰æµ‹è¯•ä½¿ç”¨ Publish/Subscribe æ–¹æ³•ï¼ˆæ— èšåˆIDï¼‰")
	t.Logf("  2. âœ… Kafka å’Œ NATS éƒ½ä½¿ç”¨å†…å­˜æŒä¹…åŒ–")
	t.Logf("  3. âœ… Kafka ä½¿ç”¨ 3 åˆ†åŒºé…ç½®")
	t.Logf("  4. âœ… ä¸¤ä¸ªç³»ç»Ÿéƒ½ä½¿ç”¨å…¨å±€ Worker æ± å¤„ç†è®¢é˜…æ¶ˆæ¯")
	t.Logf("  5. âœ… Topic æ•°é‡: 5")
	t.Logf("  6. âœ… æµ‹è¯•å‰å·²æ¸…ç† Kafka å’Œ NATS æ•°æ®")

	t.Logf("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
	t.Logf("%s", separator)
}
